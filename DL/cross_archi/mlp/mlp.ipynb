{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7620488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.data import Subset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba272555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "INPUT_DIR = \"/home/HardDisk/Satang/thesis_proj/New_45/15/split_tws/X_csv_split_31\"\n",
    "CHUNK_SIZE = 31\n",
    "NUM_FEATURES = 8\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "K_FOLDS = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf7ac9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HELPERS ===\n",
    "def collate_fn(batch):\n",
    "    streams, labels = zip(*batch)\n",
    "    streams = list(zip(*streams))\n",
    "    streams = [torch.stack(s) for s in streams]\n",
    "    labels = torch.tensor(labels)\n",
    "    return streams, labels\n",
    "\n",
    "def compute_class_weights(labels, device):\n",
    "    counter = Counter(labels)\n",
    "    total = sum(counter.values())\n",
    "    weights = [np.log(total / (counter[i] + 1)) for i in range(len(counter))]\n",
    "    return torch.tensor(weights, dtype=torch.float).to(device)\n",
    "\n",
    "def print_model_info(model):\n",
    "    print(\"Model Architecture:\")\n",
    "    print(model)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "def plot_loss_curve(train_losses, val_losses):\n",
    "    epochs = range(len(train_losses))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, train_losses, label='Train Loss', color='blue')\n",
    "    plt.plot(epochs, val_losses, label='Val Loss', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy_curve(train_acc, val_acc):\n",
    "    epochs = range(len(train_acc))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, train_acc, label='Train Accuracy', color='green')\n",
    "    plt.plot(epochs, val_acc, label='Val Accuracy', color='red')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training vs Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86cf8f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DATASET CLASS ===\n",
    "class MultiStreamDataset(Dataset):\n",
    "    def __init__(self, data, labels, label_encoder, augment=False):\n",
    "        self.data = data\n",
    "        self.labels = label_encoder.transform(labels)\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def augment_stream(self, stream):\n",
    "        jitter = np.random.normal(0, 0.01, stream.shape)\n",
    "        scale = np.random.normal(1.0, 0.05, stream.shape)\n",
    "        return stream * scale + jitter\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]  # shape: (T, 8)\n",
    "        \n",
    "        if self.augment:\n",
    "            # Apply augmentation to each feature independently\n",
    "            jitter = np.random.normal(0, 0.01, sample.shape)\n",
    "            scale = np.random.normal(1.0, 0.05, sample.shape)\n",
    "            sample = sample * scale + jitter\n",
    "\n",
    "        sample_tensor = torch.tensor(sample, dtype=torch.float32)  # shape: (T, 8)\n",
    "        label_tensor = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "    \n",
    "        return sample_tensor, label_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04b030d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PatchMLP(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class ChannelMLP(nn.Module):\n",
    "    def __init__(self, num_patches, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_patches, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, num_patches)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class MLPMixerRansomwareClassifier(nn.Module):\n",
    "    def __init__(self, seq_len=32, feature_dim=8, num_classes=12, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.patch_mlp = PatchMLP(feature_dim, hidden_dim)\n",
    "        self.channel_mlp = ChannelMLP(seq_len, hidden_dim)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        self.projection = nn.Linear(feature_dim, 128)  # for KD\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(feature_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_features=False):  # x: (B, T, 8)\n",
    "        x = self.patch_mlp(x)          # (B, T, 8)\n",
    "        x = x.permute(0, 2, 1)         # (B, 8, T)\n",
    "        x = self.channel_mlp(x)        # (B, 8, T)\n",
    "\n",
    "        x = self.global_pool(x)        # (B, 8, 1)\n",
    "        feat = x.squeeze(-1)           # (B, 8)\n",
    "\n",
    "        feat_proj = self.projection(feat)      # (B, 128)\n",
    "        logits = self.classifier(feat)         # (B, num_classes)\n",
    "\n",
    "        if return_features:\n",
    "            return logits, feat_proj\n",
    "        else:\n",
    "            return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d3ade61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # === COLLATE FUNCTION ===\n",
    "# def collate_fn(batch):\n",
    "#     streams, labels = zip(*batch)\n",
    "#     streams = list(zip(*streams))\n",
    "#     streams = [torch.stack(s) for s in streams]\n",
    "#     labels = torch.tensor(labels)\n",
    "#     return streams, labels\n",
    "\n",
    "# === LOAD SPLIT FUNCTION ===\n",
    "def load_split_from_folder(split_dir, expected_shape):\n",
    "    X, y = [], []\n",
    "    for class_name in sorted(os.listdir(split_dir)):\n",
    "        class_path = os.path.join(split_dir, class_name)\n",
    "        for fname in sorted(os.listdir(class_path)):\n",
    "            if fname.endswith(\".csv\"):\n",
    "                fpath = os.path.join(class_path, fname)\n",
    "                chunk = pd.read_csv(fpath, header=None).values\n",
    "                if chunk.shape == expected_shape:\n",
    "                    X.append(chunk)\n",
    "                    y.append(class_name)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5820cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === APPLY SMOTE ===\n",
    "def apply_smote_on_training(X_chunks, y_labels, chunk_size, num_features):\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y_labels)\n",
    "\n",
    "    smote_encoder = LabelEncoder()\n",
    "    y_encoded_for_smote = smote_encoder.fit_transform(y_labels)\n",
    "\n",
    "    X_flat = X_chunks.reshape(X_chunks.shape[0], -1)\n",
    "    X_resampled, y_resampled = SMOTE().fit_resample(X_flat, y_encoded_for_smote)\n",
    "\n",
    "    X_res = X_resampled.reshape(-1, chunk_size, num_features)\n",
    "    y_res_str = smote_encoder.inverse_transform(y_resampled)\n",
    "\n",
    "    return X_res, y_res_str, label_encoder\n",
    "\n",
    "# # === LOAD DATASETS ===\n",
    "# expected_shape = (CHUNK_SIZE, NUM_FEATURES)\n",
    "\n",
    "# X_train_raw, y_train_raw = load_split_from_folder(os.path.join(INPUT_DIR, \"train\"), expected_shape)\n",
    "# X_val_raw,   y_val_raw   = load_split_from_folder(os.path.join(INPUT_DIR, \"val\"), expected_shape)\n",
    "\n",
    "# # === SMOTE ONLY ON TRAIN ===\n",
    "# X_train_balanced, y_train_str, label_encoder = apply_smote_on_training(\n",
    "#     X_train_raw, y_train_raw, CHUNK_SIZE, NUM_FEATURES\n",
    "# )\n",
    "\n",
    "# # === CREATE DATASETS ===\n",
    "# train_dataset = MultiStreamDataset(X_train_balanced, y_train_str, label_encoder, augment=True)\n",
    "# val_dataset   = MultiStreamDataset(X_val_raw, y_val_raw, label_encoder, augment=False)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01654335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CLASS WEIGHTING ===\n",
    "def compute_class_weights(labels, device):\n",
    "    from collections import Counter\n",
    "    total = len(labels)\n",
    "    counts = Counter(labels)\n",
    "    weights = [np.log(total / (counts[i] + 1)) for i in range(len(counts))]\n",
    "    return torch.tensor(weights, dtype=torch.float).to(device)\n",
    "\n",
    "# class_weights_tensor = compute_class_weights(label_encoder.transform(y_train_str), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcba14a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, epochs=50, lr=0.001,\n",
    "                  class_weights=None, optimizer=None, scheduler=None,\n",
    "                  best_model_path=\"mlpmixer_teacher.pth\"):\n",
    "\n",
    "    print(\"🔍 Class Weights Tensor:\")\n",
    "    print(class_weights)\n",
    "\n",
    "    criterion_ce = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    best_val_acc = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct = 0.0, 0\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            logits = model(inputs)\n",
    "\n",
    "            loss = criterion_ce(logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (logits.argmax(1) == labels).sum().item()\n",
    "\n",
    "        train_acc = correct / len(train_loader.dataset)\n",
    "        train_losses.append(total_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        # === Validation ===\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                logits = model(inputs)\n",
    "\n",
    "                loss = criterion_ce(logits, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_correct += (logits.argmax(1) == labels).sum().item()\n",
    "\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "              f\"Train Loss: {total_loss:.4f} - Val Loss: {val_loss:.4f} - \"\n",
    "              f\"Train Acc: {train_acc:.4f} - Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"💾 Best model saved to: {best_model_path}\")\n",
    "\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    return train_accuracies, val_accuracies, train_losses, val_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e0fac89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Running for Td=30, Tw=10\n",
      "🔍 Class Weights Tensor:\n",
      "tensor([2.4843, 2.4843, 2.4843, 2.4843, 2.4843, 2.4843, 2.4843, 2.4843, 2.4843,\n",
      "        2.4843, 2.4843, 2.4843], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ea301b/anaconda3/envs/dl_satang/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 336.0935 - Val Loss: 35.1825 - Train Acc: 0.5693 - Val Acc: 0.5486\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw10.pth\n",
      "Epoch 2/50 - Train Loss: 221.1273 - Val Loss: 28.9831 - Train Acc: 0.7053 - Val Acc: 0.6187\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw10.pth\n",
      "Epoch 3/50 - Train Loss: 190.3074 - Val Loss: 27.8167 - Train Acc: 0.7472 - Val Acc: 0.6508\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw10.pth\n",
      "Epoch 4/50 - Train Loss: 165.7556 - Val Loss: 25.9019 - Train Acc: 0.7779 - Val Acc: 0.6628\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw10.pth\n",
      "Epoch 5/50 - Train Loss: 146.8350 - Val Loss: 22.4268 - Train Acc: 0.8091 - Val Acc: 0.7425\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw10.pth\n",
      "Epoch 6/50 - Train Loss: 135.7757 - Val Loss: 21.8446 - Train Acc: 0.8237 - Val Acc: 0.7034\n",
      "Epoch 7/50 - Train Loss: 127.2665 - Val Loss: 24.3631 - Train Acc: 0.8357 - Val Acc: 0.7009\n",
      "Epoch 8/50 - Train Loss: 125.2570 - Val Loss: 20.8889 - Train Acc: 0.8351 - Val Acc: 0.7094\n",
      "Epoch 9/50 - Train Loss: 117.1680 - Val Loss: 15.6109 - Train Acc: 0.8486 - Val Acc: 0.8292\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw10.pth\n",
      "Epoch 10/50 - Train Loss: 114.1823 - Val Loss: 20.3764 - Train Acc: 0.8514 - Val Acc: 0.7029\n",
      "Epoch 11/50 - Train Loss: 110.6470 - Val Loss: 17.9000 - Train Acc: 0.8587 - Val Acc: 0.7705\n",
      "Epoch 12/50 - Train Loss: 107.9995 - Val Loss: 15.6417 - Train Acc: 0.8606 - Val Acc: 0.8282\n",
      "Epoch 13/50 - Train Loss: 103.9711 - Val Loss: 16.9090 - Train Acc: 0.8664 - Val Acc: 0.8041\n",
      "Epoch 14/50 - Train Loss: 105.5545 - Val Loss: 15.8790 - Train Acc: 0.8622 - Val Acc: 0.8362\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw10.pth\n",
      "Epoch 15/50 - Train Loss: 101.7786 - Val Loss: 15.5597 - Train Acc: 0.8684 - Val Acc: 0.8081\n",
      "Epoch 16/50 - Train Loss: 100.6094 - Val Loss: 14.9100 - Train Acc: 0.8690 - Val Acc: 0.8397\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw10.pth\n",
      "Epoch 17/50 - Train Loss: 98.7500 - Val Loss: 15.5528 - Train Acc: 0.8718 - Val Acc: 0.8136\n",
      "Epoch 18/50 - Train Loss: 98.9165 - Val Loss: 15.1880 - Train Acc: 0.8708 - Val Acc: 0.8297\n",
      "Epoch 19/50 - Train Loss: 96.5186 - Val Loss: 13.8573 - Train Acc: 0.8752 - Val Acc: 0.8417\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw10.pth\n",
      "Epoch 20/50 - Train Loss: 96.8597 - Val Loss: 14.1308 - Train Acc: 0.8747 - Val Acc: 0.8372\n",
      "Epoch 21/50 - Train Loss: 92.1984 - Val Loss: 14.7401 - Train Acc: 0.8804 - Val Acc: 0.8337\n",
      "Epoch 22/50 - Train Loss: 92.6990 - Val Loss: 14.5596 - Train Acc: 0.8798 - Val Acc: 0.8377\n",
      "Epoch 23/50 - Train Loss: 91.9635 - Val Loss: 14.9875 - Train Acc: 0.8797 - Val Acc: 0.8312\n",
      "Epoch 24/50 - Train Loss: 89.8876 - Val Loss: 14.3322 - Train Acc: 0.8842 - Val Acc: 0.8527\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw10.pth\n",
      "Epoch 25/50 - Train Loss: 88.7119 - Val Loss: 14.4360 - Train Acc: 0.8856 - Val Acc: 0.8472\n",
      "Epoch 26/50 - Train Loss: 89.9609 - Val Loss: 14.9163 - Train Acc: 0.8837 - Val Acc: 0.8317\n",
      "Epoch 27/50 - Train Loss: 85.7513 - Val Loss: 14.0614 - Train Acc: 0.8908 - Val Acc: 0.8442\n",
      "Epoch 28/50 - Train Loss: 85.8021 - Val Loss: 13.4931 - Train Acc: 0.8894 - Val Acc: 0.8597\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw10.pth\n",
      "Epoch 29/50 - Train Loss: 85.4653 - Val Loss: 12.7190 - Train Acc: 0.8891 - Val Acc: 0.8637\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw10.pth\n",
      "Epoch 30/50 - Train Loss: 85.5534 - Val Loss: 12.7089 - Train Acc: 0.8887 - Val Acc: 0.8557\n",
      "Epoch 31/50 - Train Loss: 85.6286 - Val Loss: 14.3389 - Train Acc: 0.8897 - Val Acc: 0.8337\n",
      "Epoch 32/50 - Train Loss: 82.8367 - Val Loss: 13.0191 - Train Acc: 0.8907 - Val Acc: 0.8732\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw10.pth\n",
      "Epoch 33/50 - Train Loss: 81.6178 - Val Loss: 13.8883 - Train Acc: 0.8927 - Val Acc: 0.8432\n",
      "Epoch 34/50 - Train Loss: 80.7576 - Val Loss: 12.8271 - Train Acc: 0.8987 - Val Acc: 0.8707\n",
      "Epoch 35/50 - Train Loss: 80.2466 - Val Loss: 12.6760 - Train Acc: 0.8973 - Val Acc: 0.8697\n",
      "Epoch 36/50 - Train Loss: 80.0096 - Val Loss: 13.9777 - Train Acc: 0.8981 - Val Acc: 0.8532\n",
      "Epoch 37/50 - Train Loss: 79.3631 - Val Loss: 13.3702 - Train Acc: 0.8974 - Val Acc: 0.8607\n",
      "Epoch 38/50 - Train Loss: 77.4268 - Val Loss: 12.5555 - Train Acc: 0.9010 - Val Acc: 0.8687\n",
      "Epoch 39/50 - Train Loss: 75.9128 - Val Loss: 12.9325 - Train Acc: 0.9036 - Val Acc: 0.8677\n",
      "Epoch 40/50 - Train Loss: 77.0316 - Val Loss: 12.3045 - Train Acc: 0.9021 - Val Acc: 0.8702\n",
      "Epoch 41/50 - Train Loss: 74.6733 - Val Loss: 12.8295 - Train Acc: 0.9055 - Val Acc: 0.8667\n",
      "Epoch 42/50 - Train Loss: 76.4922 - Val Loss: 12.3792 - Train Acc: 0.9010 - Val Acc: 0.8717\n",
      "Epoch 43/50 - Train Loss: 75.5863 - Val Loss: 12.6070 - Train Acc: 0.9070 - Val Acc: 0.8747\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw10.pth\n",
      "Epoch 44/50 - Train Loss: 74.4895 - Val Loss: 12.4841 - Train Acc: 0.9043 - Val Acc: 0.8747\n",
      "Epoch 45/50 - Train Loss: 74.5557 - Val Loss: 12.2280 - Train Acc: 0.9047 - Val Acc: 0.8763\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw10.pth\n",
      "Epoch 46/50 - Train Loss: 73.8275 - Val Loss: 12.6465 - Train Acc: 0.9049 - Val Acc: 0.8682\n",
      "Epoch 47/50 - Train Loss: 73.3247 - Val Loss: 12.4295 - Train Acc: 0.9054 - Val Acc: 0.8763\n",
      "Epoch 48/50 - Train Loss: 73.7151 - Val Loss: 12.3824 - Train Acc: 0.9043 - Val Acc: 0.8773\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw10.pth\n",
      "Epoch 49/50 - Train Loss: 73.5543 - Val Loss: 12.4151 - Train Acc: 0.9060 - Val Acc: 0.8773\n",
      "Epoch 50/50 - Train Loss: 73.7569 - Val Loss: 12.4151 - Train Acc: 0.9058 - Val Acc: 0.8773\n",
      "✅ Finished Td=30, Tw=10 — saved to /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw10.pth\n",
      "\n",
      "🚀 Running for Td=30, Tw=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/2461672243.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Class Weights Tensor:\n",
      "tensor([2.4844, 2.4844, 2.4844, 2.4844, 2.4844, 2.4844, 2.4844, 2.4844, 2.4844,\n",
      "        2.4844, 2.4844, 2.4844], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ea301b/anaconda3/envs/dl_satang/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 437.2708 - Val Loss: 42.4754 - Train Acc: 0.5970 - Val Acc: 0.6063\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "Epoch 2/50 - Train Loss: 229.7026 - Val Loss: 36.3275 - Train Acc: 0.7827 - Val Acc: 0.6932\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "Epoch 3/50 - Train Loss: 192.2737 - Val Loss: 29.7900 - Train Acc: 0.8171 - Val Acc: 0.7411\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "Epoch 4/50 - Train Loss: 168.2614 - Val Loss: 27.3415 - Train Acc: 0.8400 - Val Acc: 0.7582\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "Epoch 5/50 - Train Loss: 158.4143 - Val Loss: 27.5267 - Train Acc: 0.8510 - Val Acc: 0.7468\n",
      "Epoch 6/50 - Train Loss: 145.4029 - Val Loss: 27.2810 - Train Acc: 0.8619 - Val Acc: 0.7396\n",
      "Epoch 7/50 - Train Loss: 139.3022 - Val Loss: 24.5636 - Train Acc: 0.8688 - Val Acc: 0.7912\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "Epoch 8/50 - Train Loss: 134.2667 - Val Loss: 23.6270 - Train Acc: 0.8713 - Val Acc: 0.7745\n",
      "Epoch 9/50 - Train Loss: 125.7099 - Val Loss: 24.5825 - Train Acc: 0.8803 - Val Acc: 0.7984\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "Epoch 10/50 - Train Loss: 123.6665 - Val Loss: 21.1040 - Train Acc: 0.8849 - Val Acc: 0.8223\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "Epoch 11/50 - Train Loss: 119.0415 - Val Loss: 21.6758 - Train Acc: 0.8865 - Val Acc: 0.8132\n",
      "Epoch 12/50 - Train Loss: 112.6220 - Val Loss: 24.5137 - Train Acc: 0.8927 - Val Acc: 0.7893\n",
      "Epoch 13/50 - Train Loss: 111.3083 - Val Loss: 20.8211 - Train Acc: 0.8928 - Val Acc: 0.8352\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "Epoch 14/50 - Train Loss: 107.8260 - Val Loss: 21.1698 - Train Acc: 0.8971 - Val Acc: 0.8288\n",
      "Epoch 15/50 - Train Loss: 104.2571 - Val Loss: 18.9806 - Train Acc: 0.9032 - Val Acc: 0.8307\n",
      "Epoch 16/50 - Train Loss: 100.0287 - Val Loss: 19.3297 - Train Acc: 0.9029 - Val Acc: 0.8181\n",
      "Epoch 17/50 - Train Loss: 98.5740 - Val Loss: 19.4375 - Train Acc: 0.9058 - Val Acc: 0.8368\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "Epoch 18/50 - Train Loss: 94.1890 - Val Loss: 19.8587 - Train Acc: 0.9104 - Val Acc: 0.8424\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "Epoch 19/50 - Train Loss: 91.6299 - Val Loss: 19.7633 - Train Acc: 0.9119 - Val Acc: 0.8436\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "Epoch 20/50 - Train Loss: 89.4966 - Val Loss: 22.5587 - Train Acc: 0.9155 - Val Acc: 0.8341\n",
      "Epoch 21/50 - Train Loss: 85.7272 - Val Loss: 17.8773 - Train Acc: 0.9190 - Val Acc: 0.8489\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "Epoch 22/50 - Train Loss: 83.7101 - Val Loss: 18.7091 - Train Acc: 0.9211 - Val Acc: 0.8470\n",
      "Epoch 23/50 - Train Loss: 81.5907 - Val Loss: 16.8030 - Train Acc: 0.9233 - Val Acc: 0.8607\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "Epoch 24/50 - Train Loss: 80.0052 - Val Loss: 18.2771 - Train Acc: 0.9247 - Val Acc: 0.8599\n",
      "Epoch 25/50 - Train Loss: 77.7408 - Val Loss: 15.8482 - Train Acc: 0.9269 - Val Acc: 0.8614\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "Epoch 26/50 - Train Loss: 74.7280 - Val Loss: 16.5698 - Train Acc: 0.9293 - Val Acc: 0.8766\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "Epoch 27/50 - Train Loss: 74.3030 - Val Loss: 17.6987 - Train Acc: 0.9317 - Val Acc: 0.8595\n",
      "Epoch 28/50 - Train Loss: 72.9992 - Val Loss: 16.7713 - Train Acc: 0.9314 - Val Acc: 0.8637\n",
      "Epoch 29/50 - Train Loss: 71.0996 - Val Loss: 16.6301 - Train Acc: 0.9326 - Val Acc: 0.8728\n",
      "Epoch 30/50 - Train Loss: 67.3689 - Val Loss: 16.2250 - Train Acc: 0.9377 - Val Acc: 0.8800\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "Epoch 31/50 - Train Loss: 66.5299 - Val Loss: 16.0853 - Train Acc: 0.9374 - Val Acc: 0.8808\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "Epoch 32/50 - Train Loss: 66.0232 - Val Loss: 15.3572 - Train Acc: 0.9383 - Val Acc: 0.8846\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "Epoch 33/50 - Train Loss: 65.1223 - Val Loss: 15.8933 - Train Acc: 0.9404 - Val Acc: 0.8853\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "Epoch 34/50 - Train Loss: 62.9301 - Val Loss: 14.9835 - Train Acc: 0.9414 - Val Acc: 0.8865\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "Epoch 35/50 - Train Loss: 62.4588 - Val Loss: 17.4823 - Train Acc: 0.9428 - Val Acc: 0.8694\n",
      "Epoch 36/50 - Train Loss: 61.3100 - Val Loss: 15.7642 - Train Acc: 0.9426 - Val Acc: 0.8834\n",
      "Epoch 37/50 - Train Loss: 60.7642 - Val Loss: 16.3028 - Train Acc: 0.9446 - Val Acc: 0.8808\n",
      "Epoch 38/50 - Train Loss: 59.5555 - Val Loss: 15.5297 - Train Acc: 0.9441 - Val Acc: 0.8869\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "Epoch 39/50 - Train Loss: 58.4679 - Val Loss: 15.8327 - Train Acc: 0.9474 - Val Acc: 0.8827\n",
      "Epoch 40/50 - Train Loss: 58.0278 - Val Loss: 16.1200 - Train Acc: 0.9465 - Val Acc: 0.8850\n",
      "Epoch 41/50 - Train Loss: 56.5358 - Val Loss: 15.7386 - Train Acc: 0.9489 - Val Acc: 0.8869\n",
      "Epoch 42/50 - Train Loss: 55.7009 - Val Loss: 15.4932 - Train Acc: 0.9490 - Val Acc: 0.8869\n",
      "Epoch 43/50 - Train Loss: 55.9794 - Val Loss: 15.7524 - Train Acc: 0.9491 - Val Acc: 0.8872\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "Epoch 44/50 - Train Loss: 55.4328 - Val Loss: 16.0806 - Train Acc: 0.9496 - Val Acc: 0.8869\n",
      "Epoch 45/50 - Train Loss: 54.8271 - Val Loss: 15.3719 - Train Acc: 0.9493 - Val Acc: 0.8850\n",
      "Epoch 46/50 - Train Loss: 54.4201 - Val Loss: 15.8004 - Train Acc: 0.9505 - Val Acc: 0.8869\n",
      "Epoch 47/50 - Train Loss: 53.6482 - Val Loss: 15.5903 - Train Acc: 0.9513 - Val Acc: 0.8869\n",
      "Epoch 48/50 - Train Loss: 54.6528 - Val Loss: 15.5630 - Train Acc: 0.9496 - Val Acc: 0.8888\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "Epoch 49/50 - Train Loss: 54.2155 - Val Loss: 15.5565 - Train Acc: 0.9515 - Val Acc: 0.8888\n",
      "Epoch 50/50 - Train Loss: 54.0929 - Val Loss: 15.5565 - Train Acc: 0.9511 - Val Acc: 0.8888\n",
      "✅ Finished Td=30, Tw=15 — saved to /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw15.pth\n",
      "\n",
      "🚀 Running for Td=30, Tw=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/2461672243.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Class Weights Tensor:\n",
      "tensor([2.4847, 2.4847, 2.4847, 2.4847, 2.4847, 2.4847, 2.4847, 2.4847, 2.4847,\n",
      "        2.4847, 2.4847, 2.4847], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ea301b/anaconda3/envs/dl_satang/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 687.5276 - Val Loss: 73.6451 - Train Acc: 0.6747 - Val Acc: 0.6543\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw20.pth\n",
      "Epoch 2/50 - Train Loss: 420.9087 - Val Loss: 68.8242 - Train Acc: 0.7937 - Val Acc: 0.6960\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw20.pth\n",
      "Epoch 3/50 - Train Loss: 365.2520 - Val Loss: 67.2361 - Train Acc: 0.8237 - Val Acc: 0.7061\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw20.pth\n",
      "Epoch 4/50 - Train Loss: 336.7744 - Val Loss: 63.5200 - Train Acc: 0.8405 - Val Acc: 0.7281\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw20.pth\n",
      "Epoch 5/50 - Train Loss: 309.0777 - Val Loss: 53.0765 - Train Acc: 0.8547 - Val Acc: 0.7679\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw20.pth\n",
      "Epoch 6/50 - Train Loss: 287.9064 - Val Loss: 57.1151 - Train Acc: 0.8637 - Val Acc: 0.7576\n",
      "Epoch 7/50 - Train Loss: 276.1860 - Val Loss: 48.0915 - Train Acc: 0.8692 - Val Acc: 0.7815\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw20.pth\n",
      "Epoch 8/50 - Train Loss: 263.5026 - Val Loss: 50.2797 - Train Acc: 0.8758 - Val Acc: 0.7816\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw20.pth\n",
      "Epoch 9/50 - Train Loss: 250.3074 - Val Loss: 44.6749 - Train Acc: 0.8808 - Val Acc: 0.7891\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw20.pth\n",
      "Epoch 10/50 - Train Loss: 240.2171 - Val Loss: 47.9228 - Train Acc: 0.8852 - Val Acc: 0.7964\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw20.pth\n",
      "Epoch 11/50 - Train Loss: 230.9009 - Val Loss: 44.2393 - Train Acc: 0.8910 - Val Acc: 0.8033\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw20.pth\n",
      "Epoch 12/50 - Train Loss: 223.6282 - Val Loss: 46.3231 - Train Acc: 0.8940 - Val Acc: 0.7883\n",
      "Epoch 13/50 - Train Loss: 217.0138 - Val Loss: 41.2677 - Train Acc: 0.8968 - Val Acc: 0.8245\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw20.pth\n",
      "Epoch 14/50 - Train Loss: 211.6021 - Val Loss: 39.7549 - Train Acc: 0.8997 - Val Acc: 0.8220\n",
      "Epoch 15/50 - Train Loss: 204.4048 - Val Loss: 40.9589 - Train Acc: 0.9038 - Val Acc: 0.8164\n",
      "Epoch 16/50 - Train Loss: 198.1718 - Val Loss: 39.0966 - Train Acc: 0.9078 - Val Acc: 0.8266\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw20.pth\n",
      "Epoch 17/50 - Train Loss: 194.0456 - Val Loss: 34.2962 - Train Acc: 0.9092 - Val Acc: 0.8543\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw20.pth\n",
      "Epoch 18/50 - Train Loss: 188.3127 - Val Loss: 36.2542 - Train Acc: 0.9104 - Val Acc: 0.8446\n",
      "Epoch 19/50 - Train Loss: 185.5972 - Val Loss: 35.6528 - Train Acc: 0.9119 - Val Acc: 0.8491\n",
      "Epoch 20/50 - Train Loss: 179.8329 - Val Loss: 33.1086 - Train Acc: 0.9159 - Val Acc: 0.8587\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw20.pth\n",
      "Epoch 21/50 - Train Loss: 174.4858 - Val Loss: 33.8561 - Train Acc: 0.9174 - Val Acc: 0.8566\n",
      "Epoch 22/50 - Train Loss: 172.3208 - Val Loss: 42.5739 - Train Acc: 0.9187 - Val Acc: 0.8195\n",
      "Epoch 23/50 - Train Loss: 168.9200 - Val Loss: 33.8105 - Train Acc: 0.9200 - Val Acc: 0.8604\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw20.pth\n",
      "Epoch 24/50 - Train Loss: 163.8244 - Val Loss: 34.5637 - Train Acc: 0.9226 - Val Acc: 0.8474\n",
      "Epoch 25/50 - Train Loss: 158.6401 - Val Loss: 33.5767 - Train Acc: 0.9251 - Val Acc: 0.8602\n",
      "Epoch 26/50 - Train Loss: 157.0885 - Val Loss: 29.4976 - Train Acc: 0.9259 - Val Acc: 0.8801\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw20.pth\n",
      "Epoch 27/50 - Train Loss: 157.0854 - Val Loss: 31.2048 - Train Acc: 0.9255 - Val Acc: 0.8663\n",
      "Epoch 28/50 - Train Loss: 151.5264 - Val Loss: 31.7951 - Train Acc: 0.9287 - Val Acc: 0.8620\n",
      "Epoch 29/50 - Train Loss: 148.7661 - Val Loss: 29.4820 - Train Acc: 0.9308 - Val Acc: 0.8782\n",
      "Epoch 30/50 - Train Loss: 147.0220 - Val Loss: 31.8062 - Train Acc: 0.9302 - Val Acc: 0.8650\n",
      "Epoch 31/50 - Train Loss: 143.4971 - Val Loss: 28.6240 - Train Acc: 0.9326 - Val Acc: 0.8790\n",
      "Epoch 32/50 - Train Loss: 142.4116 - Val Loss: 28.1223 - Train Acc: 0.9343 - Val Acc: 0.8822\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw20.pth\n",
      "Epoch 33/50 - Train Loss: 138.9072 - Val Loss: 31.4304 - Train Acc: 0.9344 - Val Acc: 0.8677\n",
      "Epoch 34/50 - Train Loss: 136.4118 - Val Loss: 28.7886 - Train Acc: 0.9354 - Val Acc: 0.8772\n",
      "Epoch 35/50 - Train Loss: 134.9066 - Val Loss: 29.5460 - Train Acc: 0.9372 - Val Acc: 0.8878\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw20.pth\n",
      "Epoch 36/50 - Train Loss: 133.6836 - Val Loss: 30.1083 - Train Acc: 0.9371 - Val Acc: 0.8845\n",
      "Epoch 37/50 - Train Loss: 130.9461 - Val Loss: 30.7170 - Train Acc: 0.9389 - Val Acc: 0.8748\n",
      "Epoch 38/50 - Train Loss: 128.8760 - Val Loss: 30.2402 - Train Acc: 0.9399 - Val Acc: 0.8759\n",
      "Epoch 39/50 - Train Loss: 128.0348 - Val Loss: 26.8466 - Train Acc: 0.9402 - Val Acc: 0.8956\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw20.pth\n",
      "Epoch 40/50 - Train Loss: 126.3641 - Val Loss: 27.4030 - Train Acc: 0.9409 - Val Acc: 0.8899\n",
      "Epoch 41/50 - Train Loss: 125.7992 - Val Loss: 28.7357 - Train Acc: 0.9420 - Val Acc: 0.8885\n",
      "Epoch 42/50 - Train Loss: 125.0425 - Val Loss: 27.5302 - Train Acc: 0.9421 - Val Acc: 0.8885\n",
      "Epoch 43/50 - Train Loss: 123.4549 - Val Loss: 28.2747 - Train Acc: 0.9425 - Val Acc: 0.8897\n",
      "Epoch 44/50 - Train Loss: 122.0601 - Val Loss: 28.1983 - Train Acc: 0.9424 - Val Acc: 0.8887\n",
      "Epoch 45/50 - Train Loss: 120.4217 - Val Loss: 27.9512 - Train Acc: 0.9442 - Val Acc: 0.8889\n",
      "Epoch 46/50 - Train Loss: 121.5609 - Val Loss: 28.2854 - Train Acc: 0.9434 - Val Acc: 0.8902\n",
      "Epoch 47/50 - Train Loss: 120.2383 - Val Loss: 27.9221 - Train Acc: 0.9447 - Val Acc: 0.8908\n",
      "Epoch 48/50 - Train Loss: 119.2282 - Val Loss: 27.6960 - Train Acc: 0.9456 - Val Acc: 0.8922\n",
      "Epoch 49/50 - Train Loss: 120.0284 - Val Loss: 27.8131 - Train Acc: 0.9443 - Val Acc: 0.8912\n",
      "Epoch 50/50 - Train Loss: 120.0486 - Val Loss: 27.8131 - Train Acc: 0.9435 - Val Acc: 0.8912\n",
      "✅ Finished Td=30, Tw=20 — saved to /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td30_Tw20.pth\n",
      "\n",
      "🚀 Running for Td=45, Tw=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/2461672243.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Class Weights Tensor:\n",
      "tensor([2.4837, 2.4837, 2.4837, 2.4837, 2.4837, 2.4837, 2.4837, 2.4837, 2.4837,\n",
      "        2.4837, 2.4837, 2.4837], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ea301b/anaconda3/envs/dl_satang/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 222.0285 - Val Loss: 17.6494 - Train Acc: 0.5185 - Val Acc: 0.5862\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw10.pth\n",
      "Epoch 2/50 - Train Loss: 108.6802 - Val Loss: 13.9528 - Train Acc: 0.7528 - Val Acc: 0.6628\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw10.pth\n",
      "Epoch 3/50 - Train Loss: 80.0092 - Val Loss: 9.9733 - Train Acc: 0.8181 - Val Acc: 0.7742\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw10.pth\n",
      "Epoch 4/50 - Train Loss: 66.0590 - Val Loss: 13.9503 - Train Acc: 0.8515 - Val Acc: 0.6996\n",
      "Epoch 5/50 - Train Loss: 59.4781 - Val Loss: 8.4655 - Train Acc: 0.8666 - Val Acc: 0.7946\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw10.pth\n",
      "Epoch 6/50 - Train Loss: 52.9078 - Val Loss: 7.7631 - Train Acc: 0.8794 - Val Acc: 0.8169\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw10.pth\n",
      "Epoch 7/50 - Train Loss: 47.6466 - Val Loss: 8.1176 - Train Acc: 0.8922 - Val Acc: 0.8052\n",
      "Epoch 8/50 - Train Loss: 48.3192 - Val Loss: 8.3449 - Train Acc: 0.8901 - Val Acc: 0.7965\n",
      "Epoch 9/50 - Train Loss: 44.7641 - Val Loss: 7.1257 - Train Acc: 0.9013 - Val Acc: 0.8304\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw10.pth\n",
      "Epoch 10/50 - Train Loss: 41.1037 - Val Loss: 8.0680 - Train Acc: 0.9101 - Val Acc: 0.8081\n",
      "Epoch 11/50 - Train Loss: 37.8552 - Val Loss: 7.0500 - Train Acc: 0.9157 - Val Acc: 0.8488\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw10.pth\n",
      "Epoch 12/50 - Train Loss: 34.5921 - Val Loss: 8.1722 - Train Acc: 0.9204 - Val Acc: 0.8333\n",
      "Epoch 13/50 - Train Loss: 33.7663 - Val Loss: 8.9204 - Train Acc: 0.9245 - Val Acc: 0.8091\n",
      "Epoch 14/50 - Train Loss: 33.5865 - Val Loss: 7.1986 - Train Acc: 0.9253 - Val Acc: 0.8517\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw10.pth\n",
      "Epoch 15/50 - Train Loss: 29.4166 - Val Loss: 6.4227 - Train Acc: 0.9353 - Val Acc: 0.8624\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw10.pth\n",
      "Epoch 16/50 - Train Loss: 30.8710 - Val Loss: 7.4925 - Train Acc: 0.9321 - Val Acc: 0.8421\n",
      "Epoch 17/50 - Train Loss: 28.8382 - Val Loss: 9.5438 - Train Acc: 0.9364 - Val Acc: 0.8062\n",
      "Epoch 18/50 - Train Loss: 27.4619 - Val Loss: 6.2882 - Train Acc: 0.9409 - Val Acc: 0.8798\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw10.pth\n",
      "Epoch 19/50 - Train Loss: 29.5099 - Val Loss: 5.8602 - Train Acc: 0.9351 - Val Acc: 0.8818\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw10.pth\n",
      "Epoch 20/50 - Train Loss: 26.6118 - Val Loss: 6.0727 - Train Acc: 0.9425 - Val Acc: 0.8769\n",
      "Epoch 21/50 - Train Loss: 25.7219 - Val Loss: 6.3446 - Train Acc: 0.9430 - Val Acc: 0.8614\n",
      "Epoch 22/50 - Train Loss: 24.4603 - Val Loss: 6.7415 - Train Acc: 0.9465 - Val Acc: 0.8488\n",
      "Epoch 23/50 - Train Loss: 24.6595 - Val Loss: 5.4325 - Train Acc: 0.9477 - Val Acc: 0.8857\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw10.pth\n",
      "Epoch 24/50 - Train Loss: 25.0230 - Val Loss: 6.0375 - Train Acc: 0.9437 - Val Acc: 0.8866\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw10.pth\n",
      "Epoch 25/50 - Train Loss: 23.1009 - Val Loss: 5.6309 - Train Acc: 0.9498 - Val Acc: 0.8866\n",
      "Epoch 26/50 - Train Loss: 22.2209 - Val Loss: 6.5101 - Train Acc: 0.9509 - Val Acc: 0.8818\n",
      "Epoch 27/50 - Train Loss: 22.0062 - Val Loss: 5.8408 - Train Acc: 0.9509 - Val Acc: 0.8769\n",
      "Epoch 28/50 - Train Loss: 21.3744 - Val Loss: 6.2409 - Train Acc: 0.9531 - Val Acc: 0.8769\n",
      "Epoch 29/50 - Train Loss: 19.8292 - Val Loss: 5.7749 - Train Acc: 0.9568 - Val Acc: 0.8895\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw10.pth\n",
      "Epoch 30/50 - Train Loss: 19.5826 - Val Loss: 5.8838 - Train Acc: 0.9578 - Val Acc: 0.8769\n",
      "Epoch 31/50 - Train Loss: 19.4598 - Val Loss: 6.0162 - Train Acc: 0.9563 - Val Acc: 0.8895\n",
      "Epoch 32/50 - Train Loss: 19.4331 - Val Loss: 5.7215 - Train Acc: 0.9580 - Val Acc: 0.9012\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw10.pth\n",
      "Epoch 33/50 - Train Loss: 18.3180 - Val Loss: 5.3333 - Train Acc: 0.9593 - Val Acc: 0.9002\n",
      "Epoch 34/50 - Train Loss: 17.5428 - Val Loss: 5.4162 - Train Acc: 0.9614 - Val Acc: 0.8973\n",
      "Epoch 35/50 - Train Loss: 17.2252 - Val Loss: 6.1005 - Train Acc: 0.9622 - Val Acc: 0.8837\n",
      "Epoch 36/50 - Train Loss: 16.4816 - Val Loss: 5.4689 - Train Acc: 0.9648 - Val Acc: 0.9031\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw10.pth\n",
      "Epoch 37/50 - Train Loss: 15.7462 - Val Loss: 5.6431 - Train Acc: 0.9658 - Val Acc: 0.9012\n",
      "Epoch 38/50 - Train Loss: 15.7743 - Val Loss: 5.4844 - Train Acc: 0.9633 - Val Acc: 0.8973\n",
      "Epoch 39/50 - Train Loss: 15.1911 - Val Loss: 5.7982 - Train Acc: 0.9662 - Val Acc: 0.8924\n",
      "Epoch 40/50 - Train Loss: 14.7822 - Val Loss: 5.2162 - Train Acc: 0.9673 - Val Acc: 0.9079\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw10.pth\n",
      "Epoch 41/50 - Train Loss: 14.2438 - Val Loss: 5.7545 - Train Acc: 0.9675 - Val Acc: 0.8992\n",
      "Epoch 42/50 - Train Loss: 14.0311 - Val Loss: 5.4481 - Train Acc: 0.9689 - Val Acc: 0.9099\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw10.pth\n",
      "Epoch 43/50 - Train Loss: 13.7831 - Val Loss: 5.5369 - Train Acc: 0.9690 - Val Acc: 0.9012\n",
      "Epoch 44/50 - Train Loss: 13.6796 - Val Loss: 5.3851 - Train Acc: 0.9699 - Val Acc: 0.9118\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw10.pth\n",
      "Epoch 45/50 - Train Loss: 13.0168 - Val Loss: 5.5671 - Train Acc: 0.9715 - Val Acc: 0.9138\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw10.pth\n",
      "Epoch 46/50 - Train Loss: 13.4375 - Val Loss: 5.4591 - Train Acc: 0.9700 - Val Acc: 0.9157\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw10.pth\n",
      "Epoch 47/50 - Train Loss: 13.0763 - Val Loss: 5.4608 - Train Acc: 0.9703 - Val Acc: 0.9138\n",
      "Epoch 48/50 - Train Loss: 13.1193 - Val Loss: 5.5075 - Train Acc: 0.9706 - Val Acc: 0.9167\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw10.pth\n",
      "Epoch 49/50 - Train Loss: 13.0894 - Val Loss: 5.5151 - Train Acc: 0.9709 - Val Acc: 0.9167\n",
      "Epoch 50/50 - Train Loss: 13.0466 - Val Loss: 5.5151 - Train Acc: 0.9709 - Val Acc: 0.9167\n",
      "✅ Finished Td=45, Tw=10 — saved to /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw10.pth\n",
      "\n",
      "🚀 Running for Td=45, Tw=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/2461672243.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Class Weights Tensor:\n",
      "tensor([2.4840, 2.4840, 2.4840, 2.4840, 2.4840, 2.4840, 2.4840, 2.4840, 2.4840,\n",
      "        2.4840, 2.4840, 2.4840], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ea301b/anaconda3/envs/dl_satang/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 267.6854 - Val Loss: 23.5324 - Train Acc: 0.5562 - Val Acc: 0.6026\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw15.pth\n",
      "Epoch 2/50 - Train Loss: 128.3177 - Val Loss: 18.0604 - Train Acc: 0.7711 - Val Acc: 0.6713\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw15.pth\n",
      "Epoch 3/50 - Train Loss: 100.3712 - Val Loss: 15.2395 - Train Acc: 0.8209 - Val Acc: 0.7392\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw15.pth\n",
      "Epoch 4/50 - Train Loss: 87.2376 - Val Loss: 16.2161 - Train Acc: 0.8456 - Val Acc: 0.7611\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw15.pth\n",
      "Epoch 5/50 - Train Loss: 77.3550 - Val Loss: 13.0925 - Train Acc: 0.8638 - Val Acc: 0.7801\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw15.pth\n",
      "Epoch 6/50 - Train Loss: 67.5180 - Val Loss: 12.8648 - Train Acc: 0.8830 - Val Acc: 0.7896\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw15.pth\n",
      "Epoch 7/50 - Train Loss: 63.5165 - Val Loss: 14.6974 - Train Acc: 0.8864 - Val Acc: 0.7663\n",
      "Epoch 8/50 - Train Loss: 60.1487 - Val Loss: 10.5969 - Train Acc: 0.8920 - Val Acc: 0.8262\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw15.pth\n",
      "Epoch 9/50 - Train Loss: 55.0751 - Val Loss: 10.3611 - Train Acc: 0.9022 - Val Acc: 0.8313\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw15.pth\n",
      "Epoch 10/50 - Train Loss: 52.5328 - Val Loss: 14.6161 - Train Acc: 0.9070 - Val Acc: 0.7641\n",
      "Epoch 11/50 - Train Loss: 52.7749 - Val Loss: 10.1545 - Train Acc: 0.9081 - Val Acc: 0.8152\n",
      "Epoch 12/50 - Train Loss: 51.4333 - Val Loss: 9.7778 - Train Acc: 0.9091 - Val Acc: 0.8298\n",
      "Epoch 13/50 - Train Loss: 47.1898 - Val Loss: 9.8967 - Train Acc: 0.9169 - Val Acc: 0.8430\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw15.pth\n",
      "Epoch 14/50 - Train Loss: 44.2422 - Val Loss: 9.1635 - Train Acc: 0.9228 - Val Acc: 0.8356\n",
      "Epoch 15/50 - Train Loss: 44.9009 - Val Loss: 9.2255 - Train Acc: 0.9235 - Val Acc: 0.8554\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw15.pth\n",
      "Epoch 16/50 - Train Loss: 43.7714 - Val Loss: 9.0578 - Train Acc: 0.9252 - Val Acc: 0.8430\n",
      "Epoch 17/50 - Train Loss: 40.5952 - Val Loss: 9.4571 - Train Acc: 0.9306 - Val Acc: 0.8422\n",
      "Epoch 18/50 - Train Loss: 40.0252 - Val Loss: 11.6287 - Train Acc: 0.9319 - Val Acc: 0.8167\n",
      "Epoch 19/50 - Train Loss: 38.9241 - Val Loss: 7.4661 - Train Acc: 0.9346 - Val Acc: 0.8780\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw15.pth\n",
      "Epoch 20/50 - Train Loss: 37.6161 - Val Loss: 7.9538 - Train Acc: 0.9372 - Val Acc: 0.8663\n",
      "Epoch 21/50 - Train Loss: 35.4935 - Val Loss: 8.7427 - Train Acc: 0.9424 - Val Acc: 0.8459\n",
      "Epoch 22/50 - Train Loss: 34.6094 - Val Loss: 8.3391 - Train Acc: 0.9419 - Val Acc: 0.8751\n",
      "Epoch 23/50 - Train Loss: 32.9643 - Val Loss: 7.8992 - Train Acc: 0.9438 - Val Acc: 0.8722\n",
      "Epoch 24/50 - Train Loss: 32.7710 - Val Loss: 8.1141 - Train Acc: 0.9451 - Val Acc: 0.8634\n",
      "Epoch 25/50 - Train Loss: 31.0407 - Val Loss: 6.8955 - Train Acc: 0.9473 - Val Acc: 0.8773\n",
      "Epoch 26/50 - Train Loss: 29.0029 - Val Loss: 6.9195 - Train Acc: 0.9506 - Val Acc: 0.8868\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw15.pth\n",
      "Epoch 27/50 - Train Loss: 27.9917 - Val Loss: 7.4286 - Train Acc: 0.9533 - Val Acc: 0.8809\n",
      "Epoch 28/50 - Train Loss: 27.7804 - Val Loss: 6.4552 - Train Acc: 0.9554 - Val Acc: 0.8977\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw15.pth\n",
      "Epoch 29/50 - Train Loss: 26.8630 - Val Loss: 6.5548 - Train Acc: 0.9566 - Val Acc: 0.8897\n",
      "Epoch 30/50 - Train Loss: 26.0551 - Val Loss: 6.6057 - Train Acc: 0.9574 - Val Acc: 0.8970\n",
      "Epoch 31/50 - Train Loss: 25.2212 - Val Loss: 6.0571 - Train Acc: 0.9587 - Val Acc: 0.9021\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw15.pth\n",
      "Epoch 32/50 - Train Loss: 24.0925 - Val Loss: 7.3022 - Train Acc: 0.9604 - Val Acc: 0.8773\n",
      "Epoch 33/50 - Train Loss: 23.5180 - Val Loss: 6.4685 - Train Acc: 0.9608 - Val Acc: 0.8955\n",
      "Epoch 34/50 - Train Loss: 22.8800 - Val Loss: 6.1656 - Train Acc: 0.9626 - Val Acc: 0.8999\n",
      "Epoch 35/50 - Train Loss: 22.0306 - Val Loss: 6.5116 - Train Acc: 0.9646 - Val Acc: 0.9028\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw15.pth\n",
      "Epoch 36/50 - Train Loss: 21.9901 - Val Loss: 6.6926 - Train Acc: 0.9644 - Val Acc: 0.9028\n",
      "Epoch 37/50 - Train Loss: 21.2779 - Val Loss: 6.6641 - Train Acc: 0.9656 - Val Acc: 0.8977\n",
      "Epoch 38/50 - Train Loss: 20.3681 - Val Loss: 6.5828 - Train Acc: 0.9671 - Val Acc: 0.8970\n",
      "Epoch 39/50 - Train Loss: 19.9836 - Val Loss: 5.8311 - Train Acc: 0.9677 - Val Acc: 0.9043\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw15.pth\n",
      "Epoch 40/50 - Train Loss: 19.3118 - Val Loss: 6.1590 - Train Acc: 0.9686 - Val Acc: 0.9050\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw15.pth\n",
      "Epoch 41/50 - Train Loss: 18.7506 - Val Loss: 6.2444 - Train Acc: 0.9704 - Val Acc: 0.9021\n",
      "Epoch 42/50 - Train Loss: 18.6127 - Val Loss: 6.0869 - Train Acc: 0.9699 - Val Acc: 0.9021\n",
      "Epoch 43/50 - Train Loss: 18.4481 - Val Loss: 6.1423 - Train Acc: 0.9702 - Val Acc: 0.9050\n",
      "Epoch 44/50 - Train Loss: 18.4039 - Val Loss: 6.1574 - Train Acc: 0.9690 - Val Acc: 0.9036\n",
      "Epoch 45/50 - Train Loss: 18.0529 - Val Loss: 6.0846 - Train Acc: 0.9709 - Val Acc: 0.9058\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw15.pth\n",
      "Epoch 46/50 - Train Loss: 17.9164 - Val Loss: 5.9955 - Train Acc: 0.9705 - Val Acc: 0.9021\n",
      "Epoch 47/50 - Train Loss: 17.6722 - Val Loss: 5.9905 - Train Acc: 0.9718 - Val Acc: 0.9050\n",
      "Epoch 48/50 - Train Loss: 17.5635 - Val Loss: 6.0114 - Train Acc: 0.9711 - Val Acc: 0.9043\n",
      "Epoch 49/50 - Train Loss: 17.5711 - Val Loss: 6.0158 - Train Acc: 0.9712 - Val Acc: 0.9050\n",
      "Epoch 50/50 - Train Loss: 17.9372 - Val Loss: 6.0158 - Train Acc: 0.9703 - Val Acc: 0.9050\n",
      "✅ Finished Td=45, Tw=15 — saved to /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw15.pth\n",
      "\n",
      "🚀 Running for Td=45, Tw=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/2461672243.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Class Weights Tensor:\n",
      "tensor([2.4841, 2.4841, 2.4841, 2.4841, 2.4841, 2.4841, 2.4841, 2.4841, 2.4841,\n",
      "        2.4841, 2.4841, 2.4841], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ea301b/anaconda3/envs/dl_satang/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 285.8109 - Val Loss: 23.2564 - Train Acc: 0.5979 - Val Acc: 0.6423\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw20.pth\n",
      "Epoch 2/50 - Train Loss: 143.7862 - Val Loss: 19.8373 - Train Acc: 0.7836 - Val Acc: 0.6902\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw20.pth\n",
      "Epoch 3/50 - Train Loss: 113.8500 - Val Loss: 19.9279 - Train Acc: 0.8278 - Val Acc: 0.7066\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw20.pth\n",
      "Epoch 4/50 - Train Loss: 96.5533 - Val Loss: 15.6458 - Train Acc: 0.8563 - Val Acc: 0.7716\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw20.pth\n",
      "Epoch 5/50 - Train Loss: 89.6440 - Val Loss: 15.3178 - Train Acc: 0.8650 - Val Acc: 0.7874\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw20.pth\n",
      "Epoch 6/50 - Train Loss: 80.4454 - Val Loss: 15.9019 - Train Acc: 0.8806 - Val Acc: 0.7609\n",
      "Epoch 7/50 - Train Loss: 75.4640 - Val Loss: 18.2077 - Train Acc: 0.8858 - Val Acc: 0.7521\n",
      "Epoch 8/50 - Train Loss: 71.4513 - Val Loss: 15.3534 - Train Acc: 0.8938 - Val Acc: 0.7956\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw20.pth\n",
      "Epoch 9/50 - Train Loss: 67.7774 - Val Loss: 13.4377 - Train Acc: 0.8989 - Val Acc: 0.8000\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw20.pth\n",
      "Epoch 10/50 - Train Loss: 64.7311 - Val Loss: 15.2292 - Train Acc: 0.9047 - Val Acc: 0.7924\n",
      "Epoch 11/50 - Train Loss: 64.4704 - Val Loss: 11.6657 - Train Acc: 0.9051 - Val Acc: 0.8410\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw20.pth\n",
      "Epoch 12/50 - Train Loss: 60.2735 - Val Loss: 13.6911 - Train Acc: 0.9092 - Val Acc: 0.8120\n",
      "Epoch 13/50 - Train Loss: 56.8221 - Val Loss: 11.2345 - Train Acc: 0.9135 - Val Acc: 0.8360\n",
      "Epoch 14/50 - Train Loss: 54.6110 - Val Loss: 12.2097 - Train Acc: 0.9180 - Val Acc: 0.8404\n",
      "Epoch 15/50 - Train Loss: 53.2510 - Val Loss: 11.4245 - Train Acc: 0.9203 - Val Acc: 0.8612\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw20.pth\n",
      "Epoch 16/50 - Train Loss: 52.4610 - Val Loss: 12.3779 - Train Acc: 0.9234 - Val Acc: 0.8536\n",
      "Epoch 17/50 - Train Loss: 49.0547 - Val Loss: 12.8386 - Train Acc: 0.9290 - Val Acc: 0.8196\n",
      "Epoch 18/50 - Train Loss: 47.8413 - Val Loss: 13.4807 - Train Acc: 0.9285 - Val Acc: 0.8069\n",
      "Epoch 19/50 - Train Loss: 46.2267 - Val Loss: 11.3259 - Train Acc: 0.9305 - Val Acc: 0.8379\n",
      "Epoch 20/50 - Train Loss: 44.8065 - Val Loss: 11.3318 - Train Acc: 0.9333 - Val Acc: 0.8429\n",
      "Epoch 21/50 - Train Loss: 42.8528 - Val Loss: 9.7285 - Train Acc: 0.9369 - Val Acc: 0.8631\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw20.pth\n",
      "Epoch 22/50 - Train Loss: 41.7876 - Val Loss: 10.8899 - Train Acc: 0.9389 - Val Acc: 0.8473\n",
      "Epoch 23/50 - Train Loss: 40.8925 - Val Loss: 11.4060 - Train Acc: 0.9398 - Val Acc: 0.8669\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw20.pth\n",
      "Epoch 24/50 - Train Loss: 39.9979 - Val Loss: 10.4194 - Train Acc: 0.9430 - Val Acc: 0.8562\n",
      "Epoch 25/50 - Train Loss: 38.4096 - Val Loss: 10.2612 - Train Acc: 0.9432 - Val Acc: 0.8574\n",
      "Epoch 26/50 - Train Loss: 36.0419 - Val Loss: 9.9820 - Train Acc: 0.9468 - Val Acc: 0.8770\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw20.pth\n",
      "Epoch 27/50 - Train Loss: 35.8489 - Val Loss: 9.3544 - Train Acc: 0.9466 - Val Acc: 0.8801\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw20.pth\n",
      "Epoch 28/50 - Train Loss: 34.4353 - Val Loss: 8.7832 - Train Acc: 0.9501 - Val Acc: 0.8890\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw20.pth\n",
      "Epoch 29/50 - Train Loss: 35.4612 - Val Loss: 9.9452 - Train Acc: 0.9487 - Val Acc: 0.8757\n",
      "Epoch 30/50 - Train Loss: 33.7409 - Val Loss: 8.7428 - Train Acc: 0.9524 - Val Acc: 0.8808\n",
      "Epoch 31/50 - Train Loss: 31.8239 - Val Loss: 9.4761 - Train Acc: 0.9548 - Val Acc: 0.8757\n",
      "Epoch 32/50 - Train Loss: 31.1861 - Val Loss: 8.5491 - Train Acc: 0.9549 - Val Acc: 0.8826\n",
      "Epoch 33/50 - Train Loss: 30.1850 - Val Loss: 9.0263 - Train Acc: 0.9559 - Val Acc: 0.8833\n",
      "Epoch 34/50 - Train Loss: 29.4420 - Val Loss: 8.6842 - Train Acc: 0.9585 - Val Acc: 0.8921\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw20.pth\n",
      "Epoch 35/50 - Train Loss: 29.0375 - Val Loss: 8.6724 - Train Acc: 0.9588 - Val Acc: 0.8890\n",
      "Epoch 36/50 - Train Loss: 28.2270 - Val Loss: 9.6390 - Train Acc: 0.9593 - Val Acc: 0.8820\n",
      "Epoch 37/50 - Train Loss: 27.0755 - Val Loss: 8.5409 - Train Acc: 0.9617 - Val Acc: 0.8890\n",
      "Epoch 38/50 - Train Loss: 27.1705 - Val Loss: 9.0031 - Train Acc: 0.9615 - Val Acc: 0.8839\n",
      "Epoch 39/50 - Train Loss: 25.5509 - Val Loss: 8.5174 - Train Acc: 0.9651 - Val Acc: 0.8940\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw20.pth\n",
      "Epoch 40/50 - Train Loss: 25.4208 - Val Loss: 8.8639 - Train Acc: 0.9646 - Val Acc: 0.8934\n",
      "Epoch 41/50 - Train Loss: 25.3600 - Val Loss: 8.3986 - Train Acc: 0.9650 - Val Acc: 0.8934\n",
      "Epoch 42/50 - Train Loss: 25.0565 - Val Loss: 8.5153 - Train Acc: 0.9640 - Val Acc: 0.8959\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw20.pth\n",
      "Epoch 43/50 - Train Loss: 24.3508 - Val Loss: 8.5606 - Train Acc: 0.9653 - Val Acc: 0.8927\n",
      "Epoch 44/50 - Train Loss: 24.2101 - Val Loss: 8.9964 - Train Acc: 0.9669 - Val Acc: 0.8902\n",
      "Epoch 45/50 - Train Loss: 24.2087 - Val Loss: 8.7993 - Train Acc: 0.9676 - Val Acc: 0.8927\n",
      "Epoch 46/50 - Train Loss: 23.6186 - Val Loss: 8.6901 - Train Acc: 0.9667 - Val Acc: 0.8946\n",
      "Epoch 47/50 - Train Loss: 23.8727 - Val Loss: 8.4374 - Train Acc: 0.9671 - Val Acc: 0.8991\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw20.pth\n",
      "Epoch 48/50 - Train Loss: 23.2136 - Val Loss: 8.5411 - Train Acc: 0.9670 - Val Acc: 0.8972\n",
      "Epoch 49/50 - Train Loss: 23.5955 - Val Loss: 8.5390 - Train Acc: 0.9679 - Val Acc: 0.8972\n",
      "Epoch 50/50 - Train Loss: 23.2953 - Val Loss: 8.5390 - Train Acc: 0.9676 - Val Acc: 0.8972\n",
      "✅ Finished Td=45, Tw=20 — saved to /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td45_Tw20.pth\n",
      "\n",
      "🚀 Running for Td=60, Tw=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/2461672243.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Class Weights Tensor:\n",
      "tensor([2.4832, 2.4832, 2.4832, 2.4832, 2.4832, 2.4832, 2.4832, 2.4832, 2.4832,\n",
      "        2.4832, 2.4832, 2.4832], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ea301b/anaconda3/envs/dl_satang/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 169.0825 - Val Loss: 13.7576 - Train Acc: 0.4900 - Val Acc: 0.4905\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw10.pth\n",
      "Epoch 2/50 - Train Loss: 80.7735 - Val Loss: 9.2341 - Train Acc: 0.7529 - Val Acc: 0.6885\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw10.pth\n",
      "Epoch 3/50 - Train Loss: 53.6513 - Val Loss: 7.9797 - Train Acc: 0.8339 - Val Acc: 0.7511\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw10.pth\n",
      "Epoch 4/50 - Train Loss: 41.2439 - Val Loss: 6.6379 - Train Acc: 0.8689 - Val Acc: 0.7904\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw10.pth\n",
      "Epoch 5/50 - Train Loss: 34.6265 - Val Loss: 6.9452 - Train Acc: 0.8912 - Val Acc: 0.7773\n",
      "Epoch 6/50 - Train Loss: 30.1530 - Val Loss: 4.8365 - Train Acc: 0.9089 - Val Acc: 0.8544\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw10.pth\n",
      "Epoch 7/50 - Train Loss: 24.8690 - Val Loss: 5.5951 - Train Acc: 0.9254 - Val Acc: 0.8428\n",
      "Epoch 8/50 - Train Loss: 25.0953 - Val Loss: 4.8196 - Train Acc: 0.9217 - Val Acc: 0.8515\n",
      "Epoch 9/50 - Train Loss: 22.0915 - Val Loss: 4.4158 - Train Acc: 0.9306 - Val Acc: 0.8777\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw10.pth\n",
      "Epoch 10/50 - Train Loss: 20.0952 - Val Loss: 4.5285 - Train Acc: 0.9413 - Val Acc: 0.8748\n",
      "Epoch 11/50 - Train Loss: 18.8138 - Val Loss: 4.3966 - Train Acc: 0.9441 - Val Acc: 0.8501\n",
      "Epoch 12/50 - Train Loss: 18.3239 - Val Loss: 3.7146 - Train Acc: 0.9459 - Val Acc: 0.8806\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw10.pth\n",
      "Epoch 13/50 - Train Loss: 17.1472 - Val Loss: 3.7740 - Train Acc: 0.9509 - Val Acc: 0.9054\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw10.pth\n",
      "Epoch 14/50 - Train Loss: 16.1572 - Val Loss: 3.4233 - Train Acc: 0.9517 - Val Acc: 0.9141\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw10.pth\n",
      "Epoch 15/50 - Train Loss: 14.6612 - Val Loss: 3.1057 - Train Acc: 0.9581 - Val Acc: 0.9243\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw10.pth\n",
      "Epoch 16/50 - Train Loss: 14.2200 - Val Loss: 3.1693 - Train Acc: 0.9585 - Val Acc: 0.9112\n",
      "Epoch 17/50 - Train Loss: 13.3939 - Val Loss: 3.7935 - Train Acc: 0.9595 - Val Acc: 0.9185\n",
      "Epoch 18/50 - Train Loss: 12.3680 - Val Loss: 4.0919 - Train Acc: 0.9641 - Val Acc: 0.9098\n",
      "Epoch 19/50 - Train Loss: 11.2512 - Val Loss: 3.3996 - Train Acc: 0.9683 - Val Acc: 0.9156\n",
      "Epoch 20/50 - Train Loss: 11.3444 - Val Loss: 2.4823 - Train Acc: 0.9647 - Val Acc: 0.9360\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw10.pth\n",
      "Epoch 21/50 - Train Loss: 10.1246 - Val Loss: 3.2240 - Train Acc: 0.9722 - Val Acc: 0.9316\n",
      "Epoch 22/50 - Train Loss: 9.0371 - Val Loss: 3.0578 - Train Acc: 0.9744 - Val Acc: 0.9170\n",
      "Epoch 23/50 - Train Loss: 10.6902 - Val Loss: 3.1989 - Train Acc: 0.9697 - Val Acc: 0.9330\n",
      "Epoch 24/50 - Train Loss: 8.4012 - Val Loss: 2.4683 - Train Acc: 0.9752 - Val Acc: 0.9389\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw10.pth\n",
      "Epoch 25/50 - Train Loss: 8.8147 - Val Loss: 3.9463 - Train Acc: 0.9725 - Val Acc: 0.9287\n",
      "Epoch 26/50 - Train Loss: 7.3258 - Val Loss: 4.2190 - Train Acc: 0.9797 - Val Acc: 0.8937\n",
      "Epoch 27/50 - Train Loss: 7.8702 - Val Loss: 2.6405 - Train Acc: 0.9775 - Val Acc: 0.9461\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw10.pth\n",
      "Epoch 28/50 - Train Loss: 6.1973 - Val Loss: 2.7163 - Train Acc: 0.9802 - Val Acc: 0.9534\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw10.pth\n",
      "Epoch 29/50 - Train Loss: 6.9866 - Val Loss: 2.8080 - Train Acc: 0.9798 - Val Acc: 0.9389\n",
      "Epoch 30/50 - Train Loss: 5.8033 - Val Loss: 2.6792 - Train Acc: 0.9843 - Val Acc: 0.9403\n",
      "Epoch 31/50 - Train Loss: 5.3034 - Val Loss: 2.7754 - Train Acc: 0.9865 - Val Acc: 0.9476\n",
      "Epoch 32/50 - Train Loss: 4.5641 - Val Loss: 3.0077 - Train Acc: 0.9883 - Val Acc: 0.9389\n",
      "Epoch 33/50 - Train Loss: 4.4658 - Val Loss: 2.4294 - Train Acc: 0.9878 - Val Acc: 0.9563\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw10.pth\n",
      "Epoch 34/50 - Train Loss: 4.4147 - Val Loss: 2.2851 - Train Acc: 0.9872 - Val Acc: 0.9520\n",
      "Epoch 35/50 - Train Loss: 3.7855 - Val Loss: 2.4013 - Train Acc: 0.9889 - Val Acc: 0.9461\n",
      "Epoch 36/50 - Train Loss: 3.3239 - Val Loss: 2.5434 - Train Acc: 0.9919 - Val Acc: 0.9476\n",
      "Epoch 37/50 - Train Loss: 3.3926 - Val Loss: 2.2627 - Train Acc: 0.9922 - Val Acc: 0.9432\n",
      "Epoch 38/50 - Train Loss: 3.4014 - Val Loss: 2.9967 - Train Acc: 0.9900 - Val Acc: 0.9389\n",
      "Epoch 39/50 - Train Loss: 3.1215 - Val Loss: 2.7765 - Train Acc: 0.9919 - Val Acc: 0.9476\n",
      "Epoch 40/50 - Train Loss: 2.5217 - Val Loss: 2.1915 - Train Acc: 0.9946 - Val Acc: 0.9491\n",
      "Epoch 41/50 - Train Loss: 2.4133 - Val Loss: 2.4065 - Train Acc: 0.9946 - Val Acc: 0.9534\n",
      "Epoch 42/50 - Train Loss: 2.2303 - Val Loss: 2.6886 - Train Acc: 0.9950 - Val Acc: 0.9476\n",
      "Epoch 43/50 - Train Loss: 2.2305 - Val Loss: 2.6299 - Train Acc: 0.9954 - Val Acc: 0.9461\n",
      "Epoch 44/50 - Train Loss: 2.0894 - Val Loss: 2.5521 - Train Acc: 0.9950 - Val Acc: 0.9418\n",
      "Epoch 45/50 - Train Loss: 2.0444 - Val Loss: 2.4142 - Train Acc: 0.9960 - Val Acc: 0.9491\n",
      "Epoch 46/50 - Train Loss: 2.0120 - Val Loss: 2.3828 - Train Acc: 0.9955 - Val Acc: 0.9461\n",
      "Epoch 47/50 - Train Loss: 1.9500 - Val Loss: 2.4058 - Train Acc: 0.9960 - Val Acc: 0.9476\n",
      "Epoch 48/50 - Train Loss: 1.9649 - Val Loss: 2.3956 - Train Acc: 0.9957 - Val Acc: 0.9476\n",
      "Epoch 49/50 - Train Loss: 1.9256 - Val Loss: 2.3965 - Train Acc: 0.9962 - Val Acc: 0.9476\n",
      "Epoch 50/50 - Train Loss: 1.8983 - Val Loss: 2.3965 - Train Acc: 0.9965 - Val Acc: 0.9476\n",
      "✅ Finished Td=60, Tw=10 — saved to /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw10.pth\n",
      "\n",
      "🚀 Running for Td=60, Tw=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/2461672243.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Class Weights Tensor:\n",
      "tensor([2.4834, 2.4834, 2.4834, 2.4834, 2.4834, 2.4834, 2.4834, 2.4834, 2.4834,\n",
      "        2.4834, 2.4834, 2.4834], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ea301b/anaconda3/envs/dl_satang/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 189.2689 - Val Loss: 14.6601 - Train Acc: 0.4937 - Val Acc: 0.5823\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw15.pth\n",
      "Epoch 2/50 - Train Loss: 83.4394 - Val Loss: 11.6205 - Train Acc: 0.7614 - Val Acc: 0.6468\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw15.pth\n",
      "Epoch 3/50 - Train Loss: 61.9773 - Val Loss: 9.7807 - Train Acc: 0.8217 - Val Acc: 0.7051\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw15.pth\n",
      "Epoch 4/50 - Train Loss: 50.6494 - Val Loss: 8.8200 - Train Acc: 0.8519 - Val Acc: 0.7582\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw15.pth\n",
      "Epoch 5/50 - Train Loss: 44.5759 - Val Loss: 8.8688 - Train Acc: 0.8695 - Val Acc: 0.7646\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw15.pth\n",
      "Epoch 6/50 - Train Loss: 41.2716 - Val Loss: 7.8235 - Train Acc: 0.8758 - Val Acc: 0.7544\n",
      "Epoch 7/50 - Train Loss: 35.7630 - Val Loss: 9.2328 - Train Acc: 0.8951 - Val Acc: 0.7316\n",
      "Epoch 8/50 - Train Loss: 34.3403 - Val Loss: 7.8048 - Train Acc: 0.8994 - Val Acc: 0.7835\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw15.pth\n",
      "Epoch 9/50 - Train Loss: 33.5122 - Val Loss: 6.2898 - Train Acc: 0.9039 - Val Acc: 0.8013\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw15.pth\n",
      "Epoch 10/50 - Train Loss: 29.7118 - Val Loss: 5.5418 - Train Acc: 0.9143 - Val Acc: 0.8494\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw15.pth\n",
      "Epoch 11/50 - Train Loss: 28.1756 - Val Loss: 4.7605 - Train Acc: 0.9173 - Val Acc: 0.8443\n",
      "Epoch 12/50 - Train Loss: 27.5317 - Val Loss: 4.9756 - Train Acc: 0.9217 - Val Acc: 0.8430\n",
      "Epoch 13/50 - Train Loss: 25.2014 - Val Loss: 4.9876 - Train Acc: 0.9298 - Val Acc: 0.8405\n",
      "Epoch 14/50 - Train Loss: 23.4765 - Val Loss: 4.3346 - Train Acc: 0.9318 - Val Acc: 0.8734\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw15.pth\n",
      "Epoch 15/50 - Train Loss: 21.0584 - Val Loss: 5.4800 - Train Acc: 0.9405 - Val Acc: 0.8494\n",
      "Epoch 16/50 - Train Loss: 20.5328 - Val Loss: 5.0602 - Train Acc: 0.9442 - Val Acc: 0.8646\n",
      "Epoch 17/50 - Train Loss: 18.9872 - Val Loss: 4.5505 - Train Acc: 0.9490 - Val Acc: 0.8696\n",
      "Epoch 18/50 - Train Loss: 19.9892 - Val Loss: 4.6484 - Train Acc: 0.9430 - Val Acc: 0.8911\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw15.pth\n",
      "Epoch 19/50 - Train Loss: 18.0598 - Val Loss: 5.0365 - Train Acc: 0.9504 - Val Acc: 0.8734\n",
      "Epoch 20/50 - Train Loss: 14.9550 - Val Loss: 3.5573 - Train Acc: 0.9589 - Val Acc: 0.9063\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw15.pth\n",
      "Epoch 21/50 - Train Loss: 15.1512 - Val Loss: 3.8915 - Train Acc: 0.9565 - Val Acc: 0.9013\n",
      "Epoch 22/50 - Train Loss: 13.1566 - Val Loss: 3.9471 - Train Acc: 0.9632 - Val Acc: 0.8949\n",
      "Epoch 23/50 - Train Loss: 13.6177 - Val Loss: 3.7092 - Train Acc: 0.9613 - Val Acc: 0.9013\n",
      "Epoch 24/50 - Train Loss: 12.7692 - Val Loss: 4.3970 - Train Acc: 0.9649 - Val Acc: 0.8937\n",
      "Epoch 25/50 - Train Loss: 12.4765 - Val Loss: 3.7509 - Train Acc: 0.9665 - Val Acc: 0.9228\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw15.pth\n",
      "Epoch 26/50 - Train Loss: 11.2568 - Val Loss: 3.7645 - Train Acc: 0.9690 - Val Acc: 0.8911\n",
      "Epoch 27/50 - Train Loss: 11.2445 - Val Loss: 4.2171 - Train Acc: 0.9682 - Val Acc: 0.9051\n",
      "Epoch 28/50 - Train Loss: 10.7630 - Val Loss: 4.4071 - Train Acc: 0.9689 - Val Acc: 0.8886\n",
      "Epoch 29/50 - Train Loss: 10.3091 - Val Loss: 4.1982 - Train Acc: 0.9723 - Val Acc: 0.8886\n",
      "Epoch 30/50 - Train Loss: 9.9313 - Val Loss: 3.6976 - Train Acc: 0.9733 - Val Acc: 0.9013\n",
      "Epoch 31/50 - Train Loss: 9.2338 - Val Loss: 4.2331 - Train Acc: 0.9753 - Val Acc: 0.8975\n",
      "Epoch 32/50 - Train Loss: 8.3180 - Val Loss: 3.4928 - Train Acc: 0.9762 - Val Acc: 0.9127\n",
      "Epoch 33/50 - Train Loss: 8.2349 - Val Loss: 3.8253 - Train Acc: 0.9791 - Val Acc: 0.8987\n",
      "Epoch 34/50 - Train Loss: 7.5495 - Val Loss: 4.8508 - Train Acc: 0.9787 - Val Acc: 0.8975\n",
      "Epoch 35/50 - Train Loss: 7.5963 - Val Loss: 3.1080 - Train Acc: 0.9797 - Val Acc: 0.9114\n",
      "Epoch 36/50 - Train Loss: 7.2362 - Val Loss: 3.7551 - Train Acc: 0.9815 - Val Acc: 0.9025\n",
      "Epoch 37/50 - Train Loss: 6.5641 - Val Loss: 3.7357 - Train Acc: 0.9828 - Val Acc: 0.9177\n",
      "Epoch 38/50 - Train Loss: 6.2921 - Val Loss: 3.4197 - Train Acc: 0.9843 - Val Acc: 0.9165\n",
      "Epoch 39/50 - Train Loss: 6.2847 - Val Loss: 4.0119 - Train Acc: 0.9838 - Val Acc: 0.9152\n",
      "Epoch 40/50 - Train Loss: 5.9540 - Val Loss: 3.6016 - Train Acc: 0.9846 - Val Acc: 0.9076\n",
      "Epoch 41/50 - Train Loss: 5.7444 - Val Loss: 4.0864 - Train Acc: 0.9856 - Val Acc: 0.9076\n",
      "Epoch 42/50 - Train Loss: 5.5638 - Val Loss: 3.8069 - Train Acc: 0.9861 - Val Acc: 0.9139\n",
      "Epoch 43/50 - Train Loss: 5.3307 - Val Loss: 3.6904 - Train Acc: 0.9866 - Val Acc: 0.9076\n",
      "Epoch 44/50 - Train Loss: 5.4049 - Val Loss: 3.8224 - Train Acc: 0.9868 - Val Acc: 0.9139\n",
      "Epoch 45/50 - Train Loss: 5.0138 - Val Loss: 3.7723 - Train Acc: 0.9873 - Val Acc: 0.9101\n",
      "Epoch 46/50 - Train Loss: 5.2019 - Val Loss: 3.7706 - Train Acc: 0.9865 - Val Acc: 0.9101\n",
      "Epoch 47/50 - Train Loss: 5.0364 - Val Loss: 3.8048 - Train Acc: 0.9876 - Val Acc: 0.9114\n",
      "Epoch 48/50 - Train Loss: 4.8676 - Val Loss: 3.7685 - Train Acc: 0.9880 - Val Acc: 0.9139\n",
      "Epoch 49/50 - Train Loss: 4.8690 - Val Loss: 3.7738 - Train Acc: 0.9883 - Val Acc: 0.9127\n",
      "Epoch 50/50 - Train Loss: 5.0126 - Val Loss: 3.7738 - Train Acc: 0.9885 - Val Acc: 0.9127\n",
      "✅ Finished Td=60, Tw=15 — saved to /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw15.pth\n",
      "\n",
      "🚀 Running for Td=60, Tw=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/2461672243.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Class Weights Tensor:\n",
      "tensor([2.4835, 2.4835, 2.4835, 2.4835, 2.4835, 2.4835, 2.4835, 2.4835, 2.4835,\n",
      "        2.4835, 2.4835, 2.4835], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ea301b/anaconda3/envs/dl_satang/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 215.0630 - Val Loss: 16.3670 - Train Acc: 0.4426 - Val Acc: 0.5316\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw20.pth\n",
      "Epoch 2/50 - Train Loss: 92.5499 - Val Loss: 11.1087 - Train Acc: 0.7667 - Val Acc: 0.6698\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw20.pth\n",
      "Epoch 3/50 - Train Loss: 66.9137 - Val Loss: 8.6636 - Train Acc: 0.8243 - Val Acc: 0.7518\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw20.pth\n",
      "Epoch 4/50 - Train Loss: 55.2634 - Val Loss: 7.6946 - Train Acc: 0.8544 - Val Acc: 0.7904\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw20.pth\n",
      "Epoch 5/50 - Train Loss: 45.9841 - Val Loss: 8.2227 - Train Acc: 0.8823 - Val Acc: 0.7506\n",
      "Epoch 6/50 - Train Loss: 43.4650 - Val Loss: 7.7386 - Train Acc: 0.8857 - Val Acc: 0.7799\n",
      "Epoch 7/50 - Train Loss: 38.4572 - Val Loss: 7.3944 - Train Acc: 0.9004 - Val Acc: 0.8068\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw20.pth\n",
      "Epoch 8/50 - Train Loss: 34.1093 - Val Loss: 5.9326 - Train Acc: 0.9136 - Val Acc: 0.8489\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw20.pth\n",
      "Epoch 9/50 - Train Loss: 33.7313 - Val Loss: 5.8371 - Train Acc: 0.9138 - Val Acc: 0.8396\n",
      "Epoch 10/50 - Train Loss: 31.0782 - Val Loss: 6.6308 - Train Acc: 0.9210 - Val Acc: 0.8126\n",
      "Epoch 11/50 - Train Loss: 28.1455 - Val Loss: 5.2617 - Train Acc: 0.9308 - Val Acc: 0.8642\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw20.pth\n",
      "Epoch 12/50 - Train Loss: 28.5660 - Val Loss: 5.6491 - Train Acc: 0.9276 - Val Acc: 0.8419\n",
      "Epoch 13/50 - Train Loss: 26.3375 - Val Loss: 5.4700 - Train Acc: 0.9329 - Val Acc: 0.8712\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw20.pth\n",
      "Epoch 14/50 - Train Loss: 26.5031 - Val Loss: 5.7736 - Train Acc: 0.9336 - Val Acc: 0.8618\n",
      "Epoch 15/50 - Train Loss: 23.1296 - Val Loss: 6.4219 - Train Acc: 0.9440 - Val Acc: 0.8279\n",
      "Epoch 16/50 - Train Loss: 23.4327 - Val Loss: 6.7100 - Train Acc: 0.9431 - Val Acc: 0.8443\n",
      "Epoch 17/50 - Train Loss: 22.9641 - Val Loss: 5.1484 - Train Acc: 0.9425 - Val Acc: 0.8735\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw20.pth\n",
      "Epoch 18/50 - Train Loss: 22.3386 - Val Loss: 5.5283 - Train Acc: 0.9429 - Val Acc: 0.8454\n",
      "Epoch 19/50 - Train Loss: 19.6192 - Val Loss: 4.6334 - Train Acc: 0.9543 - Val Acc: 0.8829\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw20.pth\n",
      "Epoch 20/50 - Train Loss: 20.2313 - Val Loss: 5.3602 - Train Acc: 0.9501 - Val Acc: 0.8642\n",
      "Epoch 21/50 - Train Loss: 19.2587 - Val Loss: 4.7719 - Train Acc: 0.9533 - Val Acc: 0.8899\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw20.pth\n",
      "Epoch 22/50 - Train Loss: 18.7979 - Val Loss: 4.4880 - Train Acc: 0.9520 - Val Acc: 0.8876\n",
      "Epoch 23/50 - Train Loss: 18.6827 - Val Loss: 5.1727 - Train Acc: 0.9548 - Val Acc: 0.8689\n",
      "Epoch 24/50 - Train Loss: 16.8140 - Val Loss: 4.6452 - Train Acc: 0.9586 - Val Acc: 0.8958\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw20.pth\n",
      "Epoch 25/50 - Train Loss: 17.3057 - Val Loss: 5.1888 - Train Acc: 0.9569 - Val Acc: 0.8970\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw20.pth\n",
      "Epoch 26/50 - Train Loss: 17.1937 - Val Loss: 4.4659 - Train Acc: 0.9570 - Val Acc: 0.9075\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw20.pth\n",
      "Epoch 27/50 - Train Loss: 14.4712 - Val Loss: 4.1544 - Train Acc: 0.9672 - Val Acc: 0.8852\n",
      "Epoch 28/50 - Train Loss: 14.5183 - Val Loss: 4.6800 - Train Acc: 0.9670 - Val Acc: 0.8712\n",
      "Epoch 29/50 - Train Loss: 13.6053 - Val Loss: 3.6892 - Train Acc: 0.9661 - Val Acc: 0.9016\n",
      "Epoch 30/50 - Train Loss: 12.7339 - Val Loss: 3.5626 - Train Acc: 0.9682 - Val Acc: 0.9122\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw20.pth\n",
      "Epoch 31/50 - Train Loss: 12.7610 - Val Loss: 4.9919 - Train Acc: 0.9686 - Val Acc: 0.8794\n",
      "Epoch 32/50 - Train Loss: 12.9898 - Val Loss: 3.3456 - Train Acc: 0.9688 - Val Acc: 0.9239\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw20.pth\n",
      "Epoch 33/50 - Train Loss: 11.4836 - Val Loss: 4.0775 - Train Acc: 0.9719 - Val Acc: 0.9075\n",
      "Epoch 34/50 - Train Loss: 10.8496 - Val Loss: 3.5993 - Train Acc: 0.9750 - Val Acc: 0.9122\n",
      "Epoch 35/50 - Train Loss: 10.1649 - Val Loss: 3.6026 - Train Acc: 0.9751 - Val Acc: 0.9239\n",
      "Epoch 36/50 - Train Loss: 10.0267 - Val Loss: 3.5609 - Train Acc: 0.9760 - Val Acc: 0.9192\n",
      "Epoch 37/50 - Train Loss: 9.5812 - Val Loss: 4.4228 - Train Acc: 0.9769 - Val Acc: 0.9005\n",
      "Epoch 38/50 - Train Loss: 9.2984 - Val Loss: 3.2623 - Train Acc: 0.9773 - Val Acc: 0.9215\n",
      "Epoch 39/50 - Train Loss: 8.8926 - Val Loss: 3.7862 - Train Acc: 0.9787 - Val Acc: 0.9215\n",
      "Epoch 40/50 - Train Loss: 8.4131 - Val Loss: 3.4101 - Train Acc: 0.9802 - Val Acc: 0.9251\n",
      "💾 Best model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw20.pth\n",
      "Epoch 41/50 - Train Loss: 8.4132 - Val Loss: 3.4994 - Train Acc: 0.9791 - Val Acc: 0.9239\n",
      "Epoch 42/50 - Train Loss: 8.1555 - Val Loss: 3.4486 - Train Acc: 0.9810 - Val Acc: 0.9239\n",
      "Epoch 43/50 - Train Loss: 7.7505 - Val Loss: 3.7372 - Train Acc: 0.9813 - Val Acc: 0.9157\n",
      "Epoch 44/50 - Train Loss: 7.4280 - Val Loss: 3.4882 - Train Acc: 0.9824 - Val Acc: 0.9157\n",
      "Epoch 45/50 - Train Loss: 7.4802 - Val Loss: 3.5252 - Train Acc: 0.9819 - Val Acc: 0.9145\n",
      "Epoch 46/50 - Train Loss: 7.0621 - Val Loss: 3.5607 - Train Acc: 0.9847 - Val Acc: 0.9169\n",
      "Epoch 47/50 - Train Loss: 7.3781 - Val Loss: 3.5252 - Train Acc: 0.9812 - Val Acc: 0.9169\n",
      "Epoch 48/50 - Train Loss: 7.0968 - Val Loss: 3.5542 - Train Acc: 0.9826 - Val Acc: 0.9169\n",
      "Epoch 49/50 - Train Loss: 7.0810 - Val Loss: 3.5741 - Train Acc: 0.9827 - Val Acc: 0.9157\n",
      "Epoch 50/50 - Train Loss: 7.1232 - Val Loss: 3.5741 - Train Acc: 0.9833 - Val Acc: 0.9157\n",
      "✅ Finished Td=60, Tw=20 — saved to /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher/mlpmixer_Td60_Tw20.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/2461672243.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# === 🧩 Config ===\n",
    "NUM_EPOCHS = 50\n",
    "BEST_LR = 0.01\n",
    "NUM_FEATURES = 8  # Assuming you use 8D input\n",
    "BATCH_SIZE = 64\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "base_dir = \"/home/HardDisk/Satang/thesis_proj\"\n",
    "save_root = \"/home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/teacher\"\n",
    "os.makedirs(save_root, exist_ok=True)\n",
    "\n",
    "detection_times = [30, 45, 60]\n",
    "window_sizes = [10, 15, 20]\n",
    "\n",
    "for T_d in detection_times:\n",
    "    for T_w in window_sizes:\n",
    "        print(f\"\\n🚀 Running for Td={T_d}, Tw={T_w}\")\n",
    "        model_name = os.path.join(save_root, f\"mlpmixer_Td{T_d}_Tw{T_w}.pth\")\n",
    "        T_len = T_d - T_w + 1\n",
    "        folder_name = f\"X_csv_split_{T_len}\"\n",
    "\n",
    "        input_dir = os.path.join(base_dir, f\"New_{T_d}\", f\"{T_w}\", \"split_tws\", folder_name)\n",
    "        train_path = os.path.join(input_dir, \"train\")\n",
    "        val_path   = os.path.join(input_dir, \"val\")\n",
    "\n",
    "        # === 1. Load Data ===\n",
    "        expected_shape = (T_len, NUM_FEATURES)\n",
    "        X_train_raw, y_train_raw = load_split_from_folder(train_path, expected_shape)\n",
    "        X_val_raw, y_val_raw     = load_split_from_folder(val_path, expected_shape)\n",
    "\n",
    "        # === 2. SMOTE + Encode ===\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_train_encoded = label_encoder.fit_transform(y_train_raw)\n",
    "        X_train_flat = X_train_raw.reshape(X_train_raw.shape[0], -1)\n",
    "        X_resampled, y_resampled = SMOTE().fit_resample(X_train_flat, y_train_encoded)\n",
    "        X_train_bal = X_resampled.reshape(-1, expected_shape[0], NUM_FEATURES)\n",
    "        y_train_str = label_encoder.inverse_transform(y_resampled)\n",
    "\n",
    "        # === 3. Datasets + Loaders ===\n",
    "        train_dataset = MultiStreamDataset(X_train_bal, y_train_str, label_encoder, augment=True)\n",
    "        val_dataset   = MultiStreamDataset(X_val_raw, y_val_raw, label_encoder, augment=False)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # === 4. MLP-Mixer Model ===\n",
    "        model = MLPMixerRansomwareClassifier(\n",
    "            seq_len=expected_shape[0],\n",
    "            feature_dim=NUM_FEATURES,\n",
    "            num_classes=len(label_encoder.classes_),\n",
    "            hidden_dim=128\n",
    "        ).to(device)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=BEST_LR, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "        class_weights_tensor = compute_class_weights(label_encoder.transform(y_train_str), device)\n",
    "\n",
    "        # === 5. Train MLP-Mixer ===\n",
    "        train_accs, val_accs, train_losses, val_losses = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            device=device,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            class_weights=class_weights_tensor,\n",
    "            lr=BEST_LR,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            best_model_path=model_name\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Finished Td={T_d}, Tw={T_w} — saved to {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1ce127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class StudentCNN(nn.Module):\n",
    "    def __init__(self, input_length, num_classes=12):\n",
    "        super().__init__()\n",
    "        self.streams = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(1, 4, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(4, 8, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool1d(1),\n",
    "                nn.Flatten()\n",
    "            ) for _ in range(8)\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(8 * 8, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "        self.proj = nn.Linear(8 * 8, 128)\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        B, T, C = x.shape\n",
    "        assert C == 8, f\"Expected 8 feature streams, got {C}\"\n",
    "\n",
    "        # Split into 8 streams (B, 1, T) → process → (B, 8)\n",
    "        streams = [x[:, :, i].unsqueeze(1) for i in range(C)]\n",
    "        features = [self.streams[i](streams[i]) for i in range(8)]\n",
    "\n",
    "        x = torch.cat(features, dim=1)  # (B, 64)\n",
    "\n",
    "\n",
    "\n",
    "        if return_features:\n",
    "            feat_proj = self.proj(x)      # (B, 128)\n",
    "            logits = self.fc(x)           # (B, num_classes)\n",
    "            return logits, feat_proj\n",
    "        else:\n",
    "            return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dad2ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, student_feat, teacher_feat, true_labels, T=3.0, alpha=0.5, beta=0.3, gamma=0.2):\n",
    "    \"\"\"\n",
    "    alpha: weight for hard loss (CE)\n",
    "    beta: weight for soft loss (KL)\n",
    "    gamma: weight for feature-based distillation (MSE)\n",
    "    T: temperature for soft distillation\n",
    "    \"\"\"\n",
    "    # Hard loss\n",
    "    ce_loss = F.cross_entropy(student_logits, true_labels)\n",
    "\n",
    "    # Soft loss (logits)\n",
    "    soft_teacher = F.softmax(teacher_logits / T, dim=1)\n",
    "    soft_student = F.log_softmax(student_logits / T, dim=1)\n",
    "    kl_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean') * (T * T)\n",
    "\n",
    "    # Feature loss (projection-matched MSE)\n",
    "    feat_loss = F.mse_loss(student_feat, teacher_feat)\n",
    "\n",
    "    return alpha * ce_loss + beta * kl_loss + gamma * feat_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "295b7d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_distilled(student, teacher, train_loader, val_loader, device,\n",
    "                    epochs=50, lr=0.0005, class_weights=None,\n",
    "                    T=3.0, alpha=0.5, beta=0.3, gamma=0.2,\n",
    "                    save_path=\"best_student.pth\"):\n",
    "\n",
    "    teacher.eval()\n",
    "    student.to(device)\n",
    "    teacher.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(student.parameters(), lr=lr)\n",
    "    ce_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    train_losses, train_accuracies, val_losses, val_accuracies = [], [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        student.train()\n",
    "        total_loss, correct = 0.0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # === Forward passes (both must return logits, features)\n",
    "            student_logits, student_feat = student(inputs, return_features=True)\n",
    "            with torch.no_grad():\n",
    "                teacher_logits, teacher_feat = teacher(inputs, return_features=True)\n",
    "\n",
    "            # === KD loss\n",
    "            loss = distillation_loss(\n",
    "                student_logits, teacher_logits,\n",
    "                student_feat, teacher_feat,\n",
    "                labels,\n",
    "                T=T, alpha=alpha, beta=beta, gamma=gamma\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (student_logits.argmax(1) == labels).sum().item()\n",
    "\n",
    "        train_acc = correct / len(train_loader.dataset)\n",
    "        train_losses.append(total_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        # === Validation (using standard CE loss)\n",
    "        student.eval()\n",
    "        val_loss, val_correct = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                logits = student(inputs)  # ❗ Note: assuming return_features=False in eval\n",
    "                loss = ce_criterion(logits, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_correct += (logits.argmax(1) == labels).sum().item()\n",
    "\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f\"[Distill] Epoch {epoch+1:02d}/{epochs} - \"\n",
    "              f\"Loss: {total_loss:.4f} - Train Acc: {train_acc:.4f} - Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(student.state_dict(), save_path)\n",
    "            print(f\"💾 Best student model saved to: {save_path}\")\n",
    "\n",
    "    student.load_state_dict(torch.load(save_path))\n",
    "    return train_accuracies, val_accuracies, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc2948db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Distilling MLP-Mixer → StudentCNN for Td=30, Tw=10 (T_len=21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/4218594340.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  teacher.load_state_dict(torch.load(teacher_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Distill] Epoch 01/50 - Loss: 103172.3219 - Train Acc: 0.4912 - Val Acc: 0.5541\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw10.pth\n",
      "[Distill] Epoch 02/50 - Loss: 51221.1766 - Train Acc: 0.6715 - Val Acc: 0.5902\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw10.pth\n",
      "[Distill] Epoch 03/50 - Loss: 43765.6398 - Train Acc: 0.6981 - Val Acc: 0.6358\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw10.pth\n",
      "[Distill] Epoch 04/50 - Loss: 41058.7328 - Train Acc: 0.7351 - Val Acc: 0.6478\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw10.pth\n",
      "[Distill] Epoch 05/50 - Loss: 39624.7720 - Train Acc: 0.7501 - Val Acc: 0.6498\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw10.pth\n",
      "[Distill] Epoch 06/50 - Loss: 38847.8469 - Train Acc: 0.7674 - Val Acc: 0.6458\n",
      "[Distill] Epoch 07/50 - Loss: 38315.5063 - Train Acc: 0.7780 - Val Acc: 0.7239\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw10.pth\n",
      "[Distill] Epoch 08/50 - Loss: 37882.7955 - Train Acc: 0.7885 - Val Acc: 0.7500\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw10.pth\n",
      "[Distill] Epoch 09/50 - Loss: 37714.5372 - Train Acc: 0.7953 - Val Acc: 0.7350\n",
      "[Distill] Epoch 10/50 - Loss: 37498.0655 - Train Acc: 0.7987 - Val Acc: 0.7700\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw10.pth\n",
      "[Distill] Epoch 11/50 - Loss: 37337.8553 - Train Acc: 0.8052 - Val Acc: 0.7695\n",
      "[Distill] Epoch 12/50 - Loss: 37228.9669 - Train Acc: 0.8050 - Val Acc: 0.7911\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw10.pth\n",
      "[Distill] Epoch 13/50 - Loss: 37141.7255 - Train Acc: 0.8182 - Val Acc: 0.7415\n",
      "[Distill] Epoch 14/50 - Loss: 36957.6889 - Train Acc: 0.8242 - Val Acc: 0.7776\n",
      "[Distill] Epoch 15/50 - Loss: 36832.2828 - Train Acc: 0.8144 - Val Acc: 0.8201\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw10.pth\n",
      "[Distill] Epoch 16/50 - Loss: 36746.6172 - Train Acc: 0.8205 - Val Acc: 0.7756\n",
      "[Distill] Epoch 17/50 - Loss: 36678.9404 - Train Acc: 0.8265 - Val Acc: 0.7570\n",
      "[Distill] Epoch 18/50 - Loss: 36564.1308 - Train Acc: 0.8248 - Val Acc: 0.7610\n",
      "[Distill] Epoch 19/50 - Loss: 36369.8664 - Train Acc: 0.8366 - Val Acc: 0.8216\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw10.pth\n",
      "[Distill] Epoch 20/50 - Loss: 36364.2127 - Train Acc: 0.8350 - Val Acc: 0.7841\n",
      "[Distill] Epoch 21/50 - Loss: 36128.3554 - Train Acc: 0.8302 - Val Acc: 0.7821\n",
      "[Distill] Epoch 22/50 - Loss: 35095.8931 - Train Acc: 0.8304 - Val Acc: 0.7801\n",
      "[Distill] Epoch 23/50 - Loss: 32509.4415 - Train Acc: 0.8363 - Val Acc: 0.7685\n",
      "[Distill] Epoch 24/50 - Loss: 30754.1789 - Train Acc: 0.8387 - Val Acc: 0.8186\n",
      "[Distill] Epoch 25/50 - Loss: 29148.5267 - Train Acc: 0.8459 - Val Acc: 0.8141\n",
      "[Distill] Epoch 26/50 - Loss: 27512.0333 - Train Acc: 0.8328 - Val Acc: 0.7881\n",
      "[Distill] Epoch 27/50 - Loss: 26929.2966 - Train Acc: 0.8266 - Val Acc: 0.7801\n",
      "[Distill] Epoch 28/50 - Loss: 26579.3491 - Train Acc: 0.8357 - Val Acc: 0.8191\n",
      "[Distill] Epoch 29/50 - Loss: 26253.3839 - Train Acc: 0.8375 - Val Acc: 0.8161\n",
      "[Distill] Epoch 30/50 - Loss: 25916.2735 - Train Acc: 0.8398 - Val Acc: 0.7986\n",
      "[Distill] Epoch 31/50 - Loss: 25790.7458 - Train Acc: 0.8426 - Val Acc: 0.7740\n",
      "[Distill] Epoch 32/50 - Loss: 25618.4624 - Train Acc: 0.8383 - Val Acc: 0.8277\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw10.pth\n",
      "[Distill] Epoch 33/50 - Loss: 25469.3943 - Train Acc: 0.8452 - Val Acc: 0.7926\n",
      "[Distill] Epoch 34/50 - Loss: 25236.2620 - Train Acc: 0.8476 - Val Acc: 0.8111\n",
      "[Distill] Epoch 35/50 - Loss: 25091.0978 - Train Acc: 0.8518 - Val Acc: 0.7946\n",
      "[Distill] Epoch 36/50 - Loss: 24906.8756 - Train Acc: 0.8534 - Val Acc: 0.7901\n",
      "[Distill] Epoch 37/50 - Loss: 24735.6097 - Train Acc: 0.8564 - Val Acc: 0.8417\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw10.pth\n",
      "[Distill] Epoch 38/50 - Loss: 24476.6445 - Train Acc: 0.8573 - Val Acc: 0.8427\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw10.pth\n",
      "[Distill] Epoch 39/50 - Loss: 24249.3731 - Train Acc: 0.8546 - Val Acc: 0.8292\n",
      "[Distill] Epoch 40/50 - Loss: 23971.6968 - Train Acc: 0.8528 - Val Acc: 0.8106\n",
      "[Distill] Epoch 41/50 - Loss: 23917.2405 - Train Acc: 0.8563 - Val Acc: 0.8081\n",
      "[Distill] Epoch 42/50 - Loss: 23484.2856 - Train Acc: 0.8574 - Val Acc: 0.8211\n",
      "[Distill] Epoch 43/50 - Loss: 23078.7160 - Train Acc: 0.8662 - Val Acc: 0.8457\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw10.pth\n",
      "[Distill] Epoch 44/50 - Loss: 22768.0985 - Train Acc: 0.8663 - Val Acc: 0.8412\n",
      "[Distill] Epoch 45/50 - Loss: 22586.3825 - Train Acc: 0.8711 - Val Acc: 0.8372\n",
      "[Distill] Epoch 46/50 - Loss: 22358.5320 - Train Acc: 0.8694 - Val Acc: 0.8046\n",
      "[Distill] Epoch 47/50 - Loss: 22201.4004 - Train Acc: 0.8679 - Val Acc: 0.8221\n",
      "[Distill] Epoch 48/50 - Loss: 21874.0877 - Train Acc: 0.8687 - Val Acc: 0.8141\n",
      "[Distill] Epoch 49/50 - Loss: 21655.6799 - Train Acc: 0.8730 - Val Acc: 0.8507\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw10.pth\n",
      "[Distill] Epoch 50/50 - Loss: 21497.8358 - Train Acc: 0.8800 - Val Acc: 0.8432\n",
      "\n",
      "📊 Final Classification Report (StudentCNN):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/3282436521.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student.load_state_dict(torch.load(save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    AESCrypt       1.00      1.00      1.00        85\n",
      "      Cerber       0.73      0.77      0.75       189\n",
      "    Darkside       0.96      0.75      0.85       323\n",
      "       Excel       1.00      1.00      1.00       147\n",
      "     Firefox       1.00      0.99      1.00       204\n",
      "   GandCrab4       0.71      0.78      0.74       319\n",
      "        Ryuk       0.76      0.77      0.76       196\n",
      "     SDelete       1.00      1.00      1.00        79\n",
      "  Sodinokibi       0.79      0.73      0.76       205\n",
      "  TeslaCrypt       0.65      1.00      0.79        85\n",
      "    WannaCry       1.00      1.00      1.00        79\n",
      "         Zip       1.00      1.00      1.00        85\n",
      "\n",
      "    accuracy                           0.85      1996\n",
      "   macro avg       0.88      0.90      0.89      1996\n",
      "weighted avg       0.86      0.85      0.85      1996\n",
      "\n",
      "✅ Student model saved: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw10.pth\n",
      "\n",
      "🚀 Distilling MLP-Mixer → StudentCNN for Td=30, Tw=15 (T_len=16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/4218594340.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  teacher.load_state_dict(torch.load(teacher_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Distill] Epoch 01/50 - Loss: 228792.3068 - Train Acc: 0.5144 - Val Acc: 0.6595\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw15.pth\n",
      "[Distill] Epoch 02/50 - Loss: 120378.6178 - Train Acc: 0.7307 - Val Acc: 0.7039\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw15.pth\n",
      "[Distill] Epoch 03/50 - Loss: 109528.4743 - Train Acc: 0.7721 - Val Acc: 0.7020\n",
      "[Distill] Epoch 04/50 - Loss: 103541.0109 - Train Acc: 0.7985 - Val Acc: 0.7301\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw15.pth\n",
      "[Distill] Epoch 05/50 - Loss: 100620.9624 - Train Acc: 0.8126 - Val Acc: 0.7479\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw15.pth\n",
      "[Distill] Epoch 06/50 - Loss: 98312.2857 - Train Acc: 0.8203 - Val Acc: 0.7384\n",
      "[Distill] Epoch 07/50 - Loss: 95383.1128 - Train Acc: 0.8294 - Val Acc: 0.7570\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw15.pth\n",
      "[Distill] Epoch 08/50 - Loss: 91605.1261 - Train Acc: 0.8409 - Val Acc: 0.7301\n",
      "[Distill] Epoch 09/50 - Loss: 87933.9520 - Train Acc: 0.8456 - Val Acc: 0.7874\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw15.pth\n",
      "[Distill] Epoch 10/50 - Loss: 85043.1153 - Train Acc: 0.8523 - Val Acc: 0.7957\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw15.pth\n",
      "[Distill] Epoch 11/50 - Loss: 83775.5486 - Train Acc: 0.8543 - Val Acc: 0.7893\n",
      "[Distill] Epoch 12/50 - Loss: 82900.0334 - Train Acc: 0.8583 - Val Acc: 0.7889\n",
      "[Distill] Epoch 13/50 - Loss: 81981.5877 - Train Acc: 0.8641 - Val Acc: 0.7957\n",
      "[Distill] Epoch 14/50 - Loss: 80873.6791 - Train Acc: 0.8666 - Val Acc: 0.8197\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw15.pth\n",
      "[Distill] Epoch 15/50 - Loss: 79544.6582 - Train Acc: 0.8710 - Val Acc: 0.7832\n",
      "[Distill] Epoch 16/50 - Loss: 78435.5155 - Train Acc: 0.8769 - Val Acc: 0.8090\n",
      "[Distill] Epoch 17/50 - Loss: 77983.4557 - Train Acc: 0.8748 - Val Acc: 0.7935\n",
      "[Distill] Epoch 18/50 - Loss: 77762.8645 - Train Acc: 0.8798 - Val Acc: 0.8212\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw15.pth\n",
      "[Distill] Epoch 19/50 - Loss: 77800.8589 - Train Acc: 0.8805 - Val Acc: 0.8295\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw15.pth\n",
      "[Distill] Epoch 20/50 - Loss: 77385.8046 - Train Acc: 0.8845 - Val Acc: 0.8162\n",
      "[Distill] Epoch 21/50 - Loss: 77175.8480 - Train Acc: 0.8848 - Val Acc: 0.8162\n",
      "[Distill] Epoch 22/50 - Loss: 77152.5899 - Train Acc: 0.8867 - Val Acc: 0.8421\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw15.pth\n",
      "[Distill] Epoch 23/50 - Loss: 76516.1076 - Train Acc: 0.8875 - Val Acc: 0.8303\n",
      "[Distill] Epoch 24/50 - Loss: 76534.1921 - Train Acc: 0.8898 - Val Acc: 0.8204\n",
      "[Distill] Epoch 25/50 - Loss: 76309.7920 - Train Acc: 0.8934 - Val Acc: 0.8292\n",
      "[Distill] Epoch 26/50 - Loss: 76315.2023 - Train Acc: 0.8944 - Val Acc: 0.8292\n",
      "[Distill] Epoch 27/50 - Loss: 76146.2093 - Train Acc: 0.8944 - Val Acc: 0.8360\n",
      "[Distill] Epoch 28/50 - Loss: 75806.4400 - Train Acc: 0.8958 - Val Acc: 0.8383\n",
      "[Distill] Epoch 29/50 - Loss: 75766.2662 - Train Acc: 0.8974 - Val Acc: 0.8326\n",
      "[Distill] Epoch 30/50 - Loss: 75594.6527 - Train Acc: 0.8961 - Val Acc: 0.8652\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw15.pth\n",
      "[Distill] Epoch 31/50 - Loss: 74453.5908 - Train Acc: 0.8978 - Val Acc: 0.8573\n",
      "[Distill] Epoch 32/50 - Loss: 73563.0111 - Train Acc: 0.9032 - Val Acc: 0.8443\n",
      "[Distill] Epoch 33/50 - Loss: 73271.1826 - Train Acc: 0.8991 - Val Acc: 0.8474\n",
      "[Distill] Epoch 34/50 - Loss: 73107.1307 - Train Acc: 0.9015 - Val Acc: 0.8345\n",
      "[Distill] Epoch 35/50 - Loss: 72672.8471 - Train Acc: 0.9047 - Val Acc: 0.8519\n",
      "[Distill] Epoch 36/50 - Loss: 72348.1713 - Train Acc: 0.9004 - Val Acc: 0.8500\n",
      "[Distill] Epoch 37/50 - Loss: 71790.3644 - Train Acc: 0.9037 - Val Acc: 0.8485\n",
      "[Distill] Epoch 38/50 - Loss: 71166.8771 - Train Acc: 0.9056 - Val Acc: 0.8489\n",
      "[Distill] Epoch 39/50 - Loss: 70501.3439 - Train Acc: 0.9069 - Val Acc: 0.8489\n",
      "[Distill] Epoch 40/50 - Loss: 69901.1127 - Train Acc: 0.9062 - Val Acc: 0.8531\n",
      "[Distill] Epoch 41/50 - Loss: 69217.3819 - Train Acc: 0.9099 - Val Acc: 0.8652\n",
      "[Distill] Epoch 42/50 - Loss: 68700.3791 - Train Acc: 0.9084 - Val Acc: 0.8641\n",
      "[Distill] Epoch 43/50 - Loss: 68280.7021 - Train Acc: 0.9089 - Val Acc: 0.8595\n",
      "[Distill] Epoch 44/50 - Loss: 67933.8375 - Train Acc: 0.9088 - Val Acc: 0.8531\n",
      "[Distill] Epoch 45/50 - Loss: 67459.9874 - Train Acc: 0.9118 - Val Acc: 0.8618\n",
      "[Distill] Epoch 46/50 - Loss: 67284.6706 - Train Acc: 0.9124 - Val Acc: 0.8721\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw15.pth\n",
      "[Distill] Epoch 47/50 - Loss: 66698.6551 - Train Acc: 0.9110 - Val Acc: 0.8679\n",
      "[Distill] Epoch 48/50 - Loss: 66170.9554 - Train Acc: 0.9136 - Val Acc: 0.8732\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw15.pth\n",
      "[Distill] Epoch 49/50 - Loss: 65832.4766 - Train Acc: 0.9141 - Val Acc: 0.8762\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw15.pth\n",
      "[Distill] Epoch 50/50 - Loss: 65737.0772 - Train Acc: 0.9154 - Val Acc: 0.8781\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw15.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/3282436521.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student.load_state_dict(torch.load(save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Final Classification Report (StudentCNN):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    AESCrypt       0.97      0.99      0.98       112\n",
      "      Cerber       0.88      0.85      0.87       249\n",
      "    Darkside       0.97      0.83      0.90       427\n",
      "       Excel       1.00      1.00      1.00       194\n",
      "     Firefox       0.97      0.92      0.95       271\n",
      "   GandCrab4       0.72      0.81      0.76       420\n",
      "        Ryuk       0.79      0.69      0.73       258\n",
      "     SDelete       1.00      1.00      1.00       103\n",
      "  Sodinokibi       0.85      0.93      0.89       271\n",
      "  TeslaCrypt       0.69      0.96      0.80       114\n",
      "    WannaCry       1.00      1.00      1.00       103\n",
      "         Zip       1.00      0.96      0.98       112\n",
      "\n",
      "    accuracy                           0.88      2634\n",
      "   macro avg       0.90      0.91      0.90      2634\n",
      "weighted avg       0.89      0.88      0.88      2634\n",
      "\n",
      "✅ Student model saved: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw15.pth\n",
      "\n",
      "🚀 Distilling MLP-Mixer → StudentCNN for Td=30, Tw=20 (T_len=11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/4218594340.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  teacher.load_state_dict(torch.load(teacher_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Distill] Epoch 01/50 - Loss: 1679728.3918 - Train Acc: 0.5655 - Val Acc: 0.5990\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw20.pth\n",
      "[Distill] Epoch 02/50 - Loss: 1008621.4836 - Train Acc: 0.7295 - Val Acc: 0.6384\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw20.pth\n",
      "[Distill] Epoch 03/50 - Loss: 856461.7225 - Train Acc: 0.7724 - Val Acc: 0.6832\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw20.pth\n",
      "[Distill] Epoch 04/50 - Loss: 799379.3102 - Train Acc: 0.8019 - Val Acc: 0.7138\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw20.pth\n",
      "[Distill] Epoch 05/50 - Loss: 769447.9376 - Train Acc: 0.8198 - Val Acc: 0.7723\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw20.pth\n",
      "[Distill] Epoch 06/50 - Loss: 755254.2500 - Train Acc: 0.8273 - Val Acc: 0.7566\n",
      "[Distill] Epoch 07/50 - Loss: 738884.4487 - Train Acc: 0.8317 - Val Acc: 0.7818\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw20.pth\n",
      "[Distill] Epoch 08/50 - Loss: 722609.5737 - Train Acc: 0.8429 - Val Acc: 0.7750\n",
      "[Distill] Epoch 09/50 - Loss: 713380.4273 - Train Acc: 0.8458 - Val Acc: 0.7910\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw20.pth\n",
      "[Distill] Epoch 10/50 - Loss: 706075.9712 - Train Acc: 0.8516 - Val Acc: 0.7841\n",
      "[Distill] Epoch 11/50 - Loss: 698080.2465 - Train Acc: 0.8567 - Val Acc: 0.8161\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw20.pth\n",
      "[Distill] Epoch 12/50 - Loss: 693806.9595 - Train Acc: 0.8602 - Val Acc: 0.7857\n",
      "[Distill] Epoch 13/50 - Loss: 688962.7974 - Train Acc: 0.8629 - Val Acc: 0.8038\n",
      "[Distill] Epoch 14/50 - Loss: 684293.5603 - Train Acc: 0.8687 - Val Acc: 0.8002\n",
      "[Distill] Epoch 15/50 - Loss: 677837.3187 - Train Acc: 0.8705 - Val Acc: 0.8237\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw20.pth\n",
      "[Distill] Epoch 16/50 - Loss: 673810.4609 - Train Acc: 0.8709 - Val Acc: 0.8163\n",
      "[Distill] Epoch 17/50 - Loss: 668040.7162 - Train Acc: 0.8748 - Val Acc: 0.8277\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw20.pth\n",
      "[Distill] Epoch 18/50 - Loss: 666167.1919 - Train Acc: 0.8745 - Val Acc: 0.8428\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw20.pth\n",
      "[Distill] Epoch 19/50 - Loss: 665127.1044 - Train Acc: 0.8776 - Val Acc: 0.8126\n",
      "[Distill] Epoch 20/50 - Loss: 663448.1618 - Train Acc: 0.8823 - Val Acc: 0.8402\n",
      "[Distill] Epoch 21/50 - Loss: 661731.4595 - Train Acc: 0.8799 - Val Acc: 0.8199\n",
      "[Distill] Epoch 22/50 - Loss: 658929.7588 - Train Acc: 0.8813 - Val Acc: 0.8268\n",
      "[Distill] Epoch 23/50 - Loss: 658028.2978 - Train Acc: 0.8828 - Val Acc: 0.8113\n",
      "[Distill] Epoch 24/50 - Loss: 657119.8398 - Train Acc: 0.8804 - Val Acc: 0.8147\n",
      "[Distill] Epoch 25/50 - Loss: 654630.4617 - Train Acc: 0.8792 - Val Acc: 0.8040\n",
      "[Distill] Epoch 26/50 - Loss: 652288.4977 - Train Acc: 0.8762 - Val Acc: 0.8300\n",
      "[Distill] Epoch 27/50 - Loss: 651618.8783 - Train Acc: 0.8785 - Val Acc: 0.8208\n",
      "[Distill] Epoch 28/50 - Loss: 650947.6327 - Train Acc: 0.8785 - Val Acc: 0.8294\n",
      "[Distill] Epoch 29/50 - Loss: 649599.9846 - Train Acc: 0.8811 - Val Acc: 0.8268\n",
      "[Distill] Epoch 30/50 - Loss: 647941.7415 - Train Acc: 0.8859 - Val Acc: 0.8191\n",
      "[Distill] Epoch 31/50 - Loss: 646197.2722 - Train Acc: 0.8894 - Val Acc: 0.8394\n",
      "[Distill] Epoch 32/50 - Loss: 642824.9201 - Train Acc: 0.8886 - Val Acc: 0.8535\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw20.pth\n",
      "[Distill] Epoch 33/50 - Loss: 642103.6365 - Train Acc: 0.8890 - Val Acc: 0.8428\n",
      "[Distill] Epoch 34/50 - Loss: 639267.4305 - Train Acc: 0.8939 - Val Acc: 0.8407\n",
      "[Distill] Epoch 35/50 - Loss: 638219.0602 - Train Acc: 0.8947 - Val Acc: 0.8480\n",
      "[Distill] Epoch 36/50 - Loss: 636978.8807 - Train Acc: 0.8945 - Val Acc: 0.8467\n",
      "[Distill] Epoch 37/50 - Loss: 635908.1267 - Train Acc: 0.8950 - Val Acc: 0.8270\n",
      "[Distill] Epoch 38/50 - Loss: 635754.8428 - Train Acc: 0.8964 - Val Acc: 0.8518\n",
      "[Distill] Epoch 39/50 - Loss: 632672.2117 - Train Acc: 0.8986 - Val Acc: 0.8438\n",
      "[Distill] Epoch 40/50 - Loss: 629776.6190 - Train Acc: 0.8964 - Val Acc: 0.8507\n",
      "[Distill] Epoch 41/50 - Loss: 629070.5810 - Train Acc: 0.8997 - Val Acc: 0.8472\n",
      "[Distill] Epoch 42/50 - Loss: 626180.8609 - Train Acc: 0.9013 - Val Acc: 0.8411\n",
      "[Distill] Epoch 43/50 - Loss: 625889.2855 - Train Acc: 0.9008 - Val Acc: 0.8396\n",
      "[Distill] Epoch 44/50 - Loss: 623748.1024 - Train Acc: 0.9017 - Val Acc: 0.8323\n",
      "[Distill] Epoch 45/50 - Loss: 625046.0156 - Train Acc: 0.9012 - Val Acc: 0.8501\n",
      "[Distill] Epoch 46/50 - Loss: 621003.3908 - Train Acc: 0.9020 - Val Acc: 0.8606\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw20.pth\n",
      "[Distill] Epoch 47/50 - Loss: 620051.7758 - Train Acc: 0.9014 - Val Acc: 0.8444\n",
      "[Distill] Epoch 48/50 - Loss: 621193.8195 - Train Acc: 0.9022 - Val Acc: 0.8537\n",
      "[Distill] Epoch 49/50 - Loss: 618500.2772 - Train Acc: 0.9048 - Val Acc: 0.8503\n",
      "[Distill] Epoch 50/50 - Loss: 619193.1562 - Train Acc: 0.9047 - Val Acc: 0.8440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/3282436521.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student.load_state_dict(torch.load(save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Final Classification Report (StudentCNN):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    AESCrypt       0.97      0.98      0.98       221\n",
      "      Cerber       0.95      0.84      0.89       496\n",
      "    Darkside       0.95      0.83      0.89       850\n",
      "       Excel       0.99      1.00      0.99       386\n",
      "     Firefox       0.98      0.90      0.94       539\n",
      "   GandCrab4       0.69      0.76      0.72       836\n",
      "        Ryuk       0.67      0.71      0.69       514\n",
      "     SDelete       1.00      0.99      0.99       204\n",
      "  Sodinokibi       0.87      0.84      0.85       537\n",
      "  TeslaCrypt       0.66      0.97      0.79       224\n",
      "    WannaCry       1.00      1.00      1.00       203\n",
      "         Zip       1.00      0.97      0.99       220\n",
      "\n",
      "    accuracy                           0.86      5230\n",
      "   macro avg       0.89      0.90      0.89      5230\n",
      "weighted avg       0.87      0.86      0.86      5230\n",
      "\n",
      "✅ Student model saved: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td30_Tw20.pth\n",
      "\n",
      "🚀 Distilling MLP-Mixer → StudentCNN for Td=45, Tw=10 (T_len=36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/4218594340.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  teacher.load_state_dict(torch.load(teacher_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Distill] Epoch 01/50 - Loss: 18360.4200 - Train Acc: 0.5250 - Val Acc: 0.6056\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw10.pth\n",
      "[Distill] Epoch 02/50 - Loss: 7367.7193 - Train Acc: 0.7217 - Val Acc: 0.6599\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw10.pth\n",
      "[Distill] Epoch 03/50 - Loss: 5496.1423 - Train Acc: 0.7722 - Val Acc: 0.6657\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw10.pth\n",
      "[Distill] Epoch 04/50 - Loss: 4974.9322 - Train Acc: 0.8049 - Val Acc: 0.7587\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw10.pth\n",
      "[Distill] Epoch 05/50 - Loss: 4642.2274 - Train Acc: 0.8167 - Val Acc: 0.7083\n",
      "[Distill] Epoch 06/50 - Loss: 4444.2775 - Train Acc: 0.8218 - Val Acc: 0.7568\n",
      "[Distill] Epoch 07/50 - Loss: 4194.4939 - Train Acc: 0.8494 - Val Acc: 0.7674\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw10.pth\n",
      "[Distill] Epoch 08/50 - Loss: 4030.9555 - Train Acc: 0.8530 - Val Acc: 0.7955\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw10.pth\n",
      "[Distill] Epoch 09/50 - Loss: 3815.9571 - Train Acc: 0.8683 - Val Acc: 0.7859\n",
      "[Distill] Epoch 10/50 - Loss: 3695.9578 - Train Acc: 0.8674 - Val Acc: 0.8382\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw10.pth\n",
      "[Distill] Epoch 11/50 - Loss: 3639.2552 - Train Acc: 0.8745 - Val Acc: 0.8110\n",
      "[Distill] Epoch 12/50 - Loss: 3573.3387 - Train Acc: 0.8789 - Val Acc: 0.8207\n",
      "[Distill] Epoch 13/50 - Loss: 3516.3114 - Train Acc: 0.8859 - Val Acc: 0.7878\n",
      "[Distill] Epoch 14/50 - Loss: 3444.2138 - Train Acc: 0.8872 - Val Acc: 0.8343\n",
      "[Distill] Epoch 15/50 - Loss: 3423.2447 - Train Acc: 0.8919 - Val Acc: 0.8324\n",
      "[Distill] Epoch 16/50 - Loss: 3400.4821 - Train Acc: 0.8949 - Val Acc: 0.8382\n",
      "[Distill] Epoch 17/50 - Loss: 3354.5713 - Train Acc: 0.8974 - Val Acc: 0.8353\n",
      "[Distill] Epoch 18/50 - Loss: 3305.6050 - Train Acc: 0.8964 - Val Acc: 0.8527\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw10.pth\n",
      "[Distill] Epoch 19/50 - Loss: 3261.8355 - Train Acc: 0.9036 - Val Acc: 0.8566\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw10.pth\n",
      "[Distill] Epoch 20/50 - Loss: 3247.1213 - Train Acc: 0.9124 - Val Acc: 0.8391\n",
      "[Distill] Epoch 21/50 - Loss: 3239.2605 - Train Acc: 0.9131 - Val Acc: 0.8634\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw10.pth\n",
      "[Distill] Epoch 22/50 - Loss: 3190.8821 - Train Acc: 0.9150 - Val Acc: 0.8556\n",
      "[Distill] Epoch 23/50 - Loss: 3189.0379 - Train Acc: 0.9112 - Val Acc: 0.8634\n",
      "[Distill] Epoch 24/50 - Loss: 3144.4432 - Train Acc: 0.9175 - Val Acc: 0.8643\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw10.pth\n",
      "[Distill] Epoch 25/50 - Loss: 3151.9036 - Train Acc: 0.9158 - Val Acc: 0.8624\n",
      "[Distill] Epoch 26/50 - Loss: 3124.4508 - Train Acc: 0.9207 - Val Acc: 0.8663\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw10.pth\n",
      "[Distill] Epoch 27/50 - Loss: 3111.7127 - Train Acc: 0.9215 - Val Acc: 0.8808\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw10.pth\n",
      "[Distill] Epoch 28/50 - Loss: 3049.6893 - Train Acc: 0.9226 - Val Acc: 0.8866\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw10.pth\n",
      "[Distill] Epoch 29/50 - Loss: 3023.7215 - Train Acc: 0.9269 - Val Acc: 0.8711\n",
      "[Distill] Epoch 30/50 - Loss: 3034.8675 - Train Acc: 0.9299 - Val Acc: 0.8740\n",
      "[Distill] Epoch 31/50 - Loss: 3061.0215 - Train Acc: 0.9304 - Val Acc: 0.8789\n",
      "[Distill] Epoch 32/50 - Loss: 3014.3190 - Train Acc: 0.9280 - Val Acc: 0.8711\n",
      "[Distill] Epoch 33/50 - Loss: 2946.8263 - Train Acc: 0.9326 - Val Acc: 0.8731\n",
      "[Distill] Epoch 34/50 - Loss: 2909.7995 - Train Acc: 0.9310 - Val Acc: 0.8808\n",
      "[Distill] Epoch 35/50 - Loss: 2880.8004 - Train Acc: 0.9302 - Val Acc: 0.8769\n",
      "[Distill] Epoch 36/50 - Loss: 2861.0082 - Train Acc: 0.9322 - Val Acc: 0.8798\n",
      "[Distill] Epoch 37/50 - Loss: 2865.3732 - Train Acc: 0.9366 - Val Acc: 0.8847\n",
      "[Distill] Epoch 38/50 - Loss: 2820.3133 - Train Acc: 0.9377 - Val Acc: 0.8983\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw10.pth\n",
      "[Distill] Epoch 39/50 - Loss: 2785.2470 - Train Acc: 0.9339 - Val Acc: 0.8672\n",
      "[Distill] Epoch 40/50 - Loss: 2759.6136 - Train Acc: 0.9396 - Val Acc: 0.8876\n",
      "[Distill] Epoch 41/50 - Loss: 2743.9477 - Train Acc: 0.9356 - Val Acc: 0.8915\n",
      "[Distill] Epoch 42/50 - Loss: 2735.0860 - Train Acc: 0.9383 - Val Acc: 0.8721\n",
      "[Distill] Epoch 43/50 - Loss: 2697.9487 - Train Acc: 0.9387 - Val Acc: 0.8944\n",
      "[Distill] Epoch 44/50 - Loss: 2678.2113 - Train Acc: 0.9397 - Val Acc: 0.8953\n",
      "[Distill] Epoch 45/50 - Loss: 2677.1186 - Train Acc: 0.9391 - Val Acc: 0.8857\n",
      "[Distill] Epoch 46/50 - Loss: 2658.9779 - Train Acc: 0.9378 - Val Acc: 0.8905\n",
      "[Distill] Epoch 47/50 - Loss: 2634.5069 - Train Acc: 0.9358 - Val Acc: 0.8934\n",
      "[Distill] Epoch 48/50 - Loss: 2652.8170 - Train Acc: 0.9410 - Val Acc: 0.8837\n",
      "[Distill] Epoch 49/50 - Loss: 2607.4843 - Train Acc: 0.9366 - Val Acc: 0.8924\n",
      "[Distill] Epoch 50/50 - Loss: 2595.8117 - Train Acc: 0.9380 - Val Acc: 0.8857\n",
      "\n",
      "📊 Final Classification Report (StudentCNN):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    AESCrypt       1.00      1.00      1.00        39\n",
      "      Cerber       0.96      0.86      0.91       100\n",
      "    Darkside       0.97      0.85      0.90       178\n",
      "       Excel       1.00      1.00      1.00        74\n",
      "     Firefox       0.97      0.94      0.95       108\n",
      "   GandCrab4       0.76      0.87      0.81       175\n",
      "        Ryuk       0.86      0.80      0.83       103\n",
      "     SDelete       1.00      1.00      1.00        36\n",
      "  Sodinokibi       0.91      0.88      0.89       109\n",
      "  TeslaCrypt       0.66      1.00      0.79        40\n",
      "    WannaCry       1.00      1.00      1.00        33\n",
      "         Zip       1.00      1.00      1.00        37\n",
      "\n",
      "    accuracy                           0.90      1032\n",
      "   macro avg       0.92      0.93      0.92      1032\n",
      "weighted avg       0.91      0.90      0.90      1032\n",
      "\n",
      "✅ Student model saved: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw10.pth\n",
      "\n",
      "🚀 Distilling MLP-Mixer → StudentCNN for Td=45, Tw=15 (T_len=31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/3282436521.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student.load_state_dict(torch.load(save_path))\n",
      "/tmp/ipykernel_2429355/4218594340.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  teacher.load_state_dict(torch.load(teacher_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Distill] Epoch 01/50 - Loss: 60786.9074 - Train Acc: 0.4919 - Val Acc: 0.4923\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw15.pth\n",
      "[Distill] Epoch 02/50 - Loss: 20969.9231 - Train Acc: 0.7058 - Val Acc: 0.6647\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw15.pth\n",
      "[Distill] Epoch 03/50 - Loss: 17194.7648 - Train Acc: 0.7749 - Val Acc: 0.6932\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw15.pth\n",
      "[Distill] Epoch 04/50 - Loss: 15713.9803 - Train Acc: 0.8194 - Val Acc: 0.6801\n",
      "[Distill] Epoch 05/50 - Loss: 14138.2543 - Train Acc: 0.8332 - Val Acc: 0.7283\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw15.pth\n",
      "[Distill] Epoch 06/50 - Loss: 13136.8051 - Train Acc: 0.8460 - Val Acc: 0.7546\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw15.pth\n",
      "[Distill] Epoch 07/50 - Loss: 12342.3636 - Train Acc: 0.8522 - Val Acc: 0.7684\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw15.pth\n",
      "[Distill] Epoch 08/50 - Loss: 11685.3674 - Train Acc: 0.8531 - Val Acc: 0.7933\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw15.pth\n",
      "[Distill] Epoch 09/50 - Loss: 11069.1915 - Train Acc: 0.8710 - Val Acc: 0.8079\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw15.pth\n",
      "[Distill] Epoch 10/50 - Loss: 10605.0372 - Train Acc: 0.8699 - Val Acc: 0.7940\n",
      "[Distill] Epoch 11/50 - Loss: 10261.8538 - Train Acc: 0.8786 - Val Acc: 0.8137\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw15.pth\n",
      "[Distill] Epoch 12/50 - Loss: 10079.5744 - Train Acc: 0.8804 - Val Acc: 0.8232\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw15.pth\n",
      "[Distill] Epoch 13/50 - Loss: 9932.0333 - Train Acc: 0.8837 - Val Acc: 0.8437\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw15.pth\n",
      "[Distill] Epoch 14/50 - Loss: 9760.6285 - Train Acc: 0.8859 - Val Acc: 0.8327\n",
      "[Distill] Epoch 15/50 - Loss: 9689.3938 - Train Acc: 0.8903 - Val Acc: 0.8203\n",
      "[Distill] Epoch 16/50 - Loss: 9614.6750 - Train Acc: 0.8937 - Val Acc: 0.8437\n",
      "[Distill] Epoch 17/50 - Loss: 9604.0123 - Train Acc: 0.8961 - Val Acc: 0.8174\n",
      "[Distill] Epoch 18/50 - Loss: 9434.8122 - Train Acc: 0.9011 - Val Acc: 0.8313\n",
      "[Distill] Epoch 19/50 - Loss: 9381.7403 - Train Acc: 0.9007 - Val Acc: 0.8473\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw15.pth\n",
      "[Distill] Epoch 20/50 - Loss: 9272.8378 - Train Acc: 0.9055 - Val Acc: 0.8408\n",
      "[Distill] Epoch 21/50 - Loss: 9196.2382 - Train Acc: 0.9092 - Val Acc: 0.8503\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw15.pth\n",
      "[Distill] Epoch 22/50 - Loss: 9053.8762 - Train Acc: 0.9073 - Val Acc: 0.8766\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw15.pth\n",
      "[Distill] Epoch 23/50 - Loss: 8967.5811 - Train Acc: 0.9148 - Val Acc: 0.8561\n",
      "[Distill] Epoch 24/50 - Loss: 8866.5903 - Train Acc: 0.9162 - Val Acc: 0.8590\n",
      "[Distill] Epoch 25/50 - Loss: 8843.0299 - Train Acc: 0.9142 - Val Acc: 0.8729\n",
      "[Distill] Epoch 26/50 - Loss: 8727.5157 - Train Acc: 0.9179 - Val Acc: 0.8831\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw15.pth\n",
      "[Distill] Epoch 27/50 - Loss: 8641.9504 - Train Acc: 0.9191 - Val Acc: 0.8634\n",
      "[Distill] Epoch 28/50 - Loss: 8555.3678 - Train Acc: 0.9170 - Val Acc: 0.8714\n",
      "[Distill] Epoch 29/50 - Loss: 8478.8632 - Train Acc: 0.9189 - Val Acc: 0.8780\n",
      "[Distill] Epoch 30/50 - Loss: 8400.3558 - Train Acc: 0.9209 - Val Acc: 0.8649\n",
      "[Distill] Epoch 31/50 - Loss: 8354.4530 - Train Acc: 0.9263 - Val Acc: 0.8736\n",
      "[Distill] Epoch 32/50 - Loss: 8290.0090 - Train Acc: 0.9264 - Val Acc: 0.8663\n",
      "[Distill] Epoch 33/50 - Loss: 8215.8855 - Train Acc: 0.9267 - Val Acc: 0.8744\n",
      "[Distill] Epoch 34/50 - Loss: 8140.0976 - Train Acc: 0.9278 - Val Acc: 0.8817\n",
      "[Distill] Epoch 35/50 - Loss: 8095.9763 - Train Acc: 0.9252 - Val Acc: 0.8860\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw15.pth\n",
      "[Distill] Epoch 36/50 - Loss: 8052.4645 - Train Acc: 0.9291 - Val Acc: 0.8904\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw15.pth\n",
      "[Distill] Epoch 37/50 - Loss: 8007.7300 - Train Acc: 0.9321 - Val Acc: 0.8926\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw15.pth\n",
      "[Distill] Epoch 38/50 - Loss: 8006.4528 - Train Acc: 0.9290 - Val Acc: 0.8802\n",
      "[Distill] Epoch 39/50 - Loss: 7946.5089 - Train Acc: 0.9290 - Val Acc: 0.8590\n",
      "[Distill] Epoch 40/50 - Loss: 7884.3646 - Train Acc: 0.9353 - Val Acc: 0.8860\n",
      "[Distill] Epoch 41/50 - Loss: 7825.3164 - Train Acc: 0.9327 - Val Acc: 0.8766\n",
      "[Distill] Epoch 42/50 - Loss: 7806.8306 - Train Acc: 0.9378 - Val Acc: 0.8860\n",
      "[Distill] Epoch 43/50 - Loss: 7722.3215 - Train Acc: 0.9372 - Val Acc: 0.8831\n",
      "[Distill] Epoch 44/50 - Loss: 7700.4526 - Train Acc: 0.9367 - Val Acc: 0.8890\n",
      "[Distill] Epoch 45/50 - Loss: 7658.6971 - Train Acc: 0.9361 - Val Acc: 0.8744\n",
      "[Distill] Epoch 46/50 - Loss: 7642.9327 - Train Acc: 0.9380 - Val Acc: 0.8875\n",
      "[Distill] Epoch 47/50 - Loss: 7619.9982 - Train Acc: 0.9400 - Val Acc: 0.8912\n",
      "[Distill] Epoch 48/50 - Loss: 7554.1061 - Train Acc: 0.9433 - Val Acc: 0.9021\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw15.pth\n",
      "[Distill] Epoch 49/50 - Loss: 7549.0481 - Train Acc: 0.9417 - Val Acc: 0.8963\n",
      "[Distill] Epoch 50/50 - Loss: 7456.6361 - Train Acc: 0.9439 - Val Acc: 0.8948\n",
      "\n",
      "📊 Final Classification Report (StudentCNN):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    AESCrypt       1.00      0.98      0.99        53\n",
      "      Cerber       0.92      0.84      0.88       132\n",
      "    Darkside       0.96      0.87      0.91       232\n",
      "       Excel       1.00      1.00      1.00       100\n",
      "     Firefox       0.99      0.94      0.96       143\n",
      "   GandCrab4       0.80      0.85      0.82       228\n",
      "        Ryuk       0.83      0.80      0.81       137\n",
      "     SDelete       0.98      1.00      0.99        48\n",
      "  Sodinokibi       0.85      0.94      0.89       143\n",
      "  TeslaCrypt       0.73      0.98      0.84        53\n",
      "    WannaCry       1.00      1.00      1.00        48\n",
      "         Zip       1.00      1.00      1.00        52\n",
      "\n",
      "    accuracy                           0.90      1369\n",
      "   macro avg       0.92      0.93      0.92      1369\n",
      "weighted avg       0.91      0.90      0.90      1369\n",
      "\n",
      "✅ Student model saved: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw15.pth\n",
      "\n",
      "🚀 Distilling MLP-Mixer → StudentCNN for Td=45, Tw=20 (T_len=26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/3282436521.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student.load_state_dict(torch.load(save_path))\n",
      "/tmp/ipykernel_2429355/4218594340.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  teacher.load_state_dict(torch.load(teacher_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Distill] Epoch 01/50 - Loss: 62977.1840 - Train Acc: 0.4857 - Val Acc: 0.5672\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw20.pth\n",
      "[Distill] Epoch 02/50 - Loss: 27750.8771 - Train Acc: 0.6904 - Val Acc: 0.5943\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw20.pth\n",
      "[Distill] Epoch 03/50 - Loss: 23933.0349 - Train Acc: 0.7284 - Val Acc: 0.6852\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw20.pth\n",
      "[Distill] Epoch 04/50 - Loss: 22902.4106 - Train Acc: 0.7570 - Val Acc: 0.6675\n",
      "[Distill] Epoch 05/50 - Loss: 22043.4213 - Train Acc: 0.7890 - Val Acc: 0.6940\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw20.pth\n",
      "[Distill] Epoch 06/50 - Loss: 21148.4081 - Train Acc: 0.8155 - Val Acc: 0.7249\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw20.pth\n",
      "[Distill] Epoch 07/50 - Loss: 20444.6238 - Train Acc: 0.8348 - Val Acc: 0.7539\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw20.pth\n",
      "[Distill] Epoch 08/50 - Loss: 19803.2908 - Train Acc: 0.8414 - Val Acc: 0.7621\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw20.pth\n",
      "[Distill] Epoch 09/50 - Loss: 19376.7511 - Train Acc: 0.8541 - Val Acc: 0.7748\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw20.pth\n",
      "[Distill] Epoch 10/50 - Loss: 18956.8733 - Train Acc: 0.8640 - Val Acc: 0.8215\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw20.pth\n",
      "[Distill] Epoch 11/50 - Loss: 18668.0012 - Train Acc: 0.8731 - Val Acc: 0.8240\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw20.pth\n",
      "[Distill] Epoch 12/50 - Loss: 18332.9024 - Train Acc: 0.8857 - Val Acc: 0.8322\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw20.pth\n",
      "[Distill] Epoch 13/50 - Loss: 18296.9635 - Train Acc: 0.8814 - Val Acc: 0.8252\n",
      "[Distill] Epoch 14/50 - Loss: 18008.8599 - Train Acc: 0.8900 - Val Acc: 0.8536\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw20.pth\n",
      "[Distill] Epoch 15/50 - Loss: 17899.3047 - Train Acc: 0.8937 - Val Acc: 0.8271\n",
      "[Distill] Epoch 16/50 - Loss: 17752.0230 - Train Acc: 0.8935 - Val Acc: 0.8189\n",
      "[Distill] Epoch 17/50 - Loss: 17548.5485 - Train Acc: 0.8942 - Val Acc: 0.7931\n",
      "[Distill] Epoch 18/50 - Loss: 17537.2730 - Train Acc: 0.8958 - Val Acc: 0.8183\n",
      "[Distill] Epoch 19/50 - Loss: 17299.6063 - Train Acc: 0.8988 - Val Acc: 0.8315\n",
      "[Distill] Epoch 20/50 - Loss: 17156.5248 - Train Acc: 0.9032 - Val Acc: 0.8385\n",
      "[Distill] Epoch 21/50 - Loss: 16805.4981 - Train Acc: 0.9049 - Val Acc: 0.8625\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw20.pth\n",
      "[Distill] Epoch 22/50 - Loss: 16636.4922 - Train Acc: 0.9026 - Val Acc: 0.8505\n",
      "[Distill] Epoch 23/50 - Loss: 16467.0653 - Train Acc: 0.9088 - Val Acc: 0.8467\n",
      "[Distill] Epoch 24/50 - Loss: 16282.6168 - Train Acc: 0.9121 - Val Acc: 0.8429\n",
      "[Distill] Epoch 25/50 - Loss: 16285.8880 - Train Acc: 0.9098 - Val Acc: 0.8517\n",
      "[Distill] Epoch 26/50 - Loss: 16048.0666 - Train Acc: 0.9151 - Val Acc: 0.8347\n",
      "[Distill] Epoch 27/50 - Loss: 15971.0877 - Train Acc: 0.9186 - Val Acc: 0.8543\n",
      "[Distill] Epoch 28/50 - Loss: 15700.8779 - Train Acc: 0.9174 - Val Acc: 0.8360\n",
      "[Distill] Epoch 29/50 - Loss: 15456.0624 - Train Acc: 0.9164 - Val Acc: 0.8517\n",
      "[Distill] Epoch 30/50 - Loss: 15259.9802 - Train Acc: 0.9220 - Val Acc: 0.8650\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw20.pth\n",
      "[Distill] Epoch 31/50 - Loss: 14983.9476 - Train Acc: 0.9201 - Val Acc: 0.8669\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw20.pth\n",
      "[Distill] Epoch 32/50 - Loss: 14864.6408 - Train Acc: 0.9215 - Val Acc: 0.8763\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw20.pth\n",
      "[Distill] Epoch 33/50 - Loss: 14669.0246 - Train Acc: 0.9244 - Val Acc: 0.8511\n",
      "[Distill] Epoch 34/50 - Loss: 14564.9744 - Train Acc: 0.9240 - Val Acc: 0.8492\n",
      "[Distill] Epoch 35/50 - Loss: 14398.8264 - Train Acc: 0.9239 - Val Acc: 0.8454\n",
      "[Distill] Epoch 36/50 - Loss: 14283.7587 - Train Acc: 0.9255 - Val Acc: 0.8385\n",
      "[Distill] Epoch 37/50 - Loss: 14230.5865 - Train Acc: 0.9262 - Val Acc: 0.8732\n",
      "[Distill] Epoch 38/50 - Loss: 14137.6145 - Train Acc: 0.9294 - Val Acc: 0.8328\n",
      "[Distill] Epoch 39/50 - Loss: 14120.9031 - Train Acc: 0.9310 - Val Acc: 0.8713\n",
      "[Distill] Epoch 40/50 - Loss: 14133.3200 - Train Acc: 0.9292 - Val Acc: 0.8675\n",
      "[Distill] Epoch 41/50 - Loss: 14095.7272 - Train Acc: 0.9315 - Val Acc: 0.8656\n",
      "[Distill] Epoch 42/50 - Loss: 13920.7575 - Train Acc: 0.9308 - Val Acc: 0.8625\n",
      "[Distill] Epoch 43/50 - Loss: 13945.8422 - Train Acc: 0.9302 - Val Acc: 0.8606\n",
      "[Distill] Epoch 44/50 - Loss: 13934.3315 - Train Acc: 0.9335 - Val Acc: 0.8845\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw20.pth\n",
      "[Distill] Epoch 45/50 - Loss: 13861.5433 - Train Acc: 0.9358 - Val Acc: 0.8757\n",
      "[Distill] Epoch 46/50 - Loss: 13791.0572 - Train Acc: 0.9346 - Val Acc: 0.8770\n",
      "[Distill] Epoch 47/50 - Loss: 13703.4730 - Train Acc: 0.9364 - Val Acc: 0.8625\n",
      "[Distill] Epoch 48/50 - Loss: 13658.7144 - Train Acc: 0.9380 - Val Acc: 0.8953\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw20.pth\n",
      "[Distill] Epoch 49/50 - Loss: 13619.5697 - Train Acc: 0.9392 - Val Acc: 0.8896\n",
      "[Distill] Epoch 50/50 - Loss: 13582.3941 - Train Acc: 0.9354 - Val Acc: 0.8915\n",
      "\n",
      "📊 Final Classification Report (StudentCNN):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    AESCrypt       1.00      1.00      1.00        60\n",
      "      Cerber       0.94      0.88      0.90       152\n",
      "    Darkside       0.99      0.84      0.91       270\n",
      "       Excel       1.00      1.00      1.00       115\n",
      "     Firefox       0.98      0.95      0.96       166\n",
      "   GandCrab4       0.77      0.80      0.78       265\n",
      "        Ryuk       0.75      0.83      0.79       158\n",
      "     SDelete       0.98      1.00      0.99        55\n",
      "  Sodinokibi       0.87      0.91      0.89       167\n",
      "  TeslaCrypt       0.76      1.00      0.86        62\n",
      "    WannaCry       1.00      1.00      1.00        55\n",
      "         Zip       1.00      0.98      0.99        60\n",
      "\n",
      "    accuracy                           0.90      1585\n",
      "   macro avg       0.92      0.93      0.92      1585\n",
      "weighted avg       0.90      0.90      0.90      1585\n",
      "\n",
      "✅ Student model saved: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td45_Tw20.pth\n",
      "\n",
      "🚀 Distilling MLP-Mixer → StudentCNN for Td=60, Tw=10 (T_len=51)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/3282436521.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student.load_state_dict(torch.load(save_path))\n",
      "/tmp/ipykernel_2429355/4218594340.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  teacher.load_state_dict(torch.load(teacher_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Distill] Epoch 01/50 - Loss: 11990.4833 - Train Acc: 0.3378 - Val Acc: 0.4003\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw10.pth\n",
      "[Distill] Epoch 02/50 - Loss: 5242.9965 - Train Acc: 0.5993 - Val Acc: 0.5051\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw10.pth\n",
      "[Distill] Epoch 03/50 - Loss: 4533.9770 - Train Acc: 0.6621 - Val Acc: 0.4934\n",
      "[Distill] Epoch 04/50 - Loss: 4013.7594 - Train Acc: 0.6884 - Val Acc: 0.5284\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw10.pth\n",
      "[Distill] Epoch 05/50 - Loss: 3732.6768 - Train Acc: 0.7339 - Val Acc: 0.6608\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw10.pth\n",
      "[Distill] Epoch 06/50 - Loss: 3554.4155 - Train Acc: 0.7618 - Val Acc: 0.6987\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw10.pth\n",
      "[Distill] Epoch 07/50 - Loss: 3423.3706 - Train Acc: 0.7859 - Val Acc: 0.6943\n",
      "[Distill] Epoch 08/50 - Loss: 3364.8252 - Train Acc: 0.8056 - Val Acc: 0.7380\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw10.pth\n",
      "[Distill] Epoch 09/50 - Loss: 3349.1853 - Train Acc: 0.8158 - Val Acc: 0.6972\n",
      "[Distill] Epoch 10/50 - Loss: 3320.8809 - Train Acc: 0.8098 - Val Acc: 0.7380\n",
      "[Distill] Epoch 11/50 - Loss: 3302.2933 - Train Acc: 0.8247 - Val Acc: 0.7380\n",
      "[Distill] Epoch 12/50 - Loss: 3266.8725 - Train Acc: 0.8331 - Val Acc: 0.7394\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw10.pth\n",
      "[Distill] Epoch 13/50 - Loss: 3262.9150 - Train Acc: 0.8360 - Val Acc: 0.7205\n",
      "[Distill] Epoch 14/50 - Loss: 3219.0237 - Train Acc: 0.8411 - Val Acc: 0.8006\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw10.pth\n",
      "[Distill] Epoch 15/50 - Loss: 3175.7784 - Train Acc: 0.8472 - Val Acc: 0.7598\n",
      "[Distill] Epoch 16/50 - Loss: 3133.2417 - Train Acc: 0.8536 - Val Acc: 0.7729\n",
      "[Distill] Epoch 17/50 - Loss: 3078.4673 - Train Acc: 0.8552 - Val Acc: 0.7948\n",
      "[Distill] Epoch 18/50 - Loss: 3045.6968 - Train Acc: 0.8566 - Val Acc: 0.7831\n",
      "[Distill] Epoch 19/50 - Loss: 3004.4957 - Train Acc: 0.8589 - Val Acc: 0.8064\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw10.pth\n",
      "[Distill] Epoch 20/50 - Loss: 2988.0089 - Train Acc: 0.8578 - Val Acc: 0.7846\n",
      "[Distill] Epoch 21/50 - Loss: 2960.2973 - Train Acc: 0.8600 - Val Acc: 0.7584\n",
      "[Distill] Epoch 22/50 - Loss: 2905.1301 - Train Acc: 0.8677 - Val Acc: 0.7787\n",
      "[Distill] Epoch 23/50 - Loss: 2880.3522 - Train Acc: 0.8801 - Val Acc: 0.7991\n",
      "[Distill] Epoch 24/50 - Loss: 2847.8367 - Train Acc: 0.8774 - Val Acc: 0.7933\n",
      "[Distill] Epoch 25/50 - Loss: 2836.5598 - Train Acc: 0.8777 - Val Acc: 0.8210\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw10.pth\n",
      "[Distill] Epoch 26/50 - Loss: 2799.9152 - Train Acc: 0.8720 - Val Acc: 0.8035\n",
      "[Distill] Epoch 27/50 - Loss: 2793.0824 - Train Acc: 0.8780 - Val Acc: 0.7787\n",
      "[Distill] Epoch 28/50 - Loss: 2759.4431 - Train Acc: 0.8744 - Val Acc: 0.8093\n",
      "[Distill] Epoch 29/50 - Loss: 2735.9506 - Train Acc: 0.8852 - Val Acc: 0.7744\n",
      "[Distill] Epoch 30/50 - Loss: 2715.8330 - Train Acc: 0.8826 - Val Acc: 0.7889\n",
      "[Distill] Epoch 31/50 - Loss: 2700.5076 - Train Acc: 0.8837 - Val Acc: 0.8166\n",
      "[Distill] Epoch 32/50 - Loss: 2686.9363 - Train Acc: 0.8855 - Val Acc: 0.7977\n",
      "[Distill] Epoch 33/50 - Loss: 2649.6239 - Train Acc: 0.8907 - Val Acc: 0.8210\n",
      "[Distill] Epoch 34/50 - Loss: 2624.2070 - Train Acc: 0.8920 - Val Acc: 0.8413\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw10.pth\n",
      "[Distill] Epoch 35/50 - Loss: 2554.5048 - Train Acc: 0.8919 - Val Acc: 0.8297\n",
      "[Distill] Epoch 36/50 - Loss: 2516.7049 - Train Acc: 0.8926 - Val Acc: 0.8297\n",
      "[Distill] Epoch 37/50 - Loss: 2471.1325 - Train Acc: 0.8918 - Val Acc: 0.7875\n",
      "[Distill] Epoch 38/50 - Loss: 2454.5706 - Train Acc: 0.8877 - Val Acc: 0.8297\n",
      "[Distill] Epoch 39/50 - Loss: 2418.5970 - Train Acc: 0.8940 - Val Acc: 0.8049\n",
      "[Distill] Epoch 40/50 - Loss: 2400.0701 - Train Acc: 0.8934 - Val Acc: 0.8472\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw10.pth\n",
      "[Distill] Epoch 41/50 - Loss: 2391.3929 - Train Acc: 0.8972 - Val Acc: 0.8399\n",
      "[Distill] Epoch 42/50 - Loss: 2358.3693 - Train Acc: 0.9018 - Val Acc: 0.8588\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw10.pth\n",
      "[Distill] Epoch 43/50 - Loss: 2333.3008 - Train Acc: 0.9028 - Val Acc: 0.8472\n",
      "[Distill] Epoch 44/50 - Loss: 2318.7452 - Train Acc: 0.9025 - Val Acc: 0.8501\n",
      "[Distill] Epoch 45/50 - Loss: 2312.9577 - Train Acc: 0.9064 - Val Acc: 0.8122\n",
      "[Distill] Epoch 46/50 - Loss: 2304.7699 - Train Acc: 0.9101 - Val Acc: 0.8617\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw10.pth\n",
      "[Distill] Epoch 47/50 - Loss: 2306.3031 - Train Acc: 0.9087 - Val Acc: 0.8632\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw10.pth\n",
      "[Distill] Epoch 48/50 - Loss: 2295.4062 - Train Acc: 0.9058 - Val Acc: 0.8632\n",
      "[Distill] Epoch 49/50 - Loss: 2271.8961 - Train Acc: 0.9193 - Val Acc: 0.8719\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw10.pth\n",
      "[Distill] Epoch 50/50 - Loss: 2281.7303 - Train Acc: 0.9125 - Val Acc: 0.8603\n",
      "\n",
      "📊 Final Classification Report (StudentCNN):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    AESCrypt       1.00      1.00      1.00        21\n",
      "      Cerber       0.92      0.90      0.91        68\n",
      "    Darkside       1.00      0.79      0.88       126\n",
      "       Excel       1.00      1.00      1.00        49\n",
      "     Firefox       0.99      0.91      0.94        74\n",
      "   GandCrab4       0.76      0.80      0.78       125\n",
      "        Ryuk       0.64      0.76      0.69        71\n",
      "     SDelete       1.00      1.00      1.00        18\n",
      "  Sodinokibi       0.83      0.93      0.88        74\n",
      "  TeslaCrypt       0.78      0.95      0.86        22\n",
      "    WannaCry       1.00      1.00      1.00        18\n",
      "         Zip       1.00      1.00      1.00        21\n",
      "\n",
      "    accuracy                           0.87       687\n",
      "   macro avg       0.91      0.92      0.91       687\n",
      "weighted avg       0.88      0.87      0.87       687\n",
      "\n",
      "✅ Student model saved: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw10.pth\n",
      "\n",
      "🚀 Distilling MLP-Mixer → StudentCNN for Td=60, Tw=15 (T_len=46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/3282436521.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student.load_state_dict(torch.load(save_path))\n",
      "/tmp/ipykernel_2429355/4218594340.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  teacher.load_state_dict(torch.load(teacher_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Distill] Epoch 01/50 - Loss: 8624.7069 - Train Acc: 0.6049 - Val Acc: 0.6405\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw15.pth\n",
      "[Distill] Epoch 02/50 - Loss: 3568.0302 - Train Acc: 0.7772 - Val Acc: 0.6949\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw15.pth\n",
      "[Distill] Epoch 03/50 - Loss: 3184.7275 - Train Acc: 0.8168 - Val Acc: 0.7468\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw15.pth\n",
      "[Distill] Epoch 04/50 - Loss: 2902.1499 - Train Acc: 0.8437 - Val Acc: 0.7987\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw15.pth\n",
      "[Distill] Epoch 05/50 - Loss: 2702.2410 - Train Acc: 0.8486 - Val Acc: 0.7519\n",
      "[Distill] Epoch 06/50 - Loss: 2584.3240 - Train Acc: 0.8642 - Val Acc: 0.7380\n",
      "[Distill] Epoch 07/50 - Loss: 2529.3966 - Train Acc: 0.8692 - Val Acc: 0.8076\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw15.pth\n",
      "[Distill] Epoch 08/50 - Loss: 2466.5359 - Train Acc: 0.8817 - Val Acc: 0.8165\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw15.pth\n",
      "[Distill] Epoch 09/50 - Loss: 2431.9635 - Train Acc: 0.8866 - Val Acc: 0.8418\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw15.pth\n",
      "[Distill] Epoch 10/50 - Loss: 2378.7940 - Train Acc: 0.8922 - Val Acc: 0.8443\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw15.pth\n",
      "[Distill] Epoch 11/50 - Loss: 2340.5971 - Train Acc: 0.8907 - Val Acc: 0.8114\n",
      "[Distill] Epoch 12/50 - Loss: 2312.2463 - Train Acc: 0.8980 - Val Acc: 0.8139\n",
      "[Distill] Epoch 13/50 - Loss: 2255.0225 - Train Acc: 0.9001 - Val Acc: 0.8114\n",
      "[Distill] Epoch 14/50 - Loss: 2198.2452 - Train Acc: 0.9100 - Val Acc: 0.8278\n",
      "[Distill] Epoch 15/50 - Loss: 2184.2513 - Train Acc: 0.9006 - Val Acc: 0.8506\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw15.pth\n",
      "[Distill] Epoch 16/50 - Loss: 2123.9325 - Train Acc: 0.9139 - Val Acc: 0.8291\n",
      "[Distill] Epoch 17/50 - Loss: 2091.0000 - Train Acc: 0.9128 - Val Acc: 0.8709\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw15.pth\n",
      "[Distill] Epoch 18/50 - Loss: 2064.4943 - Train Acc: 0.9143 - Val Acc: 0.8392\n",
      "[Distill] Epoch 19/50 - Loss: 2028.3575 - Train Acc: 0.9190 - Val Acc: 0.8456\n",
      "[Distill] Epoch 20/50 - Loss: 1999.2347 - Train Acc: 0.9214 - Val Acc: 0.8494\n",
      "[Distill] Epoch 21/50 - Loss: 1971.5514 - Train Acc: 0.9208 - Val Acc: 0.8367\n",
      "[Distill] Epoch 22/50 - Loss: 1933.0949 - Train Acc: 0.9244 - Val Acc: 0.8608\n",
      "[Distill] Epoch 23/50 - Loss: 1912.8885 - Train Acc: 0.9238 - Val Acc: 0.8633\n",
      "[Distill] Epoch 24/50 - Loss: 1888.6347 - Train Acc: 0.9268 - Val Acc: 0.8570\n",
      "[Distill] Epoch 25/50 - Loss: 1874.0888 - Train Acc: 0.9271 - Val Acc: 0.8532\n",
      "[Distill] Epoch 26/50 - Loss: 1891.4675 - Train Acc: 0.9244 - Val Acc: 0.8494\n",
      "[Distill] Epoch 27/50 - Loss: 1858.4258 - Train Acc: 0.9274 - Val Acc: 0.8481\n",
      "[Distill] Epoch 28/50 - Loss: 1803.9755 - Train Acc: 0.9273 - Val Acc: 0.8544\n",
      "[Distill] Epoch 29/50 - Loss: 1752.7729 - Train Acc: 0.9340 - Val Acc: 0.8418\n",
      "[Distill] Epoch 30/50 - Loss: 1743.2464 - Train Acc: 0.9321 - Val Acc: 0.8646\n",
      "[Distill] Epoch 31/50 - Loss: 1727.6292 - Train Acc: 0.9346 - Val Acc: 0.8734\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw15.pth\n",
      "[Distill] Epoch 32/50 - Loss: 1729.1958 - Train Acc: 0.9335 - Val Acc: 0.8810\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw15.pth\n",
      "[Distill] Epoch 33/50 - Loss: 1722.1509 - Train Acc: 0.9372 - Val Acc: 0.8797\n",
      "[Distill] Epoch 34/50 - Loss: 1702.1153 - Train Acc: 0.9373 - Val Acc: 0.8570\n",
      "[Distill] Epoch 35/50 - Loss: 1703.8242 - Train Acc: 0.9375 - Val Acc: 0.8747\n",
      "[Distill] Epoch 36/50 - Loss: 1698.2002 - Train Acc: 0.9411 - Val Acc: 0.8671\n",
      "[Distill] Epoch 37/50 - Loss: 1697.3856 - Train Acc: 0.9383 - Val Acc: 0.8608\n",
      "[Distill] Epoch 38/50 - Loss: 1696.7036 - Train Acc: 0.9356 - Val Acc: 0.8709\n",
      "[Distill] Epoch 39/50 - Loss: 1672.4553 - Train Acc: 0.9335 - Val Acc: 0.8848\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw15.pth\n",
      "[Distill] Epoch 40/50 - Loss: 1660.2768 - Train Acc: 0.9373 - Val Acc: 0.8861\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw15.pth\n",
      "[Distill] Epoch 41/50 - Loss: 1656.2259 - Train Acc: 0.9386 - Val Acc: 0.8810\n",
      "[Distill] Epoch 42/50 - Loss: 1633.8519 - Train Acc: 0.9396 - Val Acc: 0.8823\n",
      "[Distill] Epoch 43/50 - Loss: 1619.7092 - Train Acc: 0.9368 - Val Acc: 0.8848\n",
      "[Distill] Epoch 44/50 - Loss: 1606.9492 - Train Acc: 0.9420 - Val Acc: 0.8481\n",
      "[Distill] Epoch 45/50 - Loss: 1600.7996 - Train Acc: 0.9405 - Val Acc: 0.8949\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw15.pth\n",
      "[Distill] Epoch 46/50 - Loss: 1597.2432 - Train Acc: 0.9410 - Val Acc: 0.8620\n",
      "[Distill] Epoch 47/50 - Loss: 1598.3171 - Train Acc: 0.9404 - Val Acc: 0.8785\n",
      "[Distill] Epoch 48/50 - Loss: 1578.7788 - Train Acc: 0.9432 - Val Acc: 0.8646\n",
      "[Distill] Epoch 49/50 - Loss: 1573.9340 - Train Acc: 0.9462 - Val Acc: 0.8987\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw15.pth\n",
      "[Distill] Epoch 50/50 - Loss: 1567.2526 - Train Acc: 0.9434 - Val Acc: 0.9025\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw15.pth\n",
      "\n",
      "📊 Final Classification Report (StudentCNN):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    AESCrypt       0.96      1.00      0.98        27\n",
      "      Cerber       0.93      0.88      0.91        76\n",
      "    Darkside       0.98      0.85      0.91       142\n",
      "       Excel       1.00      1.00      1.00        56\n",
      "     Firefox       0.99      0.95      0.97        85\n",
      "   GandCrab4       0.79      0.90      0.84       138\n",
      "        Ryuk       0.79      0.80      0.79        79\n",
      "     SDelete       1.00      1.00      1.00        24\n",
      "  Sodinokibi       0.83      0.88      0.86        84\n",
      "  TeslaCrypt       0.90      0.96      0.93        28\n",
      "    WannaCry       1.00      1.00      1.00        24\n",
      "         Zip       1.00      0.96      0.98        27\n",
      "\n",
      "    accuracy                           0.90       790\n",
      "   macro avg       0.93      0.93      0.93       790\n",
      "weighted avg       0.91      0.90      0.90       790\n",
      "\n",
      "✅ Student model saved: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw15.pth\n",
      "\n",
      "🚀 Distilling MLP-Mixer → StudentCNN for Td=60, Tw=20 (T_len=41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/3282436521.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student.load_state_dict(torch.load(save_path))\n",
      "/tmp/ipykernel_2429355/4218594340.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  teacher.load_state_dict(torch.load(teacher_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Distill] Epoch 01/50 - Loss: 16039.2905 - Train Acc: 0.4151 - Val Acc: 0.3326\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw20.pth\n",
      "[Distill] Epoch 02/50 - Loss: 5970.4138 - Train Acc: 0.6829 - Val Acc: 0.6417\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw20.pth\n",
      "[Distill] Epoch 03/50 - Loss: 4851.9650 - Train Acc: 0.7727 - Val Acc: 0.7564\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw20.pth\n",
      "[Distill] Epoch 04/50 - Loss: 4442.7708 - Train Acc: 0.7982 - Val Acc: 0.7014\n",
      "[Distill] Epoch 05/50 - Loss: 4041.2446 - Train Acc: 0.8319 - Val Acc: 0.7670\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw20.pth\n",
      "[Distill] Epoch 06/50 - Loss: 3780.3928 - Train Acc: 0.8505 - Val Acc: 0.7892\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw20.pth\n",
      "[Distill] Epoch 07/50 - Loss: 3644.1303 - Train Acc: 0.8640 - Val Acc: 0.7752\n",
      "[Distill] Epoch 08/50 - Loss: 3496.4626 - Train Acc: 0.8670 - Val Acc: 0.8080\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw20.pth\n",
      "[Distill] Epoch 09/50 - Loss: 3385.3302 - Train Acc: 0.8776 - Val Acc: 0.8290\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw20.pth\n",
      "[Distill] Epoch 10/50 - Loss: 3305.3052 - Train Acc: 0.8855 - Val Acc: 0.8103\n",
      "[Distill] Epoch 11/50 - Loss: 3199.6686 - Train Acc: 0.8916 - Val Acc: 0.8197\n",
      "[Distill] Epoch 12/50 - Loss: 3130.3569 - Train Acc: 0.9003 - Val Acc: 0.8466\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw20.pth\n",
      "[Distill] Epoch 13/50 - Loss: 3080.4547 - Train Acc: 0.9030 - Val Acc: 0.8091\n",
      "[Distill] Epoch 14/50 - Loss: 3050.1049 - Train Acc: 0.9063 - Val Acc: 0.8372\n",
      "[Distill] Epoch 15/50 - Loss: 3036.1766 - Train Acc: 0.9080 - Val Acc: 0.8419\n",
      "[Distill] Epoch 16/50 - Loss: 3012.4002 - Train Acc: 0.9047 - Val Acc: 0.8548\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw20.pth\n",
      "[Distill] Epoch 17/50 - Loss: 3009.6041 - Train Acc: 0.9095 - Val Acc: 0.8407\n",
      "[Distill] Epoch 18/50 - Loss: 2955.9656 - Train Acc: 0.9138 - Val Acc: 0.8173\n",
      "[Distill] Epoch 19/50 - Loss: 2956.9251 - Train Acc: 0.9142 - Val Acc: 0.8009\n",
      "[Distill] Epoch 20/50 - Loss: 2934.7364 - Train Acc: 0.9194 - Val Acc: 0.8396\n",
      "[Distill] Epoch 21/50 - Loss: 2911.8393 - Train Acc: 0.9181 - Val Acc: 0.8571\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw20.pth\n",
      "[Distill] Epoch 22/50 - Loss: 2908.4208 - Train Acc: 0.9160 - Val Acc: 0.8454\n",
      "[Distill] Epoch 23/50 - Loss: 2873.7462 - Train Acc: 0.9216 - Val Acc: 0.8443\n",
      "[Distill] Epoch 24/50 - Loss: 2860.0130 - Train Acc: 0.9211 - Val Acc: 0.8361\n",
      "[Distill] Epoch 25/50 - Loss: 2865.2110 - Train Acc: 0.9214 - Val Acc: 0.8536\n",
      "[Distill] Epoch 26/50 - Loss: 2825.7518 - Train Acc: 0.9210 - Val Acc: 0.8478\n",
      "[Distill] Epoch 27/50 - Loss: 2794.8855 - Train Acc: 0.9240 - Val Acc: 0.8454\n",
      "[Distill] Epoch 28/50 - Loss: 2800.7779 - Train Acc: 0.9249 - Val Acc: 0.8642\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw20.pth\n",
      "[Distill] Epoch 29/50 - Loss: 2794.1962 - Train Acc: 0.9271 - Val Acc: 0.8536\n",
      "[Distill] Epoch 30/50 - Loss: 2777.0077 - Train Acc: 0.9266 - Val Acc: 0.8642\n",
      "[Distill] Epoch 31/50 - Loss: 2774.3487 - Train Acc: 0.9308 - Val Acc: 0.8583\n",
      "[Distill] Epoch 32/50 - Loss: 2750.1702 - Train Acc: 0.9307 - Val Acc: 0.8419\n",
      "[Distill] Epoch 33/50 - Loss: 2753.9830 - Train Acc: 0.9284 - Val Acc: 0.8724\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw20.pth\n",
      "[Distill] Epoch 34/50 - Loss: 2747.7725 - Train Acc: 0.9301 - Val Acc: 0.8560\n",
      "[Distill] Epoch 35/50 - Loss: 2750.3683 - Train Acc: 0.9340 - Val Acc: 0.8466\n",
      "[Distill] Epoch 36/50 - Loss: 2728.0649 - Train Acc: 0.9302 - Val Acc: 0.8653\n",
      "[Distill] Epoch 37/50 - Loss: 2731.2548 - Train Acc: 0.9327 - Val Acc: 0.8665\n",
      "[Distill] Epoch 38/50 - Loss: 2730.6302 - Train Acc: 0.9374 - Val Acc: 0.8770\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw20.pth\n",
      "[Distill] Epoch 39/50 - Loss: 2713.4148 - Train Acc: 0.9343 - Val Acc: 0.8642\n",
      "[Distill] Epoch 40/50 - Loss: 2723.7054 - Train Acc: 0.9390 - Val Acc: 0.8852\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw20.pth\n",
      "[Distill] Epoch 41/50 - Loss: 2696.3355 - Train Acc: 0.9365 - Val Acc: 0.8677\n",
      "[Distill] Epoch 42/50 - Loss: 2699.1625 - Train Acc: 0.9385 - Val Acc: 0.8700\n",
      "[Distill] Epoch 43/50 - Loss: 2688.9490 - Train Acc: 0.9386 - Val Acc: 0.8162\n",
      "[Distill] Epoch 44/50 - Loss: 2699.5959 - Train Acc: 0.9416 - Val Acc: 0.8841\n",
      "[Distill] Epoch 45/50 - Loss: 2679.4960 - Train Acc: 0.9397 - Val Acc: 0.8829\n",
      "[Distill] Epoch 46/50 - Loss: 2675.6341 - Train Acc: 0.9362 - Val Acc: 0.8525\n",
      "[Distill] Epoch 47/50 - Loss: 2680.3423 - Train Acc: 0.9434 - Val Acc: 0.8478\n",
      "[Distill] Epoch 48/50 - Loss: 2673.9317 - Train Acc: 0.9389 - Val Acc: 0.8864\n",
      "💾 Best student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw20.pth\n",
      "[Distill] Epoch 49/50 - Loss: 2656.6440 - Train Acc: 0.9423 - Val Acc: 0.8326\n",
      "[Distill] Epoch 50/50 - Loss: 2666.0265 - Train Acc: 0.9430 - Val Acc: 0.8864\n",
      "\n",
      "📊 Final Classification Report (StudentCNN):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    AESCrypt       1.00      1.00      1.00        28\n",
      "      Cerber       0.95      0.88      0.91        84\n",
      "    Darkside       0.96      0.86      0.91       154\n",
      "       Excel       1.00      1.00      1.00        61\n",
      "     Firefox       0.99      0.96      0.97        91\n",
      "   GandCrab4       0.78      0.75      0.77       151\n",
      "        Ryuk       0.62      0.84      0.71        88\n",
      "     SDelete       0.96      1.00      0.98        24\n",
      "  Sodinokibi       0.91      0.92      0.92        91\n",
      "  TeslaCrypt       0.93      0.89      0.91        28\n",
      "    WannaCry       1.00      1.00      1.00        25\n",
      "         Zip       1.00      1.00      1.00        29\n",
      "\n",
      "    accuracy                           0.89       854\n",
      "   macro avg       0.93      0.93      0.92       854\n",
      "weighted avg       0.90      0.89      0.89       854\n",
      "\n",
      "✅ Student model saved: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/student/student_from_mlp_Td60_Tw20.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/3282436521.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student.load_state_dict(torch.load(save_path))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# === Configs ===\n",
    "NUM_FEATURES = 8\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "detection_times = [30, 45, 60]\n",
    "window_sizes = [10, 15, 20]\n",
    "\n",
    "base_dir = \"/home/HardDisk/Satang/thesis_proj\"\n",
    "teacher_dir = os.path.join(base_dir, \"Deep_Learning\", \"cross_archi\",\"mlp\", \"teacher\")  # ← not AE\n",
    "student_save_dir = os.path.join(base_dir, \"Deep_Learning\", \"cross_archi\",\"mlp\", \"student\")\n",
    "os.makedirs(student_save_dir, exist_ok=True)\n",
    "\n",
    "for T_d in detection_times:\n",
    "    for T_w in window_sizes:\n",
    "        T_len = T_d - T_w + 1\n",
    "        expected_shape = (T_len, NUM_FEATURES)\n",
    "\n",
    "        print(f\"\\n🚀 Distilling MLP-Mixer → StudentCNN for Td={T_d}, Tw={T_w} (T_len={T_len})\")\n",
    "\n",
    "        # === Paths\n",
    "        folder_name = f\"X_csv_split_{T_len}\"\n",
    "        input_dir = os.path.join(base_dir, f\"New_{T_d}\", f\"{T_w}\", \"split_tws\", folder_name)\n",
    "        train_path = os.path.join(input_dir, \"train\")\n",
    "        val_path   = os.path.join(input_dir, \"val\")\n",
    "        teacher_path = os.path.join(teacher_dir, f\"mlpmixer_Td{T_d}_Tw{T_w}.pth\")\n",
    "        student_path = os.path.join(student_save_dir, f\"student_from_mlp_Td{T_d}_Tw{T_w}.pth\")\n",
    "\n",
    "        # === 1. Load Data\n",
    "        X_train_raw, y_train_raw = load_split_from_folder(train_path, expected_shape)\n",
    "        X_val_raw, y_val_raw     = load_split_from_folder(val_path, expected_shape)\n",
    "\n",
    "        # === 2. Encode & Balance\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_train_encoded = label_encoder.fit_transform(y_train_raw)\n",
    "        X_train_flat = X_train_raw.reshape(X_train_raw.shape[0], -1)\n",
    "        X_resampled, y_resampled = SMOTE().fit_resample(X_train_flat, y_train_encoded)\n",
    "        X_train_bal = X_resampled.reshape(-1, T_len, NUM_FEATURES)\n",
    "        y_train_str = label_encoder.inverse_transform(y_resampled)\n",
    "\n",
    "        # === 3. Dataset & Dataloader\n",
    "        train_dataset = MultiStreamDataset(X_train_bal, y_train_str, label_encoder, augment=True)\n",
    "        val_dataset   = MultiStreamDataset(X_val_raw, y_val_raw, label_encoder, augment=False)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        class_weights_tensor = compute_class_weights(label_encoder.transform(y_train_str), device)\n",
    "\n",
    "        # === 4. Load MLP-Mixer Teacher\n",
    "        teacher = MLPMixerRansomwareClassifier(\n",
    "            seq_len=T_len,\n",
    "            feature_dim=NUM_FEATURES,\n",
    "            num_classes=len(label_encoder.classes_),\n",
    "            hidden_dim=128,\n",
    "\n",
    "        )\n",
    "        teacher.load_state_dict(torch.load(teacher_path, map_location=device))\n",
    "        teacher.to(device)\n",
    "        teacher.eval()\n",
    "        for p in teacher.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # === 5. Init StudentCNN\n",
    "        student = StudentCNN(\n",
    "            input_length=T_len,\n",
    "            num_classes=len(label_encoder.classes_)\n",
    "        ).to(device)\n",
    "\n",
    "        # === 6. Train with Knowledge Distillation\n",
    "        train_accs, val_accs, train_losses, val_losses = train_distilled(\n",
    "            student=student,\n",
    "            teacher=teacher,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            device=device,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            lr=0.01,\n",
    "            class_weights=class_weights_tensor,\n",
    "            T=3,\n",
    "            alpha=0.6,\n",
    "            beta=0.3,\n",
    "            gamma=0.3,\n",
    "            save_path=student_path\n",
    "        )\n",
    "\n",
    "        # === 7. Final Evaluation\n",
    "        student.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = student(inputs)  # (logits only)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        print(\"\\n📊 Final Classification Report (StudentCNN):\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "        print(f\"✅ Student model saved: {student_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "962da988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_student_baseline(student, train_loader, val_loader, device,\n",
    "                           epochs=50, lr=0.0005, class_weights=None,\n",
    "                           save_path=\"best_student_baseline.pth\"):\n",
    "\n",
    "    student.to(device)\n",
    "    optimizer = optim.Adam(student.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        student.train()\n",
    "        running_loss, correct = 0.0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            logits = student(inputs)  # 🔹 logits only for baseline\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)  # weighted sum\n",
    "            correct += (logits.argmax(1) == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_acc = correct / len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        # === Validation ===\n",
    "        student.eval()\n",
    "        val_running_loss, val_correct = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                logits = student(inputs)\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                val_correct += (logits.argmax(1) == labels).sum().item()\n",
    "\n",
    "        val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f\"[Baseline] Epoch {epoch+1:02d}/{epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f} - \"\n",
    "              f\"Train Acc: {train_acc:.4f} - Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(student.state_dict(), save_path)\n",
    "            print(f\"💾 Best standalone student model saved to: {save_path}\")\n",
    "\n",
    "    student.load_state_dict(torch.load(save_path))\n",
    "    return train_accuracies, val_accuracies, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c04a31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Training Standalone Student for Td=30, Tw=10 (T_len=21)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline] Epoch 01/50 - Train Loss: 2.2955 - Val Loss: 1.9555 - Train Acc: 0.1961 - Val Acc: 0.3141\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 02/50 - Train Loss: 1.5758 - Val Loss: 1.5551 - Train Acc: 0.4764 - Val Acc: 0.4765\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 03/50 - Train Loss: 1.2834 - Val Loss: 1.3521 - Train Acc: 0.5754 - Val Acc: 0.5476\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 04/50 - Train Loss: 1.0933 - Val Loss: 1.2085 - Train Acc: 0.6357 - Val Acc: 0.5691\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 05/50 - Train Loss: 0.9734 - Val Loss: 1.1339 - Train Acc: 0.6767 - Val Acc: 0.5736\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 06/50 - Train Loss: 0.8977 - Val Loss: 1.0486 - Train Acc: 0.6970 - Val Acc: 0.6022\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 07/50 - Train Loss: 0.8447 - Val Loss: 1.0255 - Train Acc: 0.7112 - Val Acc: 0.6328\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 08/50 - Train Loss: 0.7985 - Val Loss: 0.9756 - Train Acc: 0.7265 - Val Acc: 0.6473\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 09/50 - Train Loss: 0.7608 - Val Loss: 0.9483 - Train Acc: 0.7382 - Val Acc: 0.6548\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 10/50 - Train Loss: 0.7323 - Val Loss: 0.9042 - Train Acc: 0.7476 - Val Acc: 0.6804\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 11/50 - Train Loss: 0.7020 - Val Loss: 0.8792 - Train Acc: 0.7568 - Val Acc: 0.6844\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 12/50 - Train Loss: 0.6760 - Val Loss: 0.8794 - Train Acc: 0.7657 - Val Acc: 0.6809\n",
      "[Baseline] Epoch 13/50 - Train Loss: 0.6453 - Val Loss: 0.8432 - Train Acc: 0.7797 - Val Acc: 0.6894\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 14/50 - Train Loss: 0.6240 - Val Loss: 0.7967 - Train Acc: 0.7837 - Val Acc: 0.7305\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 15/50 - Train Loss: 0.6036 - Val Loss: 0.8094 - Train Acc: 0.7921 - Val Acc: 0.7079\n",
      "[Baseline] Epoch 16/50 - Train Loss: 0.5823 - Val Loss: 0.7980 - Train Acc: 0.7972 - Val Acc: 0.7169\n",
      "[Baseline] Epoch 17/50 - Train Loss: 0.5631 - Val Loss: 0.7585 - Train Acc: 0.8049 - Val Acc: 0.7390\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 18/50 - Train Loss: 0.5477 - Val Loss: 0.7261 - Train Acc: 0.8099 - Val Acc: 0.7395\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 19/50 - Train Loss: 0.5355 - Val Loss: 0.6974 - Train Acc: 0.8143 - Val Acc: 0.7781\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 20/50 - Train Loss: 0.5208 - Val Loss: 0.6931 - Train Acc: 0.8143 - Val Acc: 0.7745\n",
      "[Baseline] Epoch 21/50 - Train Loss: 0.5071 - Val Loss: 0.6726 - Train Acc: 0.8225 - Val Acc: 0.7751\n",
      "[Baseline] Epoch 22/50 - Train Loss: 0.4919 - Val Loss: 0.6583 - Train Acc: 0.8250 - Val Acc: 0.7806\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 23/50 - Train Loss: 0.4898 - Val Loss: 0.6984 - Train Acc: 0.8262 - Val Acc: 0.7255\n",
      "[Baseline] Epoch 24/50 - Train Loss: 0.4732 - Val Loss: 0.6547 - Train Acc: 0.8341 - Val Acc: 0.7605\n",
      "[Baseline] Epoch 25/50 - Train Loss: 0.4625 - Val Loss: 0.6271 - Train Acc: 0.8373 - Val Acc: 0.7811\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 26/50 - Train Loss: 0.4579 - Val Loss: 0.6506 - Train Acc: 0.8383 - Val Acc: 0.7415\n",
      "[Baseline] Epoch 27/50 - Train Loss: 0.4489 - Val Loss: 0.5939 - Train Acc: 0.8400 - Val Acc: 0.8046\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 28/50 - Train Loss: 0.4424 - Val Loss: 0.6091 - Train Acc: 0.8428 - Val Acc: 0.7876\n",
      "[Baseline] Epoch 29/50 - Train Loss: 0.4379 - Val Loss: 0.6243 - Train Acc: 0.8425 - Val Acc: 0.7791\n",
      "[Baseline] Epoch 30/50 - Train Loss: 0.4353 - Val Loss: 0.6060 - Train Acc: 0.8458 - Val Acc: 0.7766\n",
      "[Baseline] Epoch 31/50 - Train Loss: 0.4296 - Val Loss: 0.5630 - Train Acc: 0.8485 - Val Acc: 0.8166\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 32/50 - Train Loss: 0.4230 - Val Loss: 0.5686 - Train Acc: 0.8503 - Val Acc: 0.8156\n",
      "[Baseline] Epoch 33/50 - Train Loss: 0.4171 - Val Loss: 0.5607 - Train Acc: 0.8515 - Val Acc: 0.8096\n",
      "[Baseline] Epoch 34/50 - Train Loss: 0.4155 - Val Loss: 0.5508 - Train Acc: 0.8529 - Val Acc: 0.8151\n",
      "[Baseline] Epoch 35/50 - Train Loss: 0.4064 - Val Loss: 0.5541 - Train Acc: 0.8552 - Val Acc: 0.8071\n",
      "[Baseline] Epoch 36/50 - Train Loss: 0.4068 - Val Loss: 0.5179 - Train Acc: 0.8535 - Val Acc: 0.8272\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 37/50 - Train Loss: 0.3991 - Val Loss: 0.5400 - Train Acc: 0.8594 - Val Acc: 0.8236\n",
      "[Baseline] Epoch 38/50 - Train Loss: 0.3989 - Val Loss: 0.5140 - Train Acc: 0.8569 - Val Acc: 0.8252\n",
      "[Baseline] Epoch 39/50 - Train Loss: 0.3894 - Val Loss: 0.5077 - Train Acc: 0.8623 - Val Acc: 0.8422\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 40/50 - Train Loss: 0.3938 - Val Loss: 0.5665 - Train Acc: 0.8602 - Val Acc: 0.7861\n",
      "[Baseline] Epoch 41/50 - Train Loss: 0.3853 - Val Loss: 0.5140 - Train Acc: 0.8631 - Val Acc: 0.8267\n",
      "[Baseline] Epoch 42/50 - Train Loss: 0.3829 - Val Loss: 0.4751 - Train Acc: 0.8626 - Val Acc: 0.8522\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 43/50 - Train Loss: 0.3778 - Val Loss: 0.5094 - Train Acc: 0.8673 - Val Acc: 0.8292\n",
      "[Baseline] Epoch 44/50 - Train Loss: 0.3816 - Val Loss: 0.5003 - Train Acc: 0.8618 - Val Acc: 0.8397\n",
      "[Baseline] Epoch 45/50 - Train Loss: 0.3666 - Val Loss: 0.4770 - Train Acc: 0.8695 - Val Acc: 0.8402\n",
      "[Baseline] Epoch 46/50 - Train Loss: 0.3674 - Val Loss: 0.4906 - Train Acc: 0.8702 - Val Acc: 0.8422\n",
      "[Baseline] Epoch 47/50 - Train Loss: 0.3658 - Val Loss: 0.4677 - Train Acc: 0.8690 - Val Acc: 0.8527\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 48/50 - Train Loss: 0.3648 - Val Loss: 0.4953 - Train Acc: 0.8681 - Val Acc: 0.8417\n",
      "[Baseline] Epoch 49/50 - Train Loss: 0.3591 - Val Loss: 0.4537 - Train Acc: 0.8701 - Val Acc: 0.8572\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "[Baseline] Epoch 50/50 - Train Loss: 0.3576 - Val Loss: 0.4610 - Train Acc: 0.8719 - Val Acc: 0.8507\n",
      "✅ Saved standalone student model: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw10.pth\n",
      "\n",
      "🚀 Training Standalone Student for Td=30, Tw=15 (T_len=16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/3582298853.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student.load_state_dict(torch.load(save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline] Epoch 01/50 - Train Loss: 2.4214 - Val Loss: 2.2727 - Train Acc: 0.1514 - Val Acc: 0.2965\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 02/50 - Train Loss: 1.7378 - Val Loss: 1.6632 - Train Acc: 0.4360 - Val Acc: 0.4590\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 03/50 - Train Loss: 1.3216 - Val Loss: 1.4112 - Train Acc: 0.5892 - Val Acc: 0.5399\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 04/50 - Train Loss: 1.1076 - Val Loss: 1.2449 - Train Acc: 0.6565 - Val Acc: 0.5866\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 05/50 - Train Loss: 0.9676 - Val Loss: 1.1316 - Train Acc: 0.6974 - Val Acc: 0.6188\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 06/50 - Train Loss: 0.8796 - Val Loss: 1.0385 - Train Acc: 0.7185 - Val Acc: 0.6374\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 07/50 - Train Loss: 0.8123 - Val Loss: 0.9944 - Train Acc: 0.7338 - Val Acc: 0.6515\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 08/50 - Train Loss: 0.7616 - Val Loss: 0.9491 - Train Acc: 0.7500 - Val Acc: 0.6686\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 09/50 - Train Loss: 0.7219 - Val Loss: 0.9055 - Train Acc: 0.7634 - Val Acc: 0.6822\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 10/50 - Train Loss: 0.6843 - Val Loss: 0.8588 - Train Acc: 0.7748 - Val Acc: 0.6986\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 11/50 - Train Loss: 0.6536 - Val Loss: 0.8399 - Train Acc: 0.7846 - Val Acc: 0.7073\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 12/50 - Train Loss: 0.6243 - Val Loss: 0.7952 - Train Acc: 0.7931 - Val Acc: 0.7160\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 13/50 - Train Loss: 0.5979 - Val Loss: 0.7832 - Train Acc: 0.8007 - Val Acc: 0.7194\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 14/50 - Train Loss: 0.5816 - Val Loss: 0.7555 - Train Acc: 0.8070 - Val Acc: 0.7206\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 15/50 - Train Loss: 0.5581 - Val Loss: 0.7531 - Train Acc: 0.8154 - Val Acc: 0.7274\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 16/50 - Train Loss: 0.5434 - Val Loss: 0.7217 - Train Acc: 0.8185 - Val Acc: 0.7441\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 17/50 - Train Loss: 0.5317 - Val Loss: 0.7167 - Train Acc: 0.8228 - Val Acc: 0.7430\n",
      "[Baseline] Epoch 18/50 - Train Loss: 0.5178 - Val Loss: 0.7117 - Train Acc: 0.8264 - Val Acc: 0.7426\n",
      "[Baseline] Epoch 19/50 - Train Loss: 0.5060 - Val Loss: 0.6863 - Train Acc: 0.8295 - Val Acc: 0.7513\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 20/50 - Train Loss: 0.4937 - Val Loss: 0.6880 - Train Acc: 0.8322 - Val Acc: 0.7540\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 21/50 - Train Loss: 0.4821 - Val Loss: 0.6823 - Train Acc: 0.8383 - Val Acc: 0.7585\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 22/50 - Train Loss: 0.4734 - Val Loss: 0.6518 - Train Acc: 0.8401 - Val Acc: 0.7775\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 23/50 - Train Loss: 0.4669 - Val Loss: 0.6499 - Train Acc: 0.8414 - Val Acc: 0.7680\n",
      "[Baseline] Epoch 24/50 - Train Loss: 0.4556 - Val Loss: 0.6591 - Train Acc: 0.8459 - Val Acc: 0.7620\n",
      "[Baseline] Epoch 25/50 - Train Loss: 0.4511 - Val Loss: 0.6312 - Train Acc: 0.8478 - Val Acc: 0.7756\n",
      "[Baseline] Epoch 26/50 - Train Loss: 0.4444 - Val Loss: 0.6254 - Train Acc: 0.8484 - Val Acc: 0.7730\n",
      "[Baseline] Epoch 27/50 - Train Loss: 0.4393 - Val Loss: 0.6237 - Train Acc: 0.8497 - Val Acc: 0.7764\n",
      "[Baseline] Epoch 28/50 - Train Loss: 0.4266 - Val Loss: 0.6223 - Train Acc: 0.8545 - Val Acc: 0.7779\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 29/50 - Train Loss: 0.4239 - Val Loss: 0.5979 - Train Acc: 0.8545 - Val Acc: 0.7897\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 30/50 - Train Loss: 0.4189 - Val Loss: 0.6006 - Train Acc: 0.8553 - Val Acc: 0.7897\n",
      "[Baseline] Epoch 31/50 - Train Loss: 0.4130 - Val Loss: 0.5938 - Train Acc: 0.8564 - Val Acc: 0.7882\n",
      "[Baseline] Epoch 32/50 - Train Loss: 0.4066 - Val Loss: 0.5968 - Train Acc: 0.8594 - Val Acc: 0.7836\n",
      "[Baseline] Epoch 33/50 - Train Loss: 0.4004 - Val Loss: 0.5938 - Train Acc: 0.8609 - Val Acc: 0.7908\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 34/50 - Train Loss: 0.3978 - Val Loss: 0.5734 - Train Acc: 0.8607 - Val Acc: 0.7992\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 35/50 - Train Loss: 0.3926 - Val Loss: 0.5761 - Train Acc: 0.8654 - Val Acc: 0.7969\n",
      "[Baseline] Epoch 36/50 - Train Loss: 0.3888 - Val Loss: 0.5677 - Train Acc: 0.8651 - Val Acc: 0.8041\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 37/50 - Train Loss: 0.3835 - Val Loss: 0.5826 - Train Acc: 0.8660 - Val Acc: 0.7984\n",
      "[Baseline] Epoch 38/50 - Train Loss: 0.3788 - Val Loss: 0.5653 - Train Acc: 0.8681 - Val Acc: 0.8056\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 39/50 - Train Loss: 0.3764 - Val Loss: 0.5552 - Train Acc: 0.8690 - Val Acc: 0.8052\n",
      "[Baseline] Epoch 40/50 - Train Loss: 0.3723 - Val Loss: 0.5277 - Train Acc: 0.8687 - Val Acc: 0.8208\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 41/50 - Train Loss: 0.3705 - Val Loss: 0.5479 - Train Acc: 0.8717 - Val Acc: 0.8083\n",
      "[Baseline] Epoch 42/50 - Train Loss: 0.3681 - Val Loss: 0.5373 - Train Acc: 0.8709 - Val Acc: 0.8113\n",
      "[Baseline] Epoch 43/50 - Train Loss: 0.3624 - Val Loss: 0.5330 - Train Acc: 0.8746 - Val Acc: 0.8193\n",
      "[Baseline] Epoch 44/50 - Train Loss: 0.3610 - Val Loss: 0.5227 - Train Acc: 0.8727 - Val Acc: 0.8193\n",
      "[Baseline] Epoch 45/50 - Train Loss: 0.3605 - Val Loss: 0.5324 - Train Acc: 0.8717 - Val Acc: 0.8132\n",
      "[Baseline] Epoch 46/50 - Train Loss: 0.3538 - Val Loss: 0.5258 - Train Acc: 0.8769 - Val Acc: 0.8208\n",
      "[Baseline] Epoch 47/50 - Train Loss: 0.3491 - Val Loss: 0.5131 - Train Acc: 0.8770 - Val Acc: 0.8238\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 48/50 - Train Loss: 0.3476 - Val Loss: 0.5289 - Train Acc: 0.8775 - Val Acc: 0.8117\n",
      "[Baseline] Epoch 49/50 - Train Loss: 0.3456 - Val Loss: 0.5275 - Train Acc: 0.8788 - Val Acc: 0.8242\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "[Baseline] Epoch 50/50 - Train Loss: 0.3401 - Val Loss: 0.5360 - Train Acc: 0.8793 - Val Acc: 0.8098\n",
      "✅ Saved standalone student model: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw15.pth\n",
      "\n",
      "🚀 Training Standalone Student for Td=30, Tw=20 (T_len=11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/3582298853.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student.load_state_dict(torch.load(save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline] Epoch 01/50 - Train Loss: 1.9434 - Val Loss: 1.5120 - Train Acc: 0.3625 - Val Acc: 0.4929\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 02/50 - Train Loss: 1.1883 - Val Loss: 1.2060 - Train Acc: 0.6189 - Val Acc: 0.6096\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 03/50 - Train Loss: 0.9574 - Val Loss: 1.0419 - Train Acc: 0.6906 - Val Acc: 0.6421\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 04/50 - Train Loss: 0.8070 - Val Loss: 0.9267 - Train Acc: 0.7360 - Val Acc: 0.6728\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 05/50 - Train Loss: 0.7156 - Val Loss: 0.8885 - Train Acc: 0.7639 - Val Acc: 0.6889\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 06/50 - Train Loss: 0.6561 - Val Loss: 0.8271 - Train Acc: 0.7830 - Val Acc: 0.7071\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 07/50 - Train Loss: 0.6109 - Val Loss: 0.7861 - Train Acc: 0.7969 - Val Acc: 0.7275\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 08/50 - Train Loss: 0.5722 - Val Loss: 0.7172 - Train Acc: 0.8120 - Val Acc: 0.7463\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 09/50 - Train Loss: 0.5446 - Val Loss: 0.7240 - Train Acc: 0.8195 - Val Acc: 0.7375\n",
      "[Baseline] Epoch 10/50 - Train Loss: 0.5196 - Val Loss: 0.7018 - Train Acc: 0.8265 - Val Acc: 0.7514\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 11/50 - Train Loss: 0.5016 - Val Loss: 0.6964 - Train Acc: 0.8323 - Val Acc: 0.7545\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 12/50 - Train Loss: 0.4826 - Val Loss: 0.6634 - Train Acc: 0.8367 - Val Acc: 0.7572\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 13/50 - Train Loss: 0.4683 - Val Loss: 0.6302 - Train Acc: 0.8398 - Val Acc: 0.7696\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 14/50 - Train Loss: 0.4556 - Val Loss: 0.6097 - Train Acc: 0.8449 - Val Acc: 0.7750\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 15/50 - Train Loss: 0.4406 - Val Loss: 0.6015 - Train Acc: 0.8487 - Val Acc: 0.7730\n",
      "[Baseline] Epoch 16/50 - Train Loss: 0.4314 - Val Loss: 0.6092 - Train Acc: 0.8533 - Val Acc: 0.7765\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 17/50 - Train Loss: 0.4236 - Val Loss: 0.5948 - Train Acc: 0.8546 - Val Acc: 0.7887\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 18/50 - Train Loss: 0.4155 - Val Loss: 0.5796 - Train Acc: 0.8560 - Val Acc: 0.7887\n",
      "[Baseline] Epoch 19/50 - Train Loss: 0.4070 - Val Loss: 0.5426 - Train Acc: 0.8593 - Val Acc: 0.8055\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 20/50 - Train Loss: 0.3976 - Val Loss: 0.5526 - Train Acc: 0.8616 - Val Acc: 0.7893\n",
      "[Baseline] Epoch 21/50 - Train Loss: 0.3922 - Val Loss: 0.5486 - Train Acc: 0.8637 - Val Acc: 0.7960\n",
      "[Baseline] Epoch 22/50 - Train Loss: 0.3861 - Val Loss: 0.5431 - Train Acc: 0.8666 - Val Acc: 0.7983\n",
      "[Baseline] Epoch 23/50 - Train Loss: 0.3790 - Val Loss: 0.5149 - Train Acc: 0.8671 - Val Acc: 0.8170\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 24/50 - Train Loss: 0.3737 - Val Loss: 0.5379 - Train Acc: 0.8687 - Val Acc: 0.8036\n",
      "[Baseline] Epoch 25/50 - Train Loss: 0.3693 - Val Loss: 0.5191 - Train Acc: 0.8713 - Val Acc: 0.8063\n",
      "[Baseline] Epoch 26/50 - Train Loss: 0.3639 - Val Loss: 0.5108 - Train Acc: 0.8728 - Val Acc: 0.8143\n",
      "[Baseline] Epoch 27/50 - Train Loss: 0.3573 - Val Loss: 0.5114 - Train Acc: 0.8754 - Val Acc: 0.8170\n",
      "[Baseline] Epoch 28/50 - Train Loss: 0.3547 - Val Loss: 0.4841 - Train Acc: 0.8750 - Val Acc: 0.8296\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 29/50 - Train Loss: 0.3499 - Val Loss: 0.4708 - Train Acc: 0.8774 - Val Acc: 0.8312\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 30/50 - Train Loss: 0.3451 - Val Loss: 0.5225 - Train Acc: 0.8792 - Val Acc: 0.8141\n",
      "[Baseline] Epoch 31/50 - Train Loss: 0.3425 - Val Loss: 0.4888 - Train Acc: 0.8791 - Val Acc: 0.8268\n",
      "[Baseline] Epoch 32/50 - Train Loss: 0.3355 - Val Loss: 0.4825 - Train Acc: 0.8835 - Val Acc: 0.8348\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 33/50 - Train Loss: 0.3358 - Val Loss: 0.4799 - Train Acc: 0.8816 - Val Acc: 0.8285\n",
      "[Baseline] Epoch 34/50 - Train Loss: 0.3300 - Val Loss: 0.4925 - Train Acc: 0.8833 - Val Acc: 0.8220\n",
      "[Baseline] Epoch 35/50 - Train Loss: 0.3274 - Val Loss: 0.4650 - Train Acc: 0.8842 - Val Acc: 0.8371\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 36/50 - Train Loss: 0.3240 - Val Loss: 0.4568 - Train Acc: 0.8857 - Val Acc: 0.8356\n",
      "[Baseline] Epoch 37/50 - Train Loss: 0.3196 - Val Loss: 0.4580 - Train Acc: 0.8864 - Val Acc: 0.8369\n",
      "[Baseline] Epoch 38/50 - Train Loss: 0.3189 - Val Loss: 0.4692 - Train Acc: 0.8862 - Val Acc: 0.8321\n",
      "[Baseline] Epoch 39/50 - Train Loss: 0.3147 - Val Loss: 0.4641 - Train Acc: 0.8889 - Val Acc: 0.8369\n",
      "[Baseline] Epoch 40/50 - Train Loss: 0.3120 - Val Loss: 0.4601 - Train Acc: 0.8894 - Val Acc: 0.8358\n",
      "[Baseline] Epoch 41/50 - Train Loss: 0.3069 - Val Loss: 0.4361 - Train Acc: 0.8916 - Val Acc: 0.8447\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 42/50 - Train Loss: 0.3046 - Val Loss: 0.4475 - Train Acc: 0.8917 - Val Acc: 0.8417\n",
      "[Baseline] Epoch 43/50 - Train Loss: 0.3033 - Val Loss: 0.4510 - Train Acc: 0.8913 - Val Acc: 0.8402\n",
      "[Baseline] Epoch 44/50 - Train Loss: 0.3037 - Val Loss: 0.4466 - Train Acc: 0.8925 - Val Acc: 0.8432\n",
      "[Baseline] Epoch 45/50 - Train Loss: 0.2982 - Val Loss: 0.4419 - Train Acc: 0.8929 - Val Acc: 0.8518\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 46/50 - Train Loss: 0.2967 - Val Loss: 0.4170 - Train Acc: 0.8936 - Val Acc: 0.8539\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "[Baseline] Epoch 47/50 - Train Loss: 0.2931 - Val Loss: 0.4494 - Train Acc: 0.8958 - Val Acc: 0.8400\n",
      "[Baseline] Epoch 48/50 - Train Loss: 0.2921 - Val Loss: 0.4595 - Train Acc: 0.8964 - Val Acc: 0.8359\n",
      "[Baseline] Epoch 49/50 - Train Loss: 0.2906 - Val Loss: 0.4346 - Train Acc: 0.8959 - Val Acc: 0.8442\n",
      "[Baseline] Epoch 50/50 - Train Loss: 0.2867 - Val Loss: 0.4364 - Train Acc: 0.8972 - Val Acc: 0.8499\n",
      "✅ Saved standalone student model: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td30_Tw20.pth\n",
      "\n",
      "🚀 Training Standalone Student for Td=45, Tw=10 (T_len=36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/3582298853.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student.load_state_dict(torch.load(save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline] Epoch 01/50 - Train Loss: 2.4644 - Val Loss: 2.3780 - Train Acc: 0.1291 - Val Acc: 0.2800\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 02/50 - Train Loss: 2.1455 - Val Loss: 1.9063 - Train Acc: 0.3244 - Val Acc: 0.4331\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 03/50 - Train Loss: 1.5489 - Val Loss: 1.5661 - Train Acc: 0.5639 - Val Acc: 0.5436\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 04/50 - Train Loss: 1.2418 - Val Loss: 1.3263 - Train Acc: 0.6606 - Val Acc: 0.6085\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 05/50 - Train Loss: 1.0450 - Val Loss: 1.1983 - Train Acc: 0.7193 - Val Acc: 0.6240\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 06/50 - Train Loss: 0.9178 - Val Loss: 1.0849 - Train Acc: 0.7491 - Val Acc: 0.6434\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 07/50 - Train Loss: 0.8354 - Val Loss: 1.0189 - Train Acc: 0.7666 - Val Acc: 0.6521\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 08/50 - Train Loss: 0.7703 - Val Loss: 0.9590 - Train Acc: 0.7778 - Val Acc: 0.6725\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 09/50 - Train Loss: 0.7167 - Val Loss: 0.9321 - Train Acc: 0.7864 - Val Acc: 0.6725\n",
      "[Baseline] Epoch 10/50 - Train Loss: 0.6746 - Val Loss: 0.8975 - Train Acc: 0.7972 - Val Acc: 0.6754\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 11/50 - Train Loss: 0.6386 - Val Loss: 0.8405 - Train Acc: 0.7978 - Val Acc: 0.6957\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 12/50 - Train Loss: 0.6098 - Val Loss: 0.8060 - Train Acc: 0.8063 - Val Acc: 0.7103\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 13/50 - Train Loss: 0.5819 - Val Loss: 0.8121 - Train Acc: 0.8135 - Val Acc: 0.6996\n",
      "[Baseline] Epoch 14/50 - Train Loss: 0.5539 - Val Loss: 0.7728 - Train Acc: 0.8193 - Val Acc: 0.7132\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 15/50 - Train Loss: 0.5353 - Val Loss: 0.7694 - Train Acc: 0.8237 - Val Acc: 0.7093\n",
      "[Baseline] Epoch 16/50 - Train Loss: 0.5162 - Val Loss: 0.7525 - Train Acc: 0.8303 - Val Acc: 0.7297\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 17/50 - Train Loss: 0.5038 - Val Loss: 0.7492 - Train Acc: 0.8325 - Val Acc: 0.7112\n",
      "[Baseline] Epoch 18/50 - Train Loss: 0.4853 - Val Loss: 0.7088 - Train Acc: 0.8414 - Val Acc: 0.7442\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 19/50 - Train Loss: 0.4676 - Val Loss: 0.7157 - Train Acc: 0.8430 - Val Acc: 0.7384\n",
      "[Baseline] Epoch 20/50 - Train Loss: 0.4560 - Val Loss: 0.6791 - Train Acc: 0.8462 - Val Acc: 0.7529\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 21/50 - Train Loss: 0.4418 - Val Loss: 0.6614 - Train Acc: 0.8515 - Val Acc: 0.7587\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 22/50 - Train Loss: 0.4348 - Val Loss: 0.6580 - Train Acc: 0.8527 - Val Acc: 0.7597\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 23/50 - Train Loss: 0.4181 - Val Loss: 0.6635 - Train Acc: 0.8596 - Val Acc: 0.7519\n",
      "[Baseline] Epoch 24/50 - Train Loss: 0.4150 - Val Loss: 0.6331 - Train Acc: 0.8622 - Val Acc: 0.7636\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 25/50 - Train Loss: 0.4002 - Val Loss: 0.6370 - Train Acc: 0.8677 - Val Acc: 0.7607\n",
      "[Baseline] Epoch 26/50 - Train Loss: 0.3907 - Val Loss: 0.6154 - Train Acc: 0.8665 - Val Acc: 0.7752\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 27/50 - Train Loss: 0.3820 - Val Loss: 0.6150 - Train Acc: 0.8729 - Val Acc: 0.7781\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 28/50 - Train Loss: 0.3809 - Val Loss: 0.6021 - Train Acc: 0.8735 - Val Acc: 0.7878\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 29/50 - Train Loss: 0.3690 - Val Loss: 0.5816 - Train Acc: 0.8775 - Val Acc: 0.7868\n",
      "[Baseline] Epoch 30/50 - Train Loss: 0.3620 - Val Loss: 0.5630 - Train Acc: 0.8816 - Val Acc: 0.7994\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 31/50 - Train Loss: 0.3538 - Val Loss: 0.5561 - Train Acc: 0.8802 - Val Acc: 0.8052\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 32/50 - Train Loss: 0.3551 - Val Loss: 0.5716 - Train Acc: 0.8793 - Val Acc: 0.7946\n",
      "[Baseline] Epoch 33/50 - Train Loss: 0.3433 - Val Loss: 0.5625 - Train Acc: 0.8839 - Val Acc: 0.8023\n",
      "[Baseline] Epoch 34/50 - Train Loss: 0.3371 - Val Loss: 0.5486 - Train Acc: 0.8866 - Val Acc: 0.8052\n",
      "[Baseline] Epoch 35/50 - Train Loss: 0.3311 - Val Loss: 0.5349 - Train Acc: 0.8922 - Val Acc: 0.8072\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 36/50 - Train Loss: 0.3253 - Val Loss: 0.5445 - Train Acc: 0.8917 - Val Acc: 0.8091\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 37/50 - Train Loss: 0.3214 - Val Loss: 0.5497 - Train Acc: 0.8919 - Val Acc: 0.7888\n",
      "[Baseline] Epoch 38/50 - Train Loss: 0.3148 - Val Loss: 0.5137 - Train Acc: 0.8952 - Val Acc: 0.8110\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 39/50 - Train Loss: 0.3175 - Val Loss: 0.5023 - Train Acc: 0.8955 - Val Acc: 0.8236\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 40/50 - Train Loss: 0.3057 - Val Loss: 0.5089 - Train Acc: 0.8986 - Val Acc: 0.8130\n",
      "[Baseline] Epoch 41/50 - Train Loss: 0.3049 - Val Loss: 0.4872 - Train Acc: 0.9010 - Val Acc: 0.8353\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 42/50 - Train Loss: 0.2981 - Val Loss: 0.4805 - Train Acc: 0.9028 - Val Acc: 0.8343\n",
      "[Baseline] Epoch 43/50 - Train Loss: 0.2934 - Val Loss: 0.4960 - Train Acc: 0.9037 - Val Acc: 0.8343\n",
      "[Baseline] Epoch 44/50 - Train Loss: 0.2916 - Val Loss: 0.4791 - Train Acc: 0.9044 - Val Acc: 0.8372\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 45/50 - Train Loss: 0.2966 - Val Loss: 0.4718 - Train Acc: 0.9016 - Val Acc: 0.8391\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "[Baseline] Epoch 46/50 - Train Loss: 0.2847 - Val Loss: 0.4618 - Train Acc: 0.9075 - Val Acc: 0.8372\n",
      "[Baseline] Epoch 47/50 - Train Loss: 0.2781 - Val Loss: 0.4791 - Train Acc: 0.9096 - Val Acc: 0.8353\n",
      "[Baseline] Epoch 48/50 - Train Loss: 0.2764 - Val Loss: 0.4868 - Train Acc: 0.9101 - Val Acc: 0.8266\n",
      "[Baseline] Epoch 49/50 - Train Loss: 0.2703 - Val Loss: 0.4772 - Train Acc: 0.9144 - Val Acc: 0.8372\n",
      "[Baseline] Epoch 50/50 - Train Loss: 0.2720 - Val Loss: 0.4598 - Train Acc: 0.9117 - Val Acc: 0.8498\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "✅ Saved standalone student model: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw10.pth\n",
      "\n",
      "🚀 Training Standalone Student for Td=45, Tw=15 (T_len=31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/3582298853.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student.load_state_dict(torch.load(save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline] Epoch 01/50 - Train Loss: 2.4684 - Val Loss: 2.4031 - Train Acc: 0.1145 - Val Acc: 0.2330\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 02/50 - Train Loss: 2.1041 - Val Loss: 1.8262 - Train Acc: 0.2998 - Val Acc: 0.4449\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 03/50 - Train Loss: 1.3963 - Val Loss: 1.4145 - Train Acc: 0.6209 - Val Acc: 0.5763\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 04/50 - Train Loss: 1.1066 - Val Loss: 1.2706 - Train Acc: 0.6920 - Val Acc: 0.6012\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 05/50 - Train Loss: 0.9595 - Val Loss: 1.1401 - Train Acc: 0.7205 - Val Acc: 0.6224\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 06/50 - Train Loss: 0.8615 - Val Loss: 1.0764 - Train Acc: 0.7413 - Val Acc: 0.6267\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 07/50 - Train Loss: 0.7897 - Val Loss: 1.0142 - Train Acc: 0.7530 - Val Acc: 0.6392\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 08/50 - Train Loss: 0.7355 - Val Loss: 0.9685 - Train Acc: 0.7664 - Val Acc: 0.6523\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 09/50 - Train Loss: 0.6888 - Val Loss: 0.9151 - Train Acc: 0.7795 - Val Acc: 0.6640\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 10/50 - Train Loss: 0.6531 - Val Loss: 0.9101 - Train Acc: 0.7845 - Val Acc: 0.6640\n",
      "[Baseline] Epoch 11/50 - Train Loss: 0.6226 - Val Loss: 0.8581 - Train Acc: 0.7922 - Val Acc: 0.6793\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 12/50 - Train Loss: 0.5925 - Val Loss: 0.8263 - Train Acc: 0.8041 - Val Acc: 0.6932\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 13/50 - Train Loss: 0.5717 - Val Loss: 0.8169 - Train Acc: 0.8120 - Val Acc: 0.6947\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 14/50 - Train Loss: 0.5473 - Val Loss: 0.7808 - Train Acc: 0.8183 - Val Acc: 0.7064\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 15/50 - Train Loss: 0.5219 - Val Loss: 0.7814 - Train Acc: 0.8260 - Val Acc: 0.7078\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 16/50 - Train Loss: 0.5053 - Val Loss: 0.7283 - Train Acc: 0.8325 - Val Acc: 0.7290\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 17/50 - Train Loss: 0.4859 - Val Loss: 0.7261 - Train Acc: 0.8412 - Val Acc: 0.7253\n",
      "[Baseline] Epoch 18/50 - Train Loss: 0.4695 - Val Loss: 0.7082 - Train Acc: 0.8456 - Val Acc: 0.7392\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 19/50 - Train Loss: 0.4545 - Val Loss: 0.6989 - Train Acc: 0.8529 - Val Acc: 0.7370\n",
      "[Baseline] Epoch 20/50 - Train Loss: 0.4394 - Val Loss: 0.6827 - Train Acc: 0.8562 - Val Acc: 0.7443\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 21/50 - Train Loss: 0.4300 - Val Loss: 0.6663 - Train Acc: 0.8592 - Val Acc: 0.7487\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 22/50 - Train Loss: 0.4161 - Val Loss: 0.6619 - Train Acc: 0.8645 - Val Acc: 0.7516\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 23/50 - Train Loss: 0.4019 - Val Loss: 0.6190 - Train Acc: 0.8689 - Val Acc: 0.7911\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 24/50 - Train Loss: 0.3918 - Val Loss: 0.6063 - Train Acc: 0.8705 - Val Acc: 0.7801\n",
      "[Baseline] Epoch 25/50 - Train Loss: 0.3826 - Val Loss: 0.5918 - Train Acc: 0.8752 - Val Acc: 0.8093\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 26/50 - Train Loss: 0.3748 - Val Loss: 0.5999 - Train Acc: 0.8789 - Val Acc: 0.7940\n",
      "[Baseline] Epoch 27/50 - Train Loss: 0.3623 - Val Loss: 0.5924 - Train Acc: 0.8840 - Val Acc: 0.7845\n",
      "[Baseline] Epoch 28/50 - Train Loss: 0.3547 - Val Loss: 0.5864 - Train Acc: 0.8868 - Val Acc: 0.8028\n",
      "[Baseline] Epoch 29/50 - Train Loss: 0.3500 - Val Loss: 0.5968 - Train Acc: 0.8858 - Val Acc: 0.7889\n",
      "[Baseline] Epoch 30/50 - Train Loss: 0.3374 - Val Loss: 0.5712 - Train Acc: 0.8924 - Val Acc: 0.7999\n",
      "[Baseline] Epoch 31/50 - Train Loss: 0.3286 - Val Loss: 0.5471 - Train Acc: 0.8949 - Val Acc: 0.8240\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 32/50 - Train Loss: 0.3225 - Val Loss: 0.5454 - Train Acc: 0.8987 - Val Acc: 0.8086\n",
      "[Baseline] Epoch 33/50 - Train Loss: 0.3195 - Val Loss: 0.5337 - Train Acc: 0.8991 - Val Acc: 0.8276\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 34/50 - Train Loss: 0.3161 - Val Loss: 0.5217 - Train Acc: 0.9002 - Val Acc: 0.8342\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 35/50 - Train Loss: 0.3063 - Val Loss: 0.5138 - Train Acc: 0.9030 - Val Acc: 0.8378\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 36/50 - Train Loss: 0.3047 - Val Loss: 0.5327 - Train Acc: 0.9032 - Val Acc: 0.8167\n",
      "[Baseline] Epoch 37/50 - Train Loss: 0.2985 - Val Loss: 0.5090 - Train Acc: 0.9054 - Val Acc: 0.8400\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 38/50 - Train Loss: 0.2919 - Val Loss: 0.4903 - Train Acc: 0.9067 - Val Acc: 0.8495\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 39/50 - Train Loss: 0.2880 - Val Loss: 0.4922 - Train Acc: 0.9098 - Val Acc: 0.8473\n",
      "[Baseline] Epoch 40/50 - Train Loss: 0.2853 - Val Loss: 0.4977 - Train Acc: 0.9078 - Val Acc: 0.8327\n",
      "[Baseline] Epoch 41/50 - Train Loss: 0.2805 - Val Loss: 0.4818 - Train Acc: 0.9115 - Val Acc: 0.8517\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 42/50 - Train Loss: 0.2762 - Val Loss: 0.4976 - Train Acc: 0.9125 - Val Acc: 0.8262\n",
      "[Baseline] Epoch 43/50 - Train Loss: 0.2705 - Val Loss: 0.4831 - Train Acc: 0.9144 - Val Acc: 0.8517\n",
      "[Baseline] Epoch 44/50 - Train Loss: 0.2650 - Val Loss: 0.4703 - Train Acc: 0.9148 - Val Acc: 0.8590\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 45/50 - Train Loss: 0.2642 - Val Loss: 0.4880 - Train Acc: 0.9148 - Val Acc: 0.8327\n",
      "[Baseline] Epoch 46/50 - Train Loss: 0.2650 - Val Loss: 0.4557 - Train Acc: 0.9142 - Val Acc: 0.8634\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 47/50 - Train Loss: 0.2559 - Val Loss: 0.4431 - Train Acc: 0.9194 - Val Acc: 0.8663\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "[Baseline] Epoch 48/50 - Train Loss: 0.2557 - Val Loss: 0.4608 - Train Acc: 0.9186 - Val Acc: 0.8649\n",
      "[Baseline] Epoch 49/50 - Train Loss: 0.2503 - Val Loss: 0.4633 - Train Acc: 0.9192 - Val Acc: 0.8590\n",
      "[Baseline] Epoch 50/50 - Train Loss: 0.2494 - Val Loss: 0.4576 - Train Acc: 0.9205 - Val Acc: 0.8598\n",
      "✅ Saved standalone student model: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw15.pth\n",
      "\n",
      "🚀 Training Standalone Student for Td=45, Tw=20 (T_len=26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/3582298853.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student.load_state_dict(torch.load(save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline] Epoch 01/50 - Train Loss: 2.4238 - Val Loss: 2.3271 - Train Acc: 0.1606 - Val Acc: 0.2069\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 02/50 - Train Loss: 1.8648 - Val Loss: 1.7529 - Train Acc: 0.3842 - Val Acc: 0.4265\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 03/50 - Train Loss: 1.4441 - Val Loss: 1.5021 - Train Acc: 0.5550 - Val Acc: 0.5028\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 04/50 - Train Loss: 1.2256 - Val Loss: 1.3425 - Train Acc: 0.6208 - Val Acc: 0.5356\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 05/50 - Train Loss: 1.0821 - Val Loss: 1.2184 - Train Acc: 0.6523 - Val Acc: 0.5716\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 06/50 - Train Loss: 0.9740 - Val Loss: 1.1633 - Train Acc: 0.6891 - Val Acc: 0.5950\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 07/50 - Train Loss: 0.8946 - Val Loss: 1.0567 - Train Acc: 0.7079 - Val Acc: 0.6208\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 08/50 - Train Loss: 0.8199 - Val Loss: 0.9842 - Train Acc: 0.7336 - Val Acc: 0.6574\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 09/50 - Train Loss: 0.7605 - Val Loss: 0.9434 - Train Acc: 0.7510 - Val Acc: 0.6568\n",
      "[Baseline] Epoch 10/50 - Train Loss: 0.7139 - Val Loss: 0.9100 - Train Acc: 0.7628 - Val Acc: 0.6738\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 11/50 - Train Loss: 0.6730 - Val Loss: 0.8768 - Train Acc: 0.7790 - Val Acc: 0.6801\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 12/50 - Train Loss: 0.6373 - Val Loss: 0.8328 - Train Acc: 0.7906 - Val Acc: 0.6965\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 13/50 - Train Loss: 0.6078 - Val Loss: 0.8129 - Train Acc: 0.7978 - Val Acc: 0.7079\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 14/50 - Train Loss: 0.5814 - Val Loss: 0.7982 - Train Acc: 0.8077 - Val Acc: 0.7073\n",
      "[Baseline] Epoch 15/50 - Train Loss: 0.5578 - Val Loss: 0.7589 - Train Acc: 0.8152 - Val Acc: 0.7117\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 16/50 - Train Loss: 0.5364 - Val Loss: 0.7302 - Train Acc: 0.8198 - Val Acc: 0.7407\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 17/50 - Train Loss: 0.5182 - Val Loss: 0.6858 - Train Acc: 0.8271 - Val Acc: 0.7539\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 18/50 - Train Loss: 0.4988 - Val Loss: 0.6667 - Train Acc: 0.8350 - Val Acc: 0.7653\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 19/50 - Train Loss: 0.4811 - Val Loss: 0.6690 - Train Acc: 0.8398 - Val Acc: 0.7527\n",
      "[Baseline] Epoch 20/50 - Train Loss: 0.4646 - Val Loss: 0.6541 - Train Acc: 0.8455 - Val Acc: 0.7685\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 21/50 - Train Loss: 0.4510 - Val Loss: 0.6525 - Train Acc: 0.8492 - Val Acc: 0.7640\n",
      "[Baseline] Epoch 22/50 - Train Loss: 0.4426 - Val Loss: 0.6204 - Train Acc: 0.8545 - Val Acc: 0.7779\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 23/50 - Train Loss: 0.4291 - Val Loss: 0.6184 - Train Acc: 0.8578 - Val Acc: 0.7760\n",
      "[Baseline] Epoch 24/50 - Train Loss: 0.4155 - Val Loss: 0.5980 - Train Acc: 0.8608 - Val Acc: 0.7849\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 25/50 - Train Loss: 0.4030 - Val Loss: 0.5958 - Train Acc: 0.8660 - Val Acc: 0.7767\n",
      "[Baseline] Epoch 26/50 - Train Loss: 0.3919 - Val Loss: 0.5827 - Train Acc: 0.8675 - Val Acc: 0.7817\n",
      "[Baseline] Epoch 27/50 - Train Loss: 0.3818 - Val Loss: 0.5475 - Train Acc: 0.8743 - Val Acc: 0.7981\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 28/50 - Train Loss: 0.3735 - Val Loss: 0.5503 - Train Acc: 0.8752 - Val Acc: 0.7975\n",
      "[Baseline] Epoch 29/50 - Train Loss: 0.3636 - Val Loss: 0.5562 - Train Acc: 0.8781 - Val Acc: 0.7975\n",
      "[Baseline] Epoch 30/50 - Train Loss: 0.3575 - Val Loss: 0.5290 - Train Acc: 0.8786 - Val Acc: 0.8095\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 31/50 - Train Loss: 0.3490 - Val Loss: 0.5415 - Train Acc: 0.8816 - Val Acc: 0.7994\n",
      "[Baseline] Epoch 32/50 - Train Loss: 0.3434 - Val Loss: 0.5037 - Train Acc: 0.8846 - Val Acc: 0.8132\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 33/50 - Train Loss: 0.3333 - Val Loss: 0.5276 - Train Acc: 0.8892 - Val Acc: 0.8095\n",
      "[Baseline] Epoch 34/50 - Train Loss: 0.3307 - Val Loss: 0.5020 - Train Acc: 0.8866 - Val Acc: 0.8132\n",
      "[Baseline] Epoch 35/50 - Train Loss: 0.3221 - Val Loss: 0.4977 - Train Acc: 0.8899 - Val Acc: 0.8126\n",
      "[Baseline] Epoch 36/50 - Train Loss: 0.3164 - Val Loss: 0.4810 - Train Acc: 0.8935 - Val Acc: 0.8170\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 37/50 - Train Loss: 0.3104 - Val Loss: 0.4793 - Train Acc: 0.8939 - Val Acc: 0.8290\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 38/50 - Train Loss: 0.3022 - Val Loss: 0.4738 - Train Acc: 0.8963 - Val Acc: 0.8303\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 39/50 - Train Loss: 0.3009 - Val Loss: 0.4762 - Train Acc: 0.8977 - Val Acc: 0.8208\n",
      "[Baseline] Epoch 40/50 - Train Loss: 0.2943 - Val Loss: 0.4707 - Train Acc: 0.8995 - Val Acc: 0.8252\n",
      "[Baseline] Epoch 41/50 - Train Loss: 0.2924 - Val Loss: 0.4517 - Train Acc: 0.9024 - Val Acc: 0.8379\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 42/50 - Train Loss: 0.2899 - Val Loss: 0.4528 - Train Acc: 0.9004 - Val Acc: 0.8328\n",
      "[Baseline] Epoch 43/50 - Train Loss: 0.2822 - Val Loss: 0.4695 - Train Acc: 0.9045 - Val Acc: 0.8366\n",
      "[Baseline] Epoch 44/50 - Train Loss: 0.2788 - Val Loss: 0.4360 - Train Acc: 0.9048 - Val Acc: 0.8416\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 45/50 - Train Loss: 0.2744 - Val Loss: 0.4293 - Train Acc: 0.9057 - Val Acc: 0.8461\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 46/50 - Train Loss: 0.2715 - Val Loss: 0.4317 - Train Acc: 0.9073 - Val Acc: 0.8479\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 47/50 - Train Loss: 0.2654 - Val Loss: 0.4326 - Train Acc: 0.9094 - Val Acc: 0.8442\n",
      "[Baseline] Epoch 48/50 - Train Loss: 0.2620 - Val Loss: 0.4207 - Train Acc: 0.9122 - Val Acc: 0.8479\n",
      "[Baseline] Epoch 49/50 - Train Loss: 0.2601 - Val Loss: 0.4164 - Train Acc: 0.9115 - Val Acc: 0.8568\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "[Baseline] Epoch 50/50 - Train Loss: 0.2600 - Val Loss: 0.4112 - Train Acc: 0.9115 - Val Acc: 0.8473\n",
      "✅ Saved standalone student model: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td45_Tw20.pth\n",
      "\n",
      "🚀 Training Standalone Student for Td=60, Tw=10 (T_len=51)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/3582298853.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student.load_state_dict(torch.load(save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline] Epoch 01/50 - Train Loss: 2.4830 - Val Loss: 2.4601 - Train Acc: 0.1280 - Val Acc: 0.2285\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 02/50 - Train Loss: 2.3813 - Val Loss: 2.3890 - Train Acc: 0.2088 - Val Acc: 0.2300\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 03/50 - Train Loss: 2.0930 - Val Loss: 2.0003 - Train Acc: 0.3222 - Val Acc: 0.3275\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 04/50 - Train Loss: 1.5852 - Val Loss: 1.6131 - Train Acc: 0.5313 - Val Acc: 0.5546\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 05/50 - Train Loss: 1.3102 - Val Loss: 1.4437 - Train Acc: 0.6434 - Val Acc: 0.5852\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 06/50 - Train Loss: 1.1379 - Val Loss: 1.2745 - Train Acc: 0.7076 - Val Acc: 0.6172\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 07/50 - Train Loss: 1.0229 - Val Loss: 1.1683 - Train Acc: 0.7302 - Val Acc: 0.6507\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 08/50 - Train Loss: 0.9343 - Val Loss: 1.0971 - Train Acc: 0.7506 - Val Acc: 0.6376\n",
      "[Baseline] Epoch 09/50 - Train Loss: 0.8565 - Val Loss: 1.0227 - Train Acc: 0.7660 - Val Acc: 0.6638\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 10/50 - Train Loss: 0.7909 - Val Loss: 0.9420 - Train Acc: 0.7812 - Val Acc: 0.6827\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 11/50 - Train Loss: 0.7400 - Val Loss: 0.9021 - Train Acc: 0.7944 - Val Acc: 0.6929\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 12/50 - Train Loss: 0.6924 - Val Loss: 0.8738 - Train Acc: 0.8041 - Val Acc: 0.6885\n",
      "[Baseline] Epoch 13/50 - Train Loss: 0.6491 - Val Loss: 0.8123 - Train Acc: 0.8091 - Val Acc: 0.7176\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 14/50 - Train Loss: 0.6149 - Val Loss: 0.8205 - Train Acc: 0.8196 - Val Acc: 0.7016\n",
      "[Baseline] Epoch 15/50 - Train Loss: 0.5817 - Val Loss: 0.7505 - Train Acc: 0.8260 - Val Acc: 0.7511\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 16/50 - Train Loss: 0.5506 - Val Loss: 0.7102 - Train Acc: 0.8342 - Val Acc: 0.7409\n",
      "[Baseline] Epoch 17/50 - Train Loss: 0.5267 - Val Loss: 0.7220 - Train Acc: 0.8406 - Val Acc: 0.7394\n",
      "[Baseline] Epoch 18/50 - Train Loss: 0.5046 - Val Loss: 0.6610 - Train Acc: 0.8467 - Val Acc: 0.7729\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 19/50 - Train Loss: 0.4900 - Val Loss: 0.6548 - Train Acc: 0.8479 - Val Acc: 0.7613\n",
      "[Baseline] Epoch 20/50 - Train Loss: 0.4728 - Val Loss: 0.6389 - Train Acc: 0.8525 - Val Acc: 0.7642\n",
      "[Baseline] Epoch 21/50 - Train Loss: 0.4539 - Val Loss: 0.6180 - Train Acc: 0.8548 - Val Acc: 0.7744\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 22/50 - Train Loss: 0.4376 - Val Loss: 0.6212 - Train Acc: 0.8619 - Val Acc: 0.7569\n",
      "[Baseline] Epoch 23/50 - Train Loss: 0.4283 - Val Loss: 0.5924 - Train Acc: 0.8644 - Val Acc: 0.7904\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 24/50 - Train Loss: 0.4162 - Val Loss: 0.5724 - Train Acc: 0.8695 - Val Acc: 0.7948\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 25/50 - Train Loss: 0.4053 - Val Loss: 0.5661 - Train Acc: 0.8696 - Val Acc: 0.8006\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 26/50 - Train Loss: 0.3936 - Val Loss: 0.5599 - Train Acc: 0.8747 - Val Acc: 0.8035\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 27/50 - Train Loss: 0.3876 - Val Loss: 0.5521 - Train Acc: 0.8759 - Val Acc: 0.8006\n",
      "[Baseline] Epoch 28/50 - Train Loss: 0.3753 - Val Loss: 0.5427 - Train Acc: 0.8797 - Val Acc: 0.8020\n",
      "[Baseline] Epoch 29/50 - Train Loss: 0.3673 - Val Loss: 0.5335 - Train Acc: 0.8804 - Val Acc: 0.8064\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 30/50 - Train Loss: 0.3601 - Val Loss: 0.5334 - Train Acc: 0.8851 - Val Acc: 0.8049\n",
      "[Baseline] Epoch 31/50 - Train Loss: 0.3544 - Val Loss: 0.5243 - Train Acc: 0.8852 - Val Acc: 0.8210\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 32/50 - Train Loss: 0.3508 - Val Loss: 0.5148 - Train Acc: 0.8862 - Val Acc: 0.8224\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 33/50 - Train Loss: 0.3436 - Val Loss: 0.5260 - Train Acc: 0.8876 - Val Acc: 0.8093\n",
      "[Baseline] Epoch 34/50 - Train Loss: 0.3412 - Val Loss: 0.4988 - Train Acc: 0.8862 - Val Acc: 0.8253\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 35/50 - Train Loss: 0.3293 - Val Loss: 0.4809 - Train Acc: 0.8939 - Val Acc: 0.8224\n",
      "[Baseline] Epoch 36/50 - Train Loss: 0.3213 - Val Loss: 0.5022 - Train Acc: 0.8934 - Val Acc: 0.8180\n",
      "[Baseline] Epoch 37/50 - Train Loss: 0.3140 - Val Loss: 0.4676 - Train Acc: 0.8971 - Val Acc: 0.8268\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 38/50 - Train Loss: 0.3140 - Val Loss: 0.4928 - Train Acc: 0.8966 - Val Acc: 0.8151\n",
      "[Baseline] Epoch 39/50 - Train Loss: 0.3086 - Val Loss: 0.4640 - Train Acc: 0.8994 - Val Acc: 0.8370\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 40/50 - Train Loss: 0.3045 - Val Loss: 0.4391 - Train Acc: 0.8959 - Val Acc: 0.8413\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 41/50 - Train Loss: 0.3005 - Val Loss: 0.4788 - Train Acc: 0.8987 - Val Acc: 0.8370\n",
      "[Baseline] Epoch 42/50 - Train Loss: 0.2949 - Val Loss: 0.4636 - Train Acc: 0.9007 - Val Acc: 0.8326\n",
      "[Baseline] Epoch 43/50 - Train Loss: 0.2895 - Val Loss: 0.4489 - Train Acc: 0.9044 - Val Acc: 0.8486\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 44/50 - Train Loss: 0.2873 - Val Loss: 0.4530 - Train Acc: 0.9030 - Val Acc: 0.8399\n",
      "[Baseline] Epoch 45/50 - Train Loss: 0.2855 - Val Loss: 0.4245 - Train Acc: 0.9085 - Val Acc: 0.8486\n",
      "[Baseline] Epoch 46/50 - Train Loss: 0.2776 - Val Loss: 0.4272 - Train Acc: 0.9061 - Val Acc: 0.8588\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "[Baseline] Epoch 47/50 - Train Loss: 0.2750 - Val Loss: 0.4501 - Train Acc: 0.9085 - Val Acc: 0.8326\n",
      "[Baseline] Epoch 48/50 - Train Loss: 0.2724 - Val Loss: 0.4477 - Train Acc: 0.9079 - Val Acc: 0.8443\n",
      "[Baseline] Epoch 49/50 - Train Loss: 0.2684 - Val Loss: 0.4290 - Train Acc: 0.9124 - Val Acc: 0.8544\n",
      "[Baseline] Epoch 50/50 - Train Loss: 0.2657 - Val Loss: 0.4094 - Train Acc: 0.9124 - Val Acc: 0.8530\n",
      "✅ Saved standalone student model: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw10.pth\n",
      "\n",
      "🚀 Training Standalone Student for Td=60, Tw=15 (T_len=46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/3582298853.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student.load_state_dict(torch.load(save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline] Epoch 01/50 - Train Loss: 2.5073 - Val Loss: 2.4620 - Train Acc: 0.0994 - Val Acc: 0.1443\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 02/50 - Train Loss: 2.3875 - Val Loss: 2.3619 - Train Acc: 0.2119 - Val Acc: 0.2924\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 03/50 - Train Loss: 2.0764 - Val Loss: 1.9494 - Train Acc: 0.3823 - Val Acc: 0.4000\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 04/50 - Train Loss: 1.5613 - Val Loss: 1.6389 - Train Acc: 0.5268 - Val Acc: 0.5000\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 05/50 - Train Loss: 1.3421 - Val Loss: 1.4686 - Train Acc: 0.5783 - Val Acc: 0.5291\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 06/50 - Train Loss: 1.1974 - Val Loss: 1.3708 - Train Acc: 0.6127 - Val Acc: 0.5342\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 07/50 - Train Loss: 1.0976 - Val Loss: 1.2934 - Train Acc: 0.6432 - Val Acc: 0.5848\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 08/50 - Train Loss: 1.0143 - Val Loss: 1.1919 - Train Acc: 0.6767 - Val Acc: 0.6025\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 09/50 - Train Loss: 0.9384 - Val Loss: 1.1184 - Train Acc: 0.7046 - Val Acc: 0.6190\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 10/50 - Train Loss: 0.8741 - Val Loss: 1.0624 - Train Acc: 0.7284 - Val Acc: 0.6241\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 11/50 - Train Loss: 0.8016 - Val Loss: 0.9713 - Train Acc: 0.7523 - Val Acc: 0.6658\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 12/50 - Train Loss: 0.7385 - Val Loss: 0.9368 - Train Acc: 0.7709 - Val Acc: 0.6810\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 13/50 - Train Loss: 0.6851 - Val Loss: 0.8874 - Train Acc: 0.7836 - Val Acc: 0.6911\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 14/50 - Train Loss: 0.6419 - Val Loss: 0.8517 - Train Acc: 0.7994 - Val Acc: 0.6835\n",
      "[Baseline] Epoch 15/50 - Train Loss: 0.6069 - Val Loss: 0.8084 - Train Acc: 0.8076 - Val Acc: 0.7063\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 16/50 - Train Loss: 0.5777 - Val Loss: 0.7774 - Train Acc: 0.8135 - Val Acc: 0.7139\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 17/50 - Train Loss: 0.5453 - Val Loss: 0.7645 - Train Acc: 0.8291 - Val Acc: 0.7177\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 18/50 - Train Loss: 0.5263 - Val Loss: 0.7329 - Train Acc: 0.8327 - Val Acc: 0.7291\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 19/50 - Train Loss: 0.5066 - Val Loss: 0.7159 - Train Acc: 0.8370 - Val Acc: 0.7405\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 20/50 - Train Loss: 0.4795 - Val Loss: 0.6888 - Train Acc: 0.8431 - Val Acc: 0.7506\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 21/50 - Train Loss: 0.4669 - Val Loss: 0.6691 - Train Acc: 0.8450 - Val Acc: 0.7557\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 22/50 - Train Loss: 0.4577 - Val Loss: 0.6661 - Train Acc: 0.8514 - Val Acc: 0.7595\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 23/50 - Train Loss: 0.4370 - Val Loss: 0.6525 - Train Acc: 0.8581 - Val Acc: 0.7557\n",
      "[Baseline] Epoch 24/50 - Train Loss: 0.4300 - Val Loss: 0.6476 - Train Acc: 0.8588 - Val Acc: 0.7608\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 25/50 - Train Loss: 0.4175 - Val Loss: 0.6235 - Train Acc: 0.8634 - Val Acc: 0.7671\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 26/50 - Train Loss: 0.4074 - Val Loss: 0.6416 - Train Acc: 0.8663 - Val Acc: 0.7658\n",
      "[Baseline] Epoch 27/50 - Train Loss: 0.3968 - Val Loss: 0.5911 - Train Acc: 0.8690 - Val Acc: 0.7848\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 28/50 - Train Loss: 0.3879 - Val Loss: 0.6025 - Train Acc: 0.8685 - Val Acc: 0.7861\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 29/50 - Train Loss: 0.3771 - Val Loss: 0.5860 - Train Acc: 0.8763 - Val Acc: 0.7785\n",
      "[Baseline] Epoch 30/50 - Train Loss: 0.3731 - Val Loss: 0.5662 - Train Acc: 0.8753 - Val Acc: 0.7987\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 31/50 - Train Loss: 0.3586 - Val Loss: 0.5646 - Train Acc: 0.8832 - Val Acc: 0.7937\n",
      "[Baseline] Epoch 32/50 - Train Loss: 0.3541 - Val Loss: 0.5430 - Train Acc: 0.8793 - Val Acc: 0.8127\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 33/50 - Train Loss: 0.3439 - Val Loss: 0.5360 - Train Acc: 0.8873 - Val Acc: 0.8076\n",
      "[Baseline] Epoch 34/50 - Train Loss: 0.3399 - Val Loss: 0.5377 - Train Acc: 0.8856 - Val Acc: 0.8076\n",
      "[Baseline] Epoch 35/50 - Train Loss: 0.3336 - Val Loss: 0.5265 - Train Acc: 0.8875 - Val Acc: 0.8228\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 36/50 - Train Loss: 0.3279 - Val Loss: 0.5285 - Train Acc: 0.8901 - Val Acc: 0.8089\n",
      "[Baseline] Epoch 37/50 - Train Loss: 0.3192 - Val Loss: 0.4911 - Train Acc: 0.8947 - Val Acc: 0.8418\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 38/50 - Train Loss: 0.3140 - Val Loss: 0.4943 - Train Acc: 0.8926 - Val Acc: 0.8329\n",
      "[Baseline] Epoch 39/50 - Train Loss: 0.3057 - Val Loss: 0.4816 - Train Acc: 0.8969 - Val Acc: 0.8456\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 40/50 - Train Loss: 0.3006 - Val Loss: 0.4886 - Train Acc: 0.9018 - Val Acc: 0.8456\n",
      "[Baseline] Epoch 41/50 - Train Loss: 0.2964 - Val Loss: 0.4906 - Train Acc: 0.9028 - Val Acc: 0.8291\n",
      "[Baseline] Epoch 42/50 - Train Loss: 0.2894 - Val Loss: 0.4896 - Train Acc: 0.9058 - Val Acc: 0.8253\n",
      "[Baseline] Epoch 43/50 - Train Loss: 0.2843 - Val Loss: 0.4801 - Train Acc: 0.9066 - Val Acc: 0.8304\n",
      "[Baseline] Epoch 44/50 - Train Loss: 0.2784 - Val Loss: 0.4816 - Train Acc: 0.9040 - Val Acc: 0.8278\n",
      "[Baseline] Epoch 45/50 - Train Loss: 0.2747 - Val Loss: 0.4538 - Train Acc: 0.9098 - Val Acc: 0.8582\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 46/50 - Train Loss: 0.2730 - Val Loss: 0.4565 - Train Acc: 0.9117 - Val Acc: 0.8532\n",
      "[Baseline] Epoch 47/50 - Train Loss: 0.2658 - Val Loss: 0.4580 - Train Acc: 0.9122 - Val Acc: 0.8418\n",
      "[Baseline] Epoch 48/50 - Train Loss: 0.2598 - Val Loss: 0.4252 - Train Acc: 0.9122 - Val Acc: 0.8734\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "[Baseline] Epoch 49/50 - Train Loss: 0.2616 - Val Loss: 0.4493 - Train Acc: 0.9124 - Val Acc: 0.8519\n",
      "[Baseline] Epoch 50/50 - Train Loss: 0.2550 - Val Loss: 0.4309 - Train Acc: 0.9153 - Val Acc: 0.8772\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "✅ Saved standalone student model: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw15.pth\n",
      "\n",
      "🚀 Training Standalone Student for Td=60, Tw=20 (T_len=41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/3582298853.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student.load_state_dict(torch.load(save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline] Epoch 01/50 - Train Loss: 2.4633 - Val Loss: 2.4337 - Train Acc: 0.1136 - Val Acc: 0.1370\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 02/50 - Train Loss: 2.2445 - Val Loss: 2.0877 - Train Acc: 0.2811 - Val Acc: 0.3407\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 03/50 - Train Loss: 1.6023 - Val Loss: 1.6121 - Train Acc: 0.5627 - Val Acc: 0.5375\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 04/50 - Train Loss: 1.2694 - Val Loss: 1.3703 - Train Acc: 0.6687 - Val Acc: 0.6077\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 05/50 - Train Loss: 1.0817 - Val Loss: 1.2249 - Train Acc: 0.7165 - Val Acc: 0.6148\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 06/50 - Train Loss: 0.9522 - Val Loss: 1.1239 - Train Acc: 0.7445 - Val Acc: 0.6347\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 07/50 - Train Loss: 0.8627 - Val Loss: 1.0414 - Train Acc: 0.7552 - Val Acc: 0.6534\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 08/50 - Train Loss: 0.7891 - Val Loss: 0.9933 - Train Acc: 0.7719 - Val Acc: 0.6593\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 09/50 - Train Loss: 0.7290 - Val Loss: 0.9508 - Train Acc: 0.7881 - Val Acc: 0.6710\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 10/50 - Train Loss: 0.6849 - Val Loss: 0.8910 - Train Acc: 0.7974 - Val Acc: 0.6862\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 11/50 - Train Loss: 0.6405 - Val Loss: 0.8619 - Train Acc: 0.8074 - Val Acc: 0.6979\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 12/50 - Train Loss: 0.6044 - Val Loss: 0.8105 - Train Acc: 0.8180 - Val Acc: 0.7119\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 13/50 - Train Loss: 0.5711 - Val Loss: 0.7918 - Train Acc: 0.8255 - Val Acc: 0.7155\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 14/50 - Train Loss: 0.5453 - Val Loss: 0.7663 - Train Acc: 0.8304 - Val Acc: 0.7307\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 15/50 - Train Loss: 0.5192 - Val Loss: 0.7335 - Train Acc: 0.8379 - Val Acc: 0.7377\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 16/50 - Train Loss: 0.4984 - Val Loss: 0.7132 - Train Acc: 0.8438 - Val Acc: 0.7482\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 17/50 - Train Loss: 0.4785 - Val Loss: 0.6933 - Train Acc: 0.8525 - Val Acc: 0.7588\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 18/50 - Train Loss: 0.4596 - Val Loss: 0.6679 - Train Acc: 0.8551 - Val Acc: 0.7576\n",
      "[Baseline] Epoch 19/50 - Train Loss: 0.4433 - Val Loss: 0.6542 - Train Acc: 0.8653 - Val Acc: 0.7576\n",
      "[Baseline] Epoch 20/50 - Train Loss: 0.4232 - Val Loss: 0.6467 - Train Acc: 0.8659 - Val Acc: 0.7646\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 21/50 - Train Loss: 0.4159 - Val Loss: 0.6273 - Train Acc: 0.8704 - Val Acc: 0.7740\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 22/50 - Train Loss: 0.3984 - Val Loss: 0.6252 - Train Acc: 0.8730 - Val Acc: 0.7740\n",
      "[Baseline] Epoch 23/50 - Train Loss: 0.3898 - Val Loss: 0.6338 - Train Acc: 0.8743 - Val Acc: 0.7717\n",
      "[Baseline] Epoch 24/50 - Train Loss: 0.3774 - Val Loss: 0.5902 - Train Acc: 0.8806 - Val Acc: 0.7857\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 25/50 - Train Loss: 0.3661 - Val Loss: 0.5707 - Train Acc: 0.8815 - Val Acc: 0.7939\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 26/50 - Train Loss: 0.3558 - Val Loss: 0.5632 - Train Acc: 0.8850 - Val Acc: 0.7998\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 27/50 - Train Loss: 0.3507 - Val Loss: 0.5574 - Train Acc: 0.8876 - Val Acc: 0.7963\n",
      "[Baseline] Epoch 28/50 - Train Loss: 0.3413 - Val Loss: 0.5352 - Train Acc: 0.8925 - Val Acc: 0.8103\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 29/50 - Train Loss: 0.3322 - Val Loss: 0.5696 - Train Acc: 0.8912 - Val Acc: 0.7857\n",
      "[Baseline] Epoch 30/50 - Train Loss: 0.3260 - Val Loss: 0.5530 - Train Acc: 0.8934 - Val Acc: 0.7904\n",
      "[Baseline] Epoch 31/50 - Train Loss: 0.3182 - Val Loss: 0.5236 - Train Acc: 0.8957 - Val Acc: 0.8126\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 32/50 - Train Loss: 0.3138 - Val Loss: 0.5200 - Train Acc: 0.8999 - Val Acc: 0.8138\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 33/50 - Train Loss: 0.3089 - Val Loss: 0.5082 - Train Acc: 0.8982 - Val Acc: 0.8138\n",
      "[Baseline] Epoch 34/50 - Train Loss: 0.2981 - Val Loss: 0.4984 - Train Acc: 0.9047 - Val Acc: 0.8349\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 35/50 - Train Loss: 0.2993 - Val Loss: 0.4895 - Train Acc: 0.9013 - Val Acc: 0.8208\n",
      "[Baseline] Epoch 36/50 - Train Loss: 0.2894 - Val Loss: 0.4718 - Train Acc: 0.9043 - Val Acc: 0.8419\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 37/50 - Train Loss: 0.2855 - Val Loss: 0.4757 - Train Acc: 0.9057 - Val Acc: 0.8361\n",
      "[Baseline] Epoch 38/50 - Train Loss: 0.2783 - Val Loss: 0.4693 - Train Acc: 0.9056 - Val Acc: 0.8349\n",
      "[Baseline] Epoch 39/50 - Train Loss: 0.2764 - Val Loss: 0.4667 - Train Acc: 0.9073 - Val Acc: 0.8372\n",
      "[Baseline] Epoch 40/50 - Train Loss: 0.2727 - Val Loss: 0.4527 - Train Acc: 0.9098 - Val Acc: 0.8396\n",
      "[Baseline] Epoch 41/50 - Train Loss: 0.2646 - Val Loss: 0.4574 - Train Acc: 0.9136 - Val Acc: 0.8396\n",
      "[Baseline] Epoch 42/50 - Train Loss: 0.2595 - Val Loss: 0.4527 - Train Acc: 0.9148 - Val Acc: 0.8443\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 43/50 - Train Loss: 0.2572 - Val Loss: 0.4511 - Train Acc: 0.9143 - Val Acc: 0.8361\n",
      "[Baseline] Epoch 44/50 - Train Loss: 0.2552 - Val Loss: 0.4360 - Train Acc: 0.9167 - Val Acc: 0.8443\n",
      "[Baseline] Epoch 45/50 - Train Loss: 0.2478 - Val Loss: 0.4229 - Train Acc: 0.9209 - Val Acc: 0.8583\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "[Baseline] Epoch 46/50 - Train Loss: 0.2486 - Val Loss: 0.4522 - Train Acc: 0.9175 - Val Acc: 0.8384\n",
      "[Baseline] Epoch 47/50 - Train Loss: 0.2422 - Val Loss: 0.4219 - Train Acc: 0.9209 - Val Acc: 0.8513\n",
      "[Baseline] Epoch 48/50 - Train Loss: 0.2388 - Val Loss: 0.4245 - Train Acc: 0.9217 - Val Acc: 0.8536\n",
      "[Baseline] Epoch 49/50 - Train Loss: 0.2362 - Val Loss: 0.4438 - Train Acc: 0.9236 - Val Acc: 0.8337\n",
      "[Baseline] Epoch 50/50 - Train Loss: 0.2307 - Val Loss: 0.4015 - Train Acc: 0.9245 - Val Acc: 0.8689\n",
      "💾 Best standalone student model saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n",
      "✅ Saved standalone student model: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student/student_baseline_Td60_Tw20.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/3582298853.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student.load_state_dict(torch.load(save_path))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "baseline_save_dir = \"/home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/baseline_student\"\n",
    "os.makedirs(baseline_save_dir, exist_ok=True)\n",
    "\n",
    "for T_d in detection_times:\n",
    "    for T_w in window_sizes:\n",
    "        T_len = T_d - T_w + 1\n",
    "        expected_shape = (T_len, NUM_FEATURES)\n",
    "\n",
    "        print(f\"\\n🚀 Training Standalone Student for Td={T_d}, Tw={T_w} (T_len={T_len})\")\n",
    "\n",
    "        # === Paths\n",
    "        folder_name = f\"X_csv_split_{T_len}\"\n",
    "        input_dir = os.path.join(base_dir, f\"New_{T_d}\", f\"{T_w}\", \"split_tws\", folder_name)\n",
    "        train_path = os.path.join(input_dir, \"train\")\n",
    "        val_path   = os.path.join(input_dir, \"val\")\n",
    "        student_model_path = os.path.join(baseline_save_dir, f\"student_baseline_Td{T_d}_Tw{T_w}.pth\")\n",
    "\n",
    "        # === 1. Load Raw Data\n",
    "        X_train_raw, y_train_raw = load_split_from_folder(train_path, expected_shape)\n",
    "        X_val_raw, y_val_raw     = load_split_from_folder(val_path, expected_shape)\n",
    "\n",
    "        # === 2. Encode Labels & Apply SMOTE\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_train_encoded = label_encoder.fit_transform(y_train_raw)\n",
    "        X_train_flat = X_train_raw.reshape(X_train_raw.shape[0], -1)\n",
    "\n",
    "        X_resampled, y_resampled = SMOTE().fit_resample(X_train_flat, y_train_encoded)\n",
    "        X_train_bal = X_resampled.reshape(-1, T_len, NUM_FEATURES)\n",
    "        y_train_str = label_encoder.inverse_transform(y_resampled)\n",
    "\n",
    "        # === 3. Dataset & DataLoader\n",
    "        train_dataset = MultiStreamDataset(X_train_bal, y_train_str, label_encoder, augment=True)\n",
    "        val_dataset   = MultiStreamDataset(X_val_raw, y_val_raw, label_encoder, augment=False)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # === 4. Class Weights\n",
    "        class_weights_tensor = compute_class_weights(label_encoder.transform(y_train_str), device)\n",
    "\n",
    "        # === 5. Initialize StudentCNN\n",
    "        student_baseline = StudentCNN(\n",
    "            input_length=T_len,\n",
    "            num_classes=len(label_encoder.classes_)\n",
    "        ).to(device)\n",
    "\n",
    "        # === 6. Train Student Model (No KD)\n",
    "        train_accs_b, val_accs_b, train_losses_b, val_losses_b = train_student_baseline(\n",
    "            student=student_baseline,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            device=device,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            lr=0.0001,\n",
    "            class_weights=class_weights_tensor,\n",
    "            save_path=student_model_path\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Saved standalone student model: {student_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8f20a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ All evaluation results saved to: /home/HardDisk/Satang/thesis_proj/Deep_Learning/cross_archi/mlp/results/model_eval_summary.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2429355/2749317716.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# === Model Definition ===\n",
    "class PatchMLP(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class ChannelMLP(nn.Module):\n",
    "    def __init__(self, num_patches, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_patches, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, num_patches)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class MLPMixerRansomwareClassifier(nn.Module):\n",
    "    def __init__(self, seq_len=32, feature_dim=8, num_classes=12, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.patch_mlp = PatchMLP(feature_dim, hidden_dim)\n",
    "        self.channel_mlp = ChannelMLP(seq_len, hidden_dim)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.projection = nn.Linear(feature_dim, 128)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(feature_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        x = self.patch_mlp(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.channel_mlp(x)\n",
    "        x = self.global_pool(x)\n",
    "        feat = x.squeeze(-1)\n",
    "        feat_proj = self.projection(feat)\n",
    "        logits = self.classifier(feat)\n",
    "        return (logits, feat_proj) if return_features else logits\n",
    "\n",
    "# === CONFIG ===\n",
    "detection_times = [30, 45, 60]\n",
    "window_sizes = [10, 15, 20]\n",
    "NUM_FEATURES = 8\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "base_dir = \"/home/HardDisk/Satang/thesis_proj/\"\n",
    "base_teacher_dir = os.path.join(base_dir, \"Deep_Learning\", \"cross_archi\", \"mlp\", \"teacher\")\n",
    "student_kd_dir = os.path.join(base_dir, \"Deep_Learning\",\"cross_archi\", \"mlp\", \"student\")\n",
    "student_baseline_dir = os.path.join(base_dir, \"Deep_Learning\", \"cross_archi\",\"mlp\", \"baseline_student\")\n",
    "csv_output_path = os.path.join(base_dir, \"Deep_Learning\", \"cross_archi\",\"mlp\", \"results\", \"model_eval_summary.csv\")\n",
    "os.makedirs(os.path.dirname(csv_output_path), exist_ok=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "# === Evaluation Function ===\n",
    "def evaluate_and_log(model, model_path, test_loader, label_encoder, T_d, T_w, model_type, results_list):\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            logits, _ = model(inputs, return_features=True)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    report = classification_report(all_labels, all_preds, target_names=label_encoder.classes_, output_dict=True)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    results_list.append({\n",
    "        \"Td\": T_d,\n",
    "        \"Tw\": T_w,\n",
    "        \"T_len\": T_d - T_w + 1,\n",
    "        \"Model\": model_type,\n",
    "        \"Accuracy\": round(acc, 4),\n",
    "        \"Precision_macro\": round(report[\"macro avg\"][\"precision\"], 4),\n",
    "        \"Recall_macro\": round(report[\"macro avg\"][\"recall\"], 4),\n",
    "        \"F1_macro\": round(report[\"macro avg\"][\"f1-score\"], 4),\n",
    "        \"F1_weighted\": round(report[\"weighted avg\"][\"f1-score\"], 4),\n",
    "    })\n",
    "\n",
    "# === MAIN EVALUATION LOOP ===\n",
    "for T_d in detection_times:\n",
    "    for T_w in window_sizes:\n",
    "        T_len = T_d - T_w + 1\n",
    "        expected_shape = (T_len, NUM_FEATURES)\n",
    "        folder_name = f\"X_csv_split_{T_len}\"\n",
    "        test_path = os.path.join(base_dir, f\"New_{T_d}\", f\"{T_w}\", \"split_tws\", folder_name, \"test\")\n",
    "\n",
    "        # === Load Test Data ===\n",
    "        X_test_raw, y_test_raw = load_split_from_folder(test_path, expected_shape)\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder.fit(y_test_raw)\n",
    "\n",
    "        test_dataset = MultiStreamDataset(X_test_raw, y_test_raw, label_encoder, augment=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # === Evaluate Teacher ===\n",
    "        teacher_path = os.path.join(base_teacher_dir, f\"mlpmixer_Td{T_d}_Tw{T_w}.pth\")\n",
    "        teacher = MLPMixerRansomwareClassifier(\n",
    "            seq_len=T_len,\n",
    "            feature_dim=NUM_FEATURES,\n",
    "            num_classes=len(label_encoder.classes_),\n",
    "            hidden_dim=128\n",
    "        )\n",
    "        evaluate_and_log(teacher, teacher_path, test_loader, label_encoder, T_d, T_w, \"Teacher\", results)\n",
    "\n",
    "        # === Evaluate KD Student ===\n",
    "        student_kd_path = os.path.join(student_kd_dir, f\"student_from_mlp_Td{T_d}_Tw{T_w}.pth\")\n",
    "        student_kd = StudentCNN(input_length=T_len, num_classes=len(label_encoder.classes_)).to(DEVICE)\n",
    "        evaluate_and_log(student_kd, student_kd_path, test_loader, label_encoder, T_d, T_w, \"Student_KD\", results)\n",
    "\n",
    "        # === Evaluate Baseline Student ===\n",
    "        student_b_path = os.path.join(student_baseline_dir, f\"student_baseline_Td{T_d}_Tw{T_w}.pth\")\n",
    "        student_b = StudentCNN(input_length=T_len, num_classes=len(label_encoder.classes_)).to(DEVICE)\n",
    "        evaluate_and_log(student_b, student_b_path, test_loader, label_encoder, T_d, T_w, \"Student_Baseline\", results)\n",
    "\n",
    "# === Save to CSV ===\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(csv_output_path, index=False)\n",
    "print(f\"\\n✅ All evaluation results saved to: {csv_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c30ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_satang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
