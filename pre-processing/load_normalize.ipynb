{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 14:35:03.463328: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-17 14:35:03.471183: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-17 14:35:03.480870: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-17 14:35:03.483629: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-17 14:35:03.490603: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-17 14:35:03.928228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/eab301b/miniconda3/envs/baseViT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import timm\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from pyts.image import GramianAngularField, MarkovTransitionField\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For shuffle\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For feature engineering\n",
    "t_d = 30 # detection time\n",
    "t_w = 10 # time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For model\n",
    "EPOCH = 30\n",
    "BATCHSIZE = 32\n",
    "VAL = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = '/media/eab301b/b60ac77c-0185-406e-8549-95e9a295e4ca/thesis/thesis_proj/RanSAP/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = 'original'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = \"win7-250gb-ssd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['win7-120gb-hdd', 'win7-120gb-ssd', 'win7-250gb-hdd', 'win7-250gb-ssd']\n"
     ]
    }
   ],
   "source": [
    "os.chdir(f'{BASE}/{DATAPATH}')\n",
    "folders = sorted(os.listdir())\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AESCrypt', 'Cerber', 'Cerber-largefiles', 'Cerber-w10dirs', 'Darkside', 'Darkside-largefiles', 'Darkside-w10dirs', 'Excel', 'Firefox', 'GandCrab4', 'GandCrab4-largefiles', 'GandCrab4-w10dirs', 'Ryuk', 'Ryuk-largefiles', 'Ryuk-w10dirs', 'SDelete', 'Sodinokibi', 'Sodinokibi-largefiles', 'Sodinokibi-w10dirs', 'TeslaCrypt', 'TeslaCrypt-largefiles', 'TeslaCrypt-w10dirs', 'WannaCry', 'WannaCry-largefiles', 'WannaCry-w10dirs', 'Zip']\n"
     ]
    }
   ],
   "source": [
    "os.chdir(f'{BASE}/{DATAPATH}/{FOLDER}')\n",
    "labels = sorted(os.listdir())\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign = ['AESCrypt', 'Zip', 'SDelete', 'Excel', 'Firefox']\n",
    "ransomware = ['TeslaCrypt', 'TeslaCrypt-largefiles', 'TeslaCrypt-w10dirs',\n",
    "              'Cerber', 'Cerber-largefiles', 'Cerber-w10dirs', \n",
    "              'Darkside', 'Darkside-largefiles', 'Darkside-w10dirs',\n",
    "              'WannaCry', 'WannaCry-largefiles', 'WannaCry-w10dirs', \n",
    "              'GandCrab4', 'GandCrab4-largefiles', 'GandCrab4-w10dirs',\n",
    "              'Ryuk', 'Ryuk-largefiles', 'Ryuk-w10dirs',\n",
    "              'Sodinokibi', 'Sodinokibi-largefiles', 'Sodinokibi-w10dirs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "y1_train = []\n",
    "y1_test = []\n",
    "window_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_r = pd.read_csv('/media/eab301b/b60ac77c-0185-406e-8549-95e9a295e4ca/thesis/thesis_proj/RanSAP/dataset/original/win7-120gb-hdd/AESCrypt/AESCrypt-20200427_16-23-28/ata_read.csv', header=None)\n",
    "# df_w = pd.read_csv('/media/eab301b/b60ac77c-0185-406e-8549-95e9a295e4ca/thesis/thesis_proj/RanSAP/dataset/original/win7-120gb-hdd/AESCrypt/AESCrypt-20200427_16-23-28/ata_write.csv', header=None)\n",
    "# timestamp_r = np.unique(df_r[0])\n",
    "# timestamp_w = np.unique(df_r[0])\n",
    "# #print(df_r)\n",
    "# print(df_r.shape)\n",
    "# print(df_w.shape)\n",
    "# print(timestamp_r.shape)\n",
    "# print(timestamp_w.shape)\n",
    "# # filtered_df_r = df_r[(df_r[0] >= timestamp_r[0]) & (df_r[0] < timestamp_r[0+t_w])]\n",
    "# # print(filtered_df_r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15.509913740906637, 4.037691325277679, 380730692293464.1, 1.029052277799915e+18, 0.4223462945045273], [15.509913740906637, 4.037691325277679, 438784249962905.6, 1.0290524446865569e+18, 0.40757960984970976], [15.509913740906637, 4.037691325277679, 455875669430278.44, 1.0303932897564366e+18, 0.40769407649862127], [15.509913740906637, 4.037691325277679, 449934305109222.44, 1.0303934564996908e+18, 0.40550072749477045], [15.509913740906637, 4.037691325277679, 414622700620106.6, 1.0312256346691035e+18, 0.4051392274105731], [15.509913740906637, 4.037691325277679, 354425254913695.25, 1.0312257161318843e+18, 0.4049290826788261], [15.509913740906637, 4.037691325277679, 269341967989988.22, 1.0307175769226424e+18, 0.404541153337153], [15.509913740906637, 4.037691325277679, 159372839848985.56, 1.030717571157676e+18, 0.37825458874888934], [15.509913740906637, 4.037691325277679, 28347046582283.375, 1.0302091813793495e+18, 0.375958299890819], [15.509913740906637, 4.037691325277679, 28703106387115.375, 1.030209175619329e+18, 0.3921032928860585], [15.509913740906637, 4.037691325277679, 29011883073732.266, 1.0297005354345297e+18, 0.4227620641831326], [15.509913740906637, 4.037691325277679, 29319504773348.26, 1.0297005296794546e+18, 0.4690261252881025], [15.509913740906637, 4.037691325277679, 30541150805546.664, 1.0293422128716274e+18, 0.4676827432256568], [15.509913740906637, 4.037691325277679, 29319504773348.266, 1.0293422071221596e+18, 0.4663506573858241], [15.509913740906637, 4.037691325277679, 25654566676753.066, 1.0294922782833957e+18, 0.46471078812622474], [15.509913740906637, 4.037691325277679, 19546336515761.066, 1.0294922725345866e+18, 0.46234833815819343]]\n"
     ]
    }
   ],
   "source": [
    "tmp = []\n",
    "df_r = pd.read_csv(\"/media/eab301b/b60ac77c-0185-406e-8549-95e9a295e4ca/thesis/thesis_proj/RanSAP/dataset/original/win7-120gb-hdd/AESCrypt/AESCrypt-20200427_16-23-28/ata_read.csv\", header=None)\n",
    "df_w = pd.read_csv(\"/media/eab301b/b60ac77c-0185-406e-8549-95e9a295e4ca/thesis/thesis_proj/RanSAP/dataset/original/win7-120gb-hdd/AESCrypt/AESCrypt-20200427_16-23-28/ata_write.csv\", header=None)\n",
    "df_r = np.array(df_r)\n",
    "df_w = np.array(df_w)\n",
    "# df_r = np.sum(df_r[0:10,3])\n",
    "# df_w = np.sum(df_w[0:10,3])\n",
    "# print(df_w)\n",
    "# print(df_r)\n",
    "\n",
    "# print(df_r.shape)\n",
    "# print(df_w.shape)\n",
    "# print(df_w[0:10,3])\n",
    "for i in range(16):\n",
    "    \n",
    "    # timestamp_r = np.unique(df_r[0])\n",
    "    # timestamp_w = np.unique(df_w[0])\n",
    "\n",
    "    # filtered_df_r = df_r[(df_r[0] >= timestamp_r[i]) & (df_r[0] < timestamp_r[i+t_w])]\n",
    "    # filtered_df_w = df_w[(df_w[0] >= timestamp_w[i]) & (df_w[0] < timestamp_w[i+t_w])]\n",
    "\n",
    "    # average write throughput [byte/s]\n",
    "    T_write = np.sum(df_w[i:i+window_size,3])/(np.sum(df_w[0:10,1])/1000000)\n",
    "    # average read throughput [byte/s]\n",
    "    T_read = np.sum(df_r[i:i+window_size,3])/(np.sum(df_r[0:10,1])/1000000)\n",
    "\n",
    "    # variance of logical block addresses (written)\n",
    "    V_write_mean = np.mean(df_w[i:i+window_size,2])\n",
    "    V_write = (1/(window_size-1)) * np.sum((df_w[i:i+window_size,2]-V_write_mean)**2)\n",
    "\n",
    "    # variance of logical block addresses (read)\n",
    "    V_read_mean = np.mean(df_r[i:i+window_size:2])\n",
    "    V_read = (1/(window_size-1)) * np.sum((df_r[i:i+window_size:2]-V_read_mean)**2)\n",
    "\n",
    "    # average normalized Shannon entropy\n",
    "    H_write = (1/window_size) * np.sum(df_w[i:i+window_size,4])\n",
    "\n",
    "    tmp.append([T_write, T_read, V_write, V_read, H_write])\n",
    "print(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(SEED)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m folder \u001b[38;5;129;01min\u001b[39;00m folders:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "window_size = 10\n",
    "np.random.seed(SEED)\n",
    "for folder in folders:\n",
    "    for label in labels:\n",
    "        os.chdir(f'{BASE}/{DATAPATH}/{folder}/{label}')\n",
    "        dirs = sorted(os.listdir())\n",
    "        dirs = np.array(dirs)\n",
    "        # Shuffle directory\n",
    "        np.random.seed(SEED)\n",
    "        np.random.shuffle(dirs)\n",
    "        # train_idx = int(len(dirs)*0.8)\n",
    "        # print(f\"Train index: {train_idx}\")\n",
    "\n",
    "        for dir_idx in range(len(dirs)):\n",
    "            print(dirs[dir_idx])\n",
    "            os.chdir(f'{BASE}/{DATAPATH}/{folder}/{label}/{dirs[dir_idx]}')\n",
    "            files = sorted(os.listdir())\n",
    "            tmp = []\n",
    "            \n",
    "            for i in range():\n",
    "                df_r = pd.read_csv(f'{BASE}/{DATAPATH}/{label}/{dirs[dir_idx]}/{files[0]}', header=None)\n",
    "                df_w = pd.read_csv(f'{BASE}/{DATAPATH}/{label}/{dirs[dir_idx]}/{files[1]}', header=None)\n",
    "                # timestamp_r = np.unique(df_r[0])\n",
    "                # timestamp_w = np.unique(df_w[0])\n",
    "\n",
    "                # filtered_df_r = df_r[(df_r[0] >= timestamp_r[i]) & (df_r[0] < timestamp_r[i+t_w])]\n",
    "                # filtered_df_w = df_w[(df_w[0] >= timestamp_w[i]) & (df_w[0] < timestamp_w[i+t_w])]\n",
    "\n",
    "                # average write throughput [byte/s]\n",
    "                T_write = np.sum(df_w[i:i+window_size,3])/window_size\n",
    "\n",
    "                # average read throughput [byte/s]\n",
    "                T_read = np.sum(df_r[i:i+window_size:3])/window_size\n",
    "\n",
    "                # variance of logical block addresses (written)\n",
    "                V_write_mean = np.mean(df_w[i:i+window_size,2])\n",
    "                V_write = (1/(window_size-1)) * np.sum((df_w[i:i+window_size,2]-V_write_mean)**2)\n",
    "\n",
    "                # variance of logical block addresses (read)\n",
    "                V_read_mean = np.mean(df_r[i:i+window_size:3])\n",
    "                V_read = (1/(window_size-1)) * np.sum((df_r[i:i+window_size:3]-V_read_mean)**2)\n",
    "\n",
    "                # average normalized Shannon entropy\n",
    "                H_write = (1/window_size) * np.sum(df_w[i:i+window_size,4])\n",
    "\n",
    "                tmp.append([T_write, T_read, V_write, V_read, H_write])\n",
    "\n",
    "            # # Train-Test split\n",
    "            # if dir_idx < train_idx:\n",
    "            #     X_train.append(tmp)\n",
    "            #     y1_train.append(label)\n",
    "            # else:\n",
    "            #     X_test.append(tmp)\n",
    "            #     y1_test.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "y1_train = np.array(y1_train)\n",
    "X_test = np.array(X_test)\n",
    "y1_test = np.array(y1_test)\n",
    "print(type(X_train))\n",
    "print(X_train.shape)\n",
    "print(y1_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y1_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13312, 5)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "X_train_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3392, 5)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n",
    "X_test_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train_reshaped)\n",
    "X_test_normalized = scaler.transform(X_test_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized = X_train_normalized.reshape(X_train.shape)\n",
    "X_test_normalized = X_test_normalized.reshape(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(832, 16, 5)\n",
      "(212, 16, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_normalized.shape)\n",
    "print(X_test_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('E:/thesis project/thesis_proj/X_train_normalized.npy',X_train_normalized)\n",
    "np.save('E:/thesis project/thesis_proj/y1_train.npy',y1_train)\n",
    "np.save('E:/thesis project/thesis_proj/X_test_normalized.npy',X_test_normalized)\n",
    "np.save('E:/thesis project/thesis_proj/y1_test.npy',y1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "832\n"
     ]
    }
   ],
   "source": [
    "# y_train = []\n",
    "# for y in y1_train:\n",
    "#     if y in benign:\n",
    "#         y_train.append('Benign')\n",
    "#     else:\n",
    "#         y_train.append('Ransomware')\n",
    "# print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = []\n",
    "\n",
    "# for y in y1_test:\n",
    "#     if y in benign:\n",
    "#         y_test.append('Benign')\n",
    "#     else:\n",
    "#         y_test.append('Ransomware')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y1_train)\n",
    "y_test = np.array(y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Benign' 'Ransomware']\n",
      "[160 672]\n"
     ]
    }
   ],
   "source": [
    "classes, count = np.unique(y_train, return_counts=True)\n",
    "print(classes)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] ['Benign' 'Ransomware']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(832,)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lbl.fit_transform(classes), classes)\n",
    "y_train_int = lbl.fit_transform(y_train)\n",
    "y_train_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Benign' 'Ransomware']\n",
      "[ 41 171]\n"
     ]
    }
   ],
   "source": [
    "classes_test, count_test = np.unique(y_test, return_counts=True)\n",
    "print(classes_test)\n",
    "print(count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(212,)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_int = lbl.transform(y_test)\n",
    "y_test_int.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized = np.load('E:/thesis project/thesis_proj/X_train_normalized.npy')\n",
    "X_test_normalized = np.load('E:/thesis project/thesis_proj/X_test_normalized.npy')\n",
    "X_train_normalized = np.load('E:/thesis project/thesis_proj/X_train_normalized.npy')\n",
    "y1_test = np.load('E:/thesis project/thesis_proj/y1_test.npy')\n",
    "y1_train = np.load('E:/thesis project/thesis_proj/y1_train.npy')\n",
    "# For model\n",
    "EPOCH = 30\n",
    "BATCHSIZE = 32\n",
    "VAL = 0.2\n",
    "BASE = 'E:/thesis project/thesis_proj/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(832, 80)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_flattened = X_train_normalized.reshape(X_train_normalized.shape[0], -1)\n",
    "data_train_flattened.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1344,)\n"
     ]
    }
   ],
   "source": [
    "SEED =42\n",
    "np.random.seed(SEED)\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=SEED) # Resample benign class to be equal to ransomware class\n",
    "data_train_resampled_flattened, y_train_int_aug = smote.fit_resample(data_train_flattened, y_train_int)\n",
    "print(y_train_int_aug.shape)\n",
    "np.save('E:/thesis project/thesis_proj/y_train_int_aug.npy',y_train_int_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized_aug = data_train_resampled_flattened.reshape(data_train_resampled_flattened.shape[0], X_train_normalized.shape[1], X_train_normalized.shape[2])\n",
    "X_train_normalized_aug.shape\n",
    "np.save('E:/thesis project/thesis_proj/X_train_normalized_aug.npy',X_train_normalized_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseViT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
