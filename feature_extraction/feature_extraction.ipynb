{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469dd4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.data import Subset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.device_count() # returns 1 in my case\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = '/home/HardDisk/Satang/thesis_proj'\n",
    "DATAPATH = 'original' #(Ransap path)\n",
    "FOLDER = \"win7-120gb-hdd\"\n",
    "os.chdir(f'{BASE}/{DATAPATH}')\n",
    "folders = sorted(os.listdir())\n",
    "print(folders)\n",
    "os.chdir(f'{BASE}/{DATAPATH}/{FOLDER}')\n",
    "labels = sorted(os.listdir())\n",
    "print(labels)\n",
    "benign = ['AESCrypt', 'Zip', 'SDelete', 'Excel', 'Firefox']\n",
    "ransomware = ['TeslaCrypt', 'Cerber', 'WannaCry', 'GandCrab4', 'Ryuk', 'Sodinokibi', 'Darkside']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd4b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def save_numpy_array(array, label, base_dir, file_format='npy'):\n",
    "    \"\"\"\n",
    "    Save a Numpy array to a file in a class-named folder, appending a dynamic suffix to the label based on the number of existing files.\n",
    "\n",
    "    Parameters:\n",
    "    - array (np.ndarray): The Numpy array to save.\n",
    "    - label (str): The base label to use as the filename and folder name for the class.\n",
    "    - base_dir (str): The base directory where the class folder will be created.\n",
    "    - file_format (str): The format to save the file in ('npy' or 'csv'). Default is 'npy'.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The full path to the saved file.\n",
    "    \"\"\"\n",
    "    # Create class folder within the base directory\n",
    "    class_dir = os.path.join(base_dir, label)\n",
    "    os.makedirs(class_dir, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "    # Count existing files in the class folder\n",
    "    existing_files = os.listdir(class_dir)\n",
    "    file_count = sum(1 for file in existing_files if file.endswith(f\".{file_format}\"))\n",
    "    \n",
    "    # Generate suffix based on the file count\n",
    "    suffix = f\"_{file_count + 1}\"  # Start from _1 if no files exist\n",
    "\n",
    "    # Define the full file path with the dynamic suffix\n",
    "    file_name = f\"{label}{suffix}.{file_format}\"\n",
    "    file_path = os.path.join(class_dir, file_name)\n",
    "\n",
    "    # Save the file in the specified format\n",
    "    if file_format == 'npy':\n",
    "        np.save(file_path, array)\n",
    "    elif file_format == 'csv':\n",
    "        np.savetxt(file_path, array, delimiter=\",\", comments=\"\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Use 'npy' or 'csv'.\")\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_r = pd.read_csv(\"/home/HardDisk/Satang/thesis_proj/dataset/dataset/original/win7-120gb-hdd/AESCrypt/AESCrypt-20200427_16-23-28/ata_write.csv\",header=None)\n",
    "df_r = np.array(df_r)\n",
    "\n",
    "# Get unique values in order of appearance and their counts\n",
    "column_1 = df_r[:,0].astype(int)\n",
    "column_2 = df_r[:,3]\n",
    "# Get unique values in column 1 and their counts\n",
    "unique_values = np.unique(column_1)\n",
    "\n",
    "def count_operations_and_sum(column_1, column_2, window_size=10):\n",
    "    # Step 1: Count unique values and occurrences\n",
    "    unique_values, counts = np.unique(column_1, return_counts=True)\n",
    "    unique_counts_array = np.array(counts)  # Store unique counts in an array\n",
    "\n",
    "    # Step 2: Compute sums and counts for `column_2`\n",
    "    sum_dict = {}  # Store sum of column_2 values for each operation\n",
    "    count_dict = {}  # Store count of occurrences for each operation\n",
    "    operations = []  # Store the sequences (as NumPy arrays)\n",
    "    sliding_sums = []  # Store sum of unique counts in each sliding window\n",
    "\n",
    "    # Identify the valid steps between start and end\n",
    "    for i in range(len(unique_values) - (window_size-1)):  # Move one step at a time\n",
    "        operation = unique_values[i:i+window_size]  # Extract 3 consecutive values\n",
    "        operation_key = str(operation)  # Convert to string for dictionary key storage\n",
    "\n",
    "        if operation_key not in sum_dict:\n",
    "            mask = np.isin(column_1, operation)  # Find rows where column_1 matches the operation\n",
    "            sum_dict[operation_key] = np.nansum(column_2[mask])  # Sum corresponding column_2 values\n",
    "            count_dict[operation_key] = np.sum(mask)  # Count occurrences\n",
    "            operations.append(operation)  # Append as a NumPy array\n",
    "\n",
    "\n",
    "\n",
    "    return operations, sum_dict, count_dict, unique_counts_array\n",
    "\n",
    "\n",
    "\n",
    "operations, sum_dict, count_dict, unique_counts = count_operations_and_sum(column_1, column_2)\n",
    "\n",
    "print(\"Operations:\", len(operations))\n",
    "print(\"Sum Dict:\", sum_dict)\n",
    "print(\"Count Dict:\", count_dict)\n",
    "print(\"\\nUnique Counts Array:\", unique_counts)\n",
    "\n",
    "# operation_r = str(operations[1])\n",
    "# print(sum_dict[operation_r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb31ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "output_dir = \"/home/HardDisk/Satang/thesis_proj/30/10/raw_data_8\"\n",
    "np.random.seed(SEED)\n",
    "window_size = 10 # change the window size\n",
    "for folder in folders:\n",
    "    for label in labels:\n",
    "        os.chdir(f'{BASE}/{DATAPATH}/{folder}/{label}')\n",
    "        dirs = sorted(os.listdir())\n",
    "        dirs = np.array(dirs)\n",
    "        # Shuffle directory\n",
    "        np.random.seed(SEED)\n",
    "        np.random.shuffle(dirs)\n",
    "\n",
    "        for dir_idx in range(len(dirs)):\n",
    "            print(dirs[dir_idx])\n",
    "            os.chdir(f'{BASE}/{DATAPATH}/{folder}/{label}/{dirs[dir_idx]}')\n",
    "            files = sorted(os.listdir())\n",
    "            tmp = []\n",
    "            tmp_train = []\n",
    "            \n",
    "            df_r = pd.read_csv(f'{BASE}/{DATAPATH}/{folder}/{label}/{dirs[dir_idx]}/{files[0]}', header=None)\n",
    "            df_w = pd.read_csv(f'{BASE}/{DATAPATH}/{folder}/{label}/{dirs[dir_idx]}/{files[1]}', header=None)\n",
    "            # df_r = np.array(df_r)\n",
    "            # df_w = np.array(df_w)\n",
    "\n",
    "            # Given column 1 (numbers) and column 2 (values to sum)\n",
    "            column_r = df_r.iloc[:, 0]\n",
    "            column_w = df_w.iloc[:, 0]\n",
    "\n",
    "            values_r = df_r.iloc[:, 3]\n",
    "            values_w = df_w.iloc[:, 3]\n",
    "\n",
    "            # Get unique operations for read and write\n",
    "            operations_r, sum_dict_r, count_dict_r, unique_count_r = count_operations_and_sum(column_r, values_r,window_size)\n",
    "            operations_w, sum_dict_w, count_dict_w, unique_count_w = count_operations_and_sum(column_w, values_w,window_size)\n",
    "            # Ensure operations exist before processing\n",
    "\n",
    "            # Ensure operations exist before processing\n",
    "            step = min(len(operations_r), len(operations_w))\n",
    "            i_r = 0\n",
    "            i_w = 0\n",
    "            for k in range(step):\n",
    "                operation_r = str(operations_r[k])  # Convert NumPy array to string key\n",
    "                operation_w = str(operations_w[k])  # Convert NumPy array to string key\n",
    "\n",
    "                # Get operation sums and counts separately\n",
    "                operation_sum_r = sum_dict_r[operation_r]\n",
    "                operation_sum_w = sum_dict_w[operation_w]\n",
    "\n",
    "                count_r = count_dict_r[operation_r]\n",
    "                count_w = count_dict_w[operation_w]\n",
    "\n",
    "                # print(count_r)\n",
    "                # print(count_w)\n",
    "\n",
    "                unique_r = unique_count_r[k:k+window_size]\n",
    "                unique_w = unique_count_w[k:k+window_size]\n",
    "                # print(k)\n",
    "                # print(unique_r)\n",
    "                # print(unique_w)\n",
    "                # Average read/write throughput [byte/s]\n",
    "                T_read = operation_sum_r / window_size\n",
    "                T_write = operation_sum_w / window_size\n",
    "\n",
    "                # print(T_read)\n",
    "                # print(T_write)\n",
    "                \n",
    "                # Variance of logical block addresses (read)\n",
    "                filtered_read = df_r.iloc[i_r:i_r + np.sum(unique_r), 2]\n",
    "                filtered_read = filtered_read[~np.isnan(filtered_read)]  # Remove NaN\n",
    "                # print(filtered_read.shape)\n",
    "                V_read_mean = np.mean(filtered_read)\n",
    "                # print(V_read_mean)\n",
    "                V_read = (1 / (count_r - 1)) * np.sum((filtered_read - V_read_mean) ** 2)\n",
    "\n",
    "                # Variance of logical block addresses (write)\n",
    "                filtered_write = df_w.iloc[i_w:i_w + np.sum(unique_w), 2]\n",
    "                filtered_write = filtered_write[~np.isnan(filtered_write)]  # Remove NaN\n",
    "                V_write_mean = np.mean(filtered_write)\n",
    "                V_write = (1 / (count_w - 1)) * np.sum((filtered_write - V_write_mean) ** 2)\n",
    "\n",
    "                # Average normalized Shannon entropy (write)\n",
    "                filtered_entropy = df_w.iloc[i_w:i_w + np.sum(unique_w), 4]\n",
    "                filtered_entropy = filtered_entropy[~np.isnan(filtered_entropy)]  # Remove NaN\n",
    "                entropy_mean = np.mean(filtered_entropy)\n",
    "                num_row_ent = filtered_entropy.shape[0]\n",
    "                if num_row_ent > 0:\n",
    "                    H_write = (1 / count_w) * np.sum(df_w.iloc[i_w:i_w + np.sum(unique_w), 4])\n",
    "                else:\n",
    "                    H_write = 0\n",
    "                # Variance normalized Shannon Entropy (write)\n",
    "                Var_H_write = (1/ (count_w - 1)) * np.sum((filtered_entropy - entropy_mean) ** 2)\n",
    "\n",
    "                # Spatial Locality Ratio on write access\n",
    "                delta = 128\n",
    "                lba_values_write = filtered_write.values\n",
    "                if len(lba_values_write) < 2:\n",
    "                    SLR_write = 0.0\n",
    "                else:\n",
    "                    lba_diffs_write = np.abs(np.diff(lba_values_write))\n",
    "                    SLR_write = np.sum(lba_diffs_write <= delta) / len(lba_diffs_write)\n",
    "\n",
    "                # Spatial Locality Ratio on read access\n",
    "                delta = 128\n",
    "                lba_values_read = filtered_read.values\n",
    "                if len(lba_values_read) < 2:\n",
    "                    SLR_read = 0.0\n",
    "                else:\n",
    "                    lba_diffs_read = np.abs(np.diff(lba_values_read))\n",
    "                    SLR_read = np.sum(lba_diffs_read <= delta) / len(lba_diffs_read)\n",
    "\n",
    "                tmp.append([T_write, T_read, V_write, V_read, H_write, Var_H_write, SLR_write, SLR_read])\n",
    "\n",
    "                # Move window based on read/write count\n",
    "                i_r += unique_count_r[k]+1\n",
    "                i_w += unique_count_w[k]+1\n",
    "\n",
    "            tmp_train.append(tmp)\n",
    "            tmp_train = np.array(tmp_train)\n",
    "            transposed_array = tmp_train.transpose(1, 2, 0)\n",
    "\n",
    "            # Combine the groups into a single array\n",
    "            result_array = transposed_array.reshape(transposed_array.shape[0], -1)\n",
    "            save_numpy_array(result_array, label, output_dir, file_format='csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110fcda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_global_min_max_per_class(base_folder):\n",
    "    \"\"\"\n",
    "    Calculate the global minimum and maximum values for the first 5 columns across all CSV files\n",
    "    in each class folder inside the given base folder.\n",
    "    \n",
    "    Args:\n",
    "        base_folder (str): Path to the base folder containing class subfolders with CSV files.\n",
    "    \"\"\"\n",
    "    for class_folder in sorted(os.listdir(base_folder)):\n",
    "        class_path = os.path.join(base_folder, class_folder)\n",
    "        \n",
    "        # Ensure it's a directory before processing\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        \n",
    "        global_min = {}\n",
    "        global_max = {}\n",
    "        \n",
    "        for root, _, files in os.walk(class_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    # Read CSV without headers, considering only the first 5 columns\n",
    "                    df = pd.read_csv(file_path, header=None, usecols=range(8))\n",
    "                    \n",
    "                    # Calculate min and max for each column\n",
    "                    for column in df.columns:\n",
    "                        col_min = df[column].min()\n",
    "                        col_max = df[column].max()\n",
    "                        \n",
    "                        if column not in global_min or col_min < global_min[column]:\n",
    "                            global_min[column] = col_min\n",
    "                        if column not in global_max or col_max > global_max[column]:\n",
    "                            global_max[column] = col_max\n",
    "        \n",
    "        # Print results for each class folder\n",
    "        print(f\"Class: {class_folder}\")\n",
    "        print(\"  Global Minimum Values:\", global_min)\n",
    "        print(\"  Global Maximum Values:\", global_max)\n",
    "        print(\"----------------------------------\")\n",
    "\n",
    "# Example usage\n",
    "base_folder = \"/home/HardDisk/Satang/thesis_proj/New_30/10/raw_data_8\"  # Replace with the path to your main folder\n",
    "calculate_global_min_max_per_class(base_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4b18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "base_dir = '/home/HardDisk/Satang/thesis_proj/30/10/raw_data_8'\n",
    "output_dir = '/home/HardDisk/Satang/thesis_proj/30/10/raw_data_normalized_8'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for class_name in os.listdir(base_dir):\n",
    "    class_path = os.path.join(base_dir, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    dfs = []\n",
    "    filenames = []\n",
    "\n",
    "    # Step 1: Load all data\n",
    "    for file in os.listdir(class_path):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(class_path, file)\n",
    "            df = pd.read_csv(file_path, header=None)\n",
    "            dfs.append(df)\n",
    "            filenames.append((file, df))\n",
    "    \n",
    "    if not dfs:\n",
    "        continue\n",
    "\n",
    "    # Step 2: Concatenate and inspect\n",
    "    all_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # DEBUG: Check actual min/max per column\n",
    "    print(f\"\\nClass: {class_name}\")\n",
    "    print(\"Original min values:\\n\", all_data.min().values)\n",
    "    print(\"Original max values:\\n\", all_data.max().values)\n",
    "\n",
    "    # Step 3: Fit scaler on the combined data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(all_data)\n",
    "\n",
    "    # DEBUG: Verify scaler fit\n",
    "    print(\"Scaler min_:\\n\", scaler.data_min_)\n",
    "    print(\"Scaler max_:\\n\", scaler.data_max_)\n",
    "\n",
    "    # Step 4: Normalize each file\n",
    "    class_output_path = os.path.join(output_dir, class_name)\n",
    "    os.makedirs(class_output_path, exist_ok=True)\n",
    "\n",
    "    for file, df in filenames:\n",
    "        norm_data = scaler.transform(df)\n",
    "\n",
    "        # DEBUG: Log first row before and after\n",
    "        print(f\"File: {file}\")\n",
    "        print(\"Original first row:\", df.iloc[0].values)\n",
    "        print(\"Normalized first row:\", norm_data[0])\n",
    "\n",
    "        pd.DataFrame(norm_data).to_csv(\n",
    "            os.path.join(class_output_path, file),\n",
    "            index=False,\n",
    "            header=False\n",
    "        )\n",
    "\n",
    "print(\"✅ Normalization complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Files have been split and copied to: /home/HardDisk/Satang/github/30/15\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from shutil import copy2\n",
    "\n",
    "# === Setup ===\n",
    "INPUT_DIR = \"/home/HardDisk/Satang/thesis_proj/New_30/15/raw_data_normalized_8\" # directory of normlaized data\n",
    "OUTPUT_BASE = \"/home/HardDisk/Satang/github\"+ \"/30\"+\"/15\"  # You can change this if needed\n",
    "SPLIT_RATIOS = (0.7, 0.15, 0.15)  # Train, Val, Test\n",
    "SEED = 42\n",
    "window_size = 15\n",
    "# === Prepare output folders ===\n",
    "splits = ['train', 'val', 'test']\n",
    "for split in splits:\n",
    "    for cls in os.listdir(INPUT_DIR):\n",
    "        os.makedirs(os.path.join(OUTPUT_BASE, split, cls), exist_ok=True)\n",
    "\n",
    "# === Perform the split ===\n",
    "random.seed(SEED)\n",
    "\n",
    "for cls in sorted(os.listdir(INPUT_DIR)):\n",
    "    class_path = os.path.join(INPUT_DIR, cls)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    files = sorted([f for f in os.listdir(class_path) if f.endswith(\".csv\")])\n",
    "    random.shuffle(files)\n",
    "\n",
    "    n_total = len(files)\n",
    "    n_train = int(n_total * SPLIT_RATIOS[0])\n",
    "    n_val = int(n_total * SPLIT_RATIOS[1])\n",
    "    n_test = n_total - n_train - n_val\n",
    "\n",
    "    train_files = files[:n_train]\n",
    "    val_files = files[n_train:n_train + n_val]\n",
    "    test_files = files[n_train + n_val:]\n",
    "\n",
    "    for f in train_files:\n",
    "        src = os.path.join(class_path, f)\n",
    "        dst = os.path.join(OUTPUT_BASE, \"train\", cls, f)\n",
    "        copy2(src, dst)\n",
    "\n",
    "    for f in val_files:\n",
    "        src = os.path.join(class_path, f)\n",
    "        dst = os.path.join(OUTPUT_BASE, \"val\", cls, f)\n",
    "        copy2(src, dst)\n",
    "\n",
    "    for f in test_files:\n",
    "        src = os.path.join(class_path, f)\n",
    "        dst = os.path.join(OUTPUT_BASE, \"test\", cls, f)\n",
    "        copy2(src, dst)\n",
    "\n",
    "print(\"✅ Done! Files have been split and copied to:\", OUTPUT_BASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11343, 16, 8), (11343,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Config\n",
    "DATA_ROOT = Path(\"/home/HardDisk/Satang/github/30/15\")  # Folder structure: split/train/class/*.csv\n",
    "\n",
    "INNER_STRIDE = 1\n",
    "NUM_FEATURES = 8\n",
    "\n",
    "def extract_x_vectors(csv_path, t_window, inner_stride):\n",
    "    df = pd.read_csv(csv_path, header=None).values  # shape: (time, 8)\n",
    "    x_vectors = []\n",
    "\n",
    "    for i in range(0, len(df) - t_window + 1, inner_stride):\n",
    "        window = df[i:i + t_window, :]  # shape: (t_window, 8)\n",
    "        vector = np.mean(window, axis=0)\n",
    "        x_vectors.append(vector)\n",
    "\n",
    "    return np.array(x_vectors)  # shape: (N_x, 8)\n",
    "\n",
    "def extract_time_aware_samples(x_vectors, t_d, t_window):\n",
    "    sample_len = t_d - t_window + 1\n",
    "    outer_stride = max(1, int(0.25 * sample_len))  # 75% overlap → 25% stride\n",
    "\n",
    "    samples = []\n",
    "    for i in range(0, len(x_vectors) - sample_len + 1, outer_stride):\n",
    "        chunk = x_vectors[i:i + sample_len]\n",
    "        samples.append(chunk)\n",
    "\n",
    "    return np.stack(samples) if samples else np.empty((0, sample_len, NUM_FEATURES))\n",
    "\n",
    "def load_dataset_by_split(split, t_window, t_d):\n",
    "    data = []\n",
    "    labels = []\n",
    "    split_path = DATA_ROOT / split\n",
    "\n",
    "    for class_dir in sorted(split_path.iterdir()):\n",
    "        if not class_dir.is_dir():\n",
    "            continue\n",
    "\n",
    "        for file in sorted(class_dir.glob(\"*.csv\")):\n",
    "            x_vecs = extract_x_vectors(file, t_window, INNER_STRIDE)\n",
    "            samples = extract_time_aware_samples(x_vecs, t_d, t_window)\n",
    "\n",
    "            data.extend(samples)\n",
    "            labels.extend([class_dir.name] * len(samples))\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Test configuration (adjust as needed)\n",
    "\n",
    "T_D = 30 \n",
    "sample_len = T_D - window_size + 1\n",
    "\n",
    "X_train, y_train = load_dataset_by_split(\"train\", window_size, T_D)\n",
    "X_train.shape, y_train.shape, sample_len\n",
    "# Example call: load training data\n",
    "X_train, y_train =load_dataset_by_split(\"train\", window_size, T_D)\n",
    "X_val, y_val = load_dataset_by_split(\"val\", window_size,T_D)\n",
    "X_test, y_test = load_dataset_by_split(\"test\", window_size,T_D)\n",
    "\n",
    "\n",
    "X_train.shape, y_train.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done: All files split into (21, 8) samples with stride=5.\n"
     ]
    }
   ],
   "source": [
    "def split_csv_into_chunks(input_root, output_root, window_size, t_d, num_features=8):\n",
    "    input_root = Path(input_root)\n",
    "    output_root = Path(output_root)\n",
    "    sample_counter = {}\n",
    "\n",
    "    chunk_len = t_d - window_size + 1\n",
    "    stride = max(1, int(0.25 * chunk_len))  # 75% overlap\n",
    "\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        for class_dir in (input_root / split).iterdir():\n",
    "            if not class_dir.is_dir():\n",
    "                continue\n",
    "\n",
    "            class_name = class_dir.name\n",
    "            out_class_dir = output_root / split / class_name\n",
    "            out_class_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            for file in sorted(class_dir.glob(\"*.csv\")):\n",
    "                df = pd.read_csv(file, header=None).values\n",
    "                if df.shape[1] != num_features:\n",
    "                    continue\n",
    "\n",
    "                sample_counter.setdefault(class_name, 0)\n",
    "                for i in range(0, len(df) - chunk_len + 1, stride):\n",
    "                    chunk = df[i:i + chunk_len]\n",
    "                    sample_counter[class_name] += 1\n",
    "\n",
    "                    out_path = out_class_dir / f\"{class_name}_sample_{sample_counter[class_name]:04d}.csv\"\n",
    "                    pd.DataFrame(chunk).to_csv(out_path, index=False, header=False)\n",
    "\n",
    "    print(f\"✅ Done: All files split into ({chunk_len}, {num_features}) samples with stride={stride}.\")\n",
    "\n",
    "split_csv_into_chunks(\n",
    "    input_root=\"/home/HardDisk/Satang/github/30/15\",\n",
    "    output_root=\"/home/HardDisk/Satang/github/30/15/split_16\",\n",
    "    window_size=10,\n",
    "    t_d=30,\n",
    "    num_features=8\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_satang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
